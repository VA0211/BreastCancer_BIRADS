{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7822434,"sourceType":"datasetVersion","datasetId":4583334},{"sourceId":8075321,"sourceType":"datasetVersion","datasetId":4765536}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\nfrom sklearn.utils import shuffle\nfrom skimage.io import imread\nimport PIL\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision import models\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# from torchsampler import ImbalancedDatasetSampler\n# from torchmetrics.functional import auroc, precision, recall, f1_score, precision_recall_curve\nimport albumentations as albu\nimport albumentations.pytorch\nimport matplotlib.pyplot as plt\nimport torchmetrics\nimport timm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess data","metadata":{}},{"cell_type":"code","source":"def preprocess_df(data_dir):\n    df = pd.read_csv(os.path.join(data_dir,'breast-level_annotations.csv'))\n    \n#     df['img_path'] = f\"{data_dir}/png/png/{df['study_id']}/{df['image_id']}.png\"\n    \n    df['malignancy_label'] = df['breast_birads']\n    # Define positive and negatives based on BI-RADS categories\n    df.loc[df['malignancy_label'] == 'BI-RADS 1', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 2', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 3', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 4', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 5', 'malignancy_label'] = 1\n\n    # Use pre-defined splits to separate data into development and testing\n    train_df = df[df['split'] == 'training']\n    test_df = df[df['split'] == 'test']\n    \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n\ndef show_image_pair(image1, image2):\n    fig = plt.figure(figsize=(10, 20))\n    fig.add_subplot(1,2,1)\n    plt.imshow(image1)\n    fig.add_subplot(1,2, 2)\n    plt.imshow(image2)\n    plt.show()\n\ndef test_dataset(df, idx=0):\n    dataset = Dataset(df, data_dir)\n    \n    img_path = os.path.join(data_dir, 'png/png', dataset.df.iloc[idx]['study_id'], dataset.df.iloc[idx]['image_id'] + '.png')\n    image1 = PIL.Image.open(img_path).convert('RGB')\n\n    tensor = dataset[idx].squeeze()\n    image2 = torchvision.transforms.ToPILImage()(tensor)\n\n    show_image_pair(image1, image2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/full-fullsize/'\n\ntrain_df, test_df = preprocess_df(data_dir)\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx in [random.choice(range(100)) for i in range(3)]:\n#     test_dataset(train_df, idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract feature","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\n\nclass Img2Vec():\n    RESNET_OUTPUT_SIZES = {\n        'resnet18': 512,\n        'resnet34': 512,\n        'resnet50': 2048,\n        'resnet101': 2048,\n        'resnet152': 2048\n    }\n\n    EFFICIENTNET_OUTPUT_SIZES = {\n        'efficientnet_b0': 1280,\n        'efficientnet_b1': 1280,\n        'efficientnet_b2': 1408,\n        'efficientnet_b3': 1536,\n        'efficientnet_b4': 1792,\n        'efficientnet_b5': 2048,\n        'efficientnet_b6': 2304,\n        'efficientnet_b7': 2560\n    }\n\n    def __init__(self, model='resnet-18', layer='default', layer_output_size=512):\n       \n        self.layer_output_size = layer_output_size\n        self.model_name = model\n\n        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n\n        self.model = self.model.to(device)\n\n        self.model.eval()\n\n        self.scaler = transforms.Resize((224, 224))\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n\n    def get_vec(self, img, tensor=False):\n        \"\"\" Get vector embedding from PIL image\n        :param img: PIL Image or list of PIL Images\n        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n        :returns: Numpy ndarray\n        \"\"\"\n        if type(img) == list:\n            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n            images = torch.stack(a).to(device)\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(len(img), self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(images)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[:, :]\n                elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[:, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[:, :, 0, 0]\n        else:\n            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(device)\n\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(1, self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(1, self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(image)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[0, :]\n                elif self.model_name == 'densenet':\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[0, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[0, :, 0, 0]\n\n    def _get_model_and_layer(self, model_name, layer):\n        \"\"\" Internal method for getting layer from model\n        :param model_name: model name such as 'resnet-18'\n        :param layer: layer as a string for resnet-18 or int for alexnet\n        :returns: pytorch model, selected layer\n        \"\"\"\n\n        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n            model = getattr(models, model_name)(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n            else:\n                layer = model._modules.get(layer)\n            return model, layer\n        elif model_name == 'resnet-18':\n            model = models.resnet18(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = 512\n            else:\n                layer = model._modules.get(layer)\n\n            return model, layer\n\n        elif model_name == 'alexnet':\n            model = models.alexnet(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'vgg':\n            # VGG-11\n            model = models.vgg11_bn(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = model.classifier[-1].in_features # should be 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'densenet':\n            # Densenet-121\n            model = models.densenet121(pretrained=True)\n            if layer == 'default':\n                layer = model.features[-1]\n                self.layer_output_size = model.classifier.in_features # should be 1024\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        elif \"efficientnet\" in model_name:\n            # efficientnet-b0 ~ efficientnet-b7\n            if model_name == \"efficientnet_b0\":\n                model = models.efficientnet_b0(pretrained=True)\n            elif model_name == \"efficientnet_b1\":\n                model = models.efficientnet_b1(pretrained=True)\n            elif model_name == \"efficientnet_b2\":\n                model = models.efficientnet_b2(pretrained=True)\n            elif model_name == \"efficientnet_b3\":\n                model = models.efficientnet_b3(pretrained=True)\n            elif model_name == \"efficientnet_b4\":\n                model = models.efficientnet_b4(pretrained=True)\n            elif model_name == \"efficientnet_b5\":\n                model = models.efficientnet_b5(pretrained=True)\n            elif model_name == \"efficientnet_b6\":\n                model = models.efficientnet_b6(pretrained=True)\n            elif model_name == \"efficientnet_b7\":\n                model = models.efficientnet_b7(pretrained=True)\n            else:\n                raise KeyError('Un support %s.' % model_name)\n\n            if layer == 'default':\n                layer = model.features\n                self.layer_output_size = self.EFFICIENTNET_OUTPUT_SIZES[model_name]\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        else:\n            raise KeyError('Model %s was not found' % model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_img_feature(df, data_dir, model, vec_length):\n    img2vec = Img2Vec(model=model, \n                      layer_output_size=vec_length)\n    \n    vec_mat = np.zeros((len(df) , vec_length))\n\n    for idx, row in df.iterrows():\n        img_path = os.path.join(data_dir, 'png/png', row['study_id'], row['image_id'] + '.png')\n        img = PIL.Image.open(img_path).convert('RGB')\n        if row['laterality'] == 'L':\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n        vec = img2vec.get_vec(img)\n        vec_mat[idx, :] = vec\n        \n    features_df = pd.DataFrame(vec_mat)\n    features_df = features_df.add_prefix('feature_')\n    features_df['label'] = df['malignancy_label']\n    features_df['view_position'] = df['view_position']\n    features_df['laterality'] = df['laterality']\n    features_df['study_id'] = df['study_id']\n    \n    return features_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'efficientnet_b0'\nnum_features = 1280\n\nfeatures_train = extract_img_feature(df=train_df, \n                                     data_dir=data_dir,\n                                     model=model_name, \n                                     vec_length=num_features\n                                     )\n\nfeatures_test = extract_img_feature(df=test_df, \n                                    data_dir=data_dir,\n                                    model=model_name, \n                                    vec_length=num_features\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_CC = features_train[features_train['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntrain_MLO = features_train[features_train['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\ntest_CC = features_test[features_test['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntest_MLO = features_test[features_test['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\nconcat_features_train = train_CC.merge(train_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))\nconcat_features_test = test_CC.merge(test_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_features_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat_features_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nconcat_features_train.to_csv(f'{save_dir}concat_features_train_{model_name}.csv', index=False)\nconcat_features_test.to_csv(f'{save_dir}concat_features_test_{model_name}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NaN\nprint(concat_features_train.isna().any().any())\nprint(concat_features_test.isna().any().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classify Model","metadata":{}},{"cell_type":"code","source":"!pip install scikit-fuzzy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import skfuzzy as fuzz\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss, TomekLinks\nfrom sklearn.metrics import roc_curve,precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay\n\nimport os\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:35:45.013719Z","iopub.execute_input":"2024-04-10T08:35:45.014126Z","iopub.status.idle":"2024-04-10T08:35:48.176619Z","shell.execute_reply.started":"2024-04-10T08:35:45.014093Z","shell.execute_reply":"2024-04-10T08:35:48.175448Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"concat_features_train = pd.read_csv('/kaggle/input/vin-feature/concat_features_train_efficientnet_b0.csv')\nconcat_features_test = pd.read_csv('/kaggle/input/vin-feature/concat_features_test_efficientnet_b0.csv')\n\nconcat_features_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:36:45.926532Z","iopub.execute_input":"2024-04-10T08:36:45.927142Z","iopub.status.idle":"2024-04-10T08:36:59.836965Z","shell.execute_reply.started":"2024-04-10T08:36:45.927108Z","shell.execute_reply":"2024-04-10T08:36:59.835738Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1271_MLO  feature_1272_MLO  feature_1273_MLO  \\\n0     ...         -0.227731         -0.069607         -0.245696   \n1     ...         -0.267062         -0.045241          0.110348   \n2     ...         -0.229044         -0.099550         -0.273775   \n3     ...         -0.215473         -0.081372         -0.271739   \n4     ...         -0.171533         -0.156897         -0.103236   \n...   ...               ...               ...               ...   \n7994  ...         -0.224987         -0.070554         -0.268184   \n7995  ...         -0.248899         -0.061061         -0.270626   \n7996  ...         -0.119826         -0.057346         -0.202150   \n7997  ...         -0.125756         -0.156107         -0.262996   \n7998  ...         -0.056127         -0.057476         -0.211795   \n\n      feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  feature_1277_MLO  \\\n0            -0.261152          0.777141         -0.277788          0.419119   \n1            -0.278391          1.332054         -0.129526         -0.214615   \n2            -0.276369         -0.198178         -0.078316         -0.242274   \n3            -0.271640         -0.221536         -0.068043         -0.256326   \n4             0.013147         -0.208144         -0.277218         -0.196322   \n...                ...               ...               ...               ...   \n7994         -0.209462         -0.272872         -0.250021         -0.247301   \n7995          0.856457         -0.182576         -0.219728         -0.203649   \n7996         -0.243588         -0.253669         -0.193268         -0.087486   \n7997         -0.273179          1.193740         -0.097256         -0.161601   \n7998         -0.274628          0.907126         -0.173844         -0.074086   \n\n      feature_1278_MLO  feature_1279_MLO  label  \n0            -0.137479         -0.125389      0  \n1            -0.171379         -0.265228      0  \n2            -0.114382         -0.211536      0  \n3            -0.115784         -0.235606      0  \n4            -0.128612         -0.212763      0  \n...                ...               ...    ...  \n7994         -0.076688         -0.267579      0  \n7995         -0.137060         -0.168686      0  \n7996         -0.161627         -0.272322      0  \n7997         -0.047568         -0.126142      0  \n7998         -0.075687         -0.064506      0  \n\n[7999 rows x 2563 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2563 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = concat_features_train.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_train = np.array(concat_features_train['label']).astype(int)\nX_test = concat_features_test.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_test = np.array(concat_features_test['label']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:37:25.674114Z","iopub.execute_input":"2024-04-10T08:37:25.674568Z","iopub.status.idle":"2024-04-10T08:37:25.870781Z","shell.execute_reply.started":"2024-04-10T08:37:25.674506Z","shell.execute_reply":"2024-04-10T08:37:25.869501Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:37:26.846391Z","iopub.execute_input":"2024-04-10T08:37:26.846808Z","iopub.status.idle":"2024-04-10T08:37:26.884168Z","shell.execute_reply.started":"2024-04-10T08:37:26.846776Z","shell.execute_reply":"2024-04-10T08:37:26.882904Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1270_MLO  feature_1271_MLO  feature_1272_MLO  \\\n0     ...         -0.219428         -0.227731         -0.069607   \n1     ...         -0.066708         -0.267062         -0.045241   \n2     ...         -0.269503         -0.229044         -0.099550   \n3     ...         -0.274468         -0.215473         -0.081372   \n4     ...         -0.238900         -0.171533         -0.156897   \n...   ...               ...               ...               ...   \n7994  ...         -0.276627         -0.224987         -0.070554   \n7995  ...         -0.278353         -0.248899         -0.061061   \n7996  ...         -0.278436         -0.119826         -0.057346   \n7997  ...         -0.251418         -0.125756         -0.156107   \n7998  ...         -0.166142         -0.056127         -0.057476   \n\n      feature_1273_MLO  feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  \\\n0            -0.245696         -0.261152          0.777141         -0.277788   \n1             0.110348         -0.278391          1.332054         -0.129526   \n2            -0.273775         -0.276369         -0.198178         -0.078316   \n3            -0.271739         -0.271640         -0.221536         -0.068043   \n4            -0.103236          0.013147         -0.208144         -0.277218   \n...                ...               ...               ...               ...   \n7994         -0.268184         -0.209462         -0.272872         -0.250021   \n7995         -0.270626          0.856457         -0.182576         -0.219728   \n7996         -0.202150         -0.243588         -0.253669         -0.193268   \n7997         -0.262996         -0.273179          1.193740         -0.097256   \n7998         -0.211795         -0.274628          0.907126         -0.173844   \n\n      feature_1277_MLO  feature_1278_MLO  feature_1279_MLO  \n0             0.419119         -0.137479         -0.125389  \n1            -0.214615         -0.171379         -0.265228  \n2            -0.242274         -0.114382         -0.211536  \n3            -0.256326         -0.115784         -0.235606  \n4            -0.196322         -0.128612         -0.212763  \n...                ...               ...               ...  \n7994         -0.247301         -0.076688         -0.267579  \n7995         -0.203649         -0.137060         -0.168686  \n7996         -0.087486         -0.161627         -0.272322  \n7997         -0.161601         -0.047568         -0.126142  \n7998         -0.074086         -0.075687         -0.064506  \n\n[7999 rows x 2560 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1270_MLO</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.219428</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.066708</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.269503</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.274468</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.238900</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.276627</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.278353</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.278436</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.251418</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.166142</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2560 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"num_select_feature = int(X_train.shape[1]*0.1)\nmi_selector = SelectKBest(mutual_info_classif, k=num_select_feature)\n\n# Transform the data\nX_selected = mi_selector.fit_transform(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:37:27.908838Z","iopub.execute_input":"2024-04-10T08:37:27.909284Z","iopub.status.idle":"2024-04-10T08:39:02.923033Z","shell.execute_reply.started":"2024-04-10T08:37:27.909248Z","shell.execute_reply":"2024-04-10T08:39:02.921634Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_selected.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:41:55.038333Z","iopub.execute_input":"2024-04-10T08:41:55.038746Z","iopub.status.idle":"2024-04-10T08:41:55.046162Z","shell.execute_reply.started":"2024-04-10T08:41:55.038714Z","shell.execute_reply":"2024-04-10T08:41:55.045165Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(7999, 256)"},"metadata":{}}]},{"cell_type":"markdown","source":"### NN Classifier","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import KFold\n\n# Define the model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(num_select_feature,)),\n    tf.keras.layers.Dense(96, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=0.005),\n              loss='binary_crossentropy',\n              metrics= [\n                        tf.keras.metrics.Recall(name='recall'),\n                        tf.keras.metrics.Precision(name='precision'),\n                        tf.keras.metrics.AUC(curve='ROC', name='auc'),\n                        tf.keras.metrics.AUC(curve='PR', name='pr_auc'),\n                      ])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:41:58.421669Z","iopub.execute_input":"2024-04-10T08:41:58.423283Z","iopub.status.idle":"2024-04-10T08:42:13.406163Z","shell.execute_reply.started":"2024-04-10T08:41:58.423225Z","shell.execute_reply":"2024-04-10T08:42:13.404848Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-04-10 08:42:00.658272: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-10 08:42:00.658412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-10 08:42:00.820013: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │        \u001b[38;5;34m24,672\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m97\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,672</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,769\u001b[0m (96.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,769</span> (96.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,769\u001b[0m (96.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,769</span> (96.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Define k-fold cross-validation\nkfold = KFold(n_splits=10, shuffle=True)\n\n# Perform k-fold cross-validation\nfold_history = []\nfor train_index, val_index in kfold.split(X_selected):\n    X_train_fold, X_val_fold = X_selected[train_index], X_selected[val_index]\n    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n\n    # Train the model\n    history = model.fit(X_train_fold, y_train_fold, \n                        epochs=30, verbose=0,\n                        validation_data=(X_val_fold, y_val_fold)\n                       )\n    \n    fold_history.append(history.history)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:42:13.408697Z","iopub.execute_input":"2024-04-10T08:42:13.409194Z","iopub.status.idle":"2024-04-10T08:45:13.882955Z","shell.execute_reply.started":"2024-04-10T08:42:13.409150Z","shell.execute_reply":"2024-04-10T08:45:13.881591Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"fold_history[9]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:46:13.930557Z","iopub.execute_input":"2024-04-10T08:46:13.931026Z","iopub.status.idle":"2024-04-10T08:46:13.946764Z","shell.execute_reply.started":"2024-04-10T08:46:13.930991Z","shell.execute_reply":"2024-04-10T08:46:13.945560Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'auc': [0.9999732971191406,\n  0.9999550580978394,\n  0.9999920129776001,\n  0.999981701374054,\n  0.9992673993110657,\n  0.9999960064888,\n  0.9999963641166687,\n  0.9999722242355347,\n  0.9999951124191284,\n  0.9999907612800598,\n  0.9999701976776123,\n  0.9999950528144836,\n  0.9999889135360718,\n  0.9998669624328613,\n  0.9999926686286926,\n  0.9999816417694092,\n  0.9999921917915344,\n  0.9999970197677612,\n  0.9999604225158691,\n  0.9999946355819702,\n  0.9999656081199646,\n  0.9999905824661255,\n  0.9999685883522034,\n  0.9999632239341736,\n  0.9999886751174927,\n  0.9999943375587463,\n  0.9992705583572388,\n  0.999261736869812,\n  0.9992520213127136,\n  0.999991774559021],\n 'loss': [0.0041976128704845905,\n  0.0034887788351625204,\n  0.0025582045782357454,\n  0.0028580825310200453,\n  0.0025088381953537464,\n  0.002126439707353711,\n  0.0020876589696854353,\n  0.003318775910884142,\n  0.0027984315529465675,\n  0.0025396805722266436,\n  0.002641325583681464,\n  0.0027536589186638594,\n  0.0024810913018882275,\n  0.0033718098420649767,\n  0.0022611834574490786,\n  0.0025639329105615616,\n  0.0027881264686584473,\n  0.001764065120369196,\n  0.003560094628483057,\n  0.002572678728029132,\n  0.00395950535312295,\n  0.0024649733677506447,\n  0.002675407798960805,\n  0.0027301558293402195,\n  0.0030543324537575245,\n  0.0023398329503834248,\n  0.0025072863791137934,\n  0.0030943662859499454,\n  0.0029986926820129156,\n  0.002369298366829753],\n 'pr_auc': [0.9997451305389404,\n  0.9995380640029907,\n  0.9999240636825562,\n  0.9998400807380676,\n  0.9988983869552612,\n  0.9999629259109497,\n  0.9999662041664124,\n  0.9997246265411377,\n  0.9999557733535767,\n  0.9999131560325623,\n  0.9996960163116455,\n  0.9999539852142334,\n  0.9998959302902222,\n  0.9978959560394287,\n  0.999930202960968,\n  0.999844491481781,\n  0.999927282333374,\n  0.9999723434448242,\n  0.9996032118797302,\n  0.9999499320983887,\n  0.9997023940086365,\n  0.9999114871025085,\n  0.999678373336792,\n  0.9996985197067261,\n  0.9998932480812073,\n  0.9999472498893738,\n  0.9988899230957031,\n  0.998876690864563,\n  0.9987969398498535,\n  0.9999231696128845],\n 'precision': [0.9971387982368469,\n  0.995720386505127,\n  0.9985693693161011,\n  0.9971469044685364,\n  0.9985673427581787,\n  0.9971469044685364,\n  0.9971469044685364,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9971387982368469,\n  0.9971469044685364,\n  0.9971428513526917,\n  0.9985693693161011,\n  0.9985693693161011,\n  0.9971428513526917,\n  0.9971469044685364,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9971346855163574,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9985693693161011,\n  0.9971387982368469,\n  0.9985693693161011,\n  0.9971469044685364,\n  0.9971428513526917,\n  0.9985693693161011,\n  0.9971469044685364],\n 'recall': [0.9957143068313599,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9985714554786682,\n  0.9957143068313599,\n  0.9985714554786682,\n  0.9985714554786682,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9957143068313599,\n  0.9985714554786682,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9985714554786682,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9942857027053833,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9957143068313599,\n  0.9971428513526917,\n  0.9985714554786682,\n  0.9971428513526917,\n  0.9971428513526917,\n  0.9985714554786682],\n 'val_auc': [1.0,\n  1.0,\n  0.9999796152114868,\n  0.9999387860298157,\n  0.9998878240585327,\n  0.9999999403953552,\n  0.9999286532402039,\n  0.9998776912689209,\n  0.9998573064804077,\n  0.9998573064804077,\n  0.9998572468757629,\n  0.9998063445091248,\n  0.9998980164527893,\n  0.9998470544815063,\n  0.9999082684516907,\n  0.9996329545974731,\n  0.9998776912689209,\n  0.9998267292976379,\n  0.9997145533561707,\n  0.9995208382606506,\n  0.9997247457504272,\n  0.9997655749320984,\n  0.9998573064804077,\n  0.9997757077217102,\n  0.9999592304229736,\n  0.9997757077217102,\n  0.9997348785400391,\n  0.9998471140861511,\n  0.999092698097229,\n  0.9991028308868408],\n 'val_loss': [0.0019819578155875206,\n  0.005453106015920639,\n  0.0048778727650642395,\n  0.005251016467809677,\n  0.007317551877349615,\n  0.004232723265886307,\n  0.005649934988468885,\n  0.011938009411096573,\n  0.006450654938817024,\n  0.008421607315540314,\n  0.005836997181177139,\n  0.006821099668741226,\n  0.005541149992495775,\n  0.006166314706206322,\n  0.006474294234067202,\n  0.007711438462138176,\n  0.00628109322860837,\n  0.008698447607457638,\n  0.00834635179489851,\n  0.009980235248804092,\n  0.007840325124561787,\n  0.007618586998432875,\n  0.00695172231644392,\n  0.006582940928637981,\n  0.005369716323912144,\n  0.007571850437670946,\n  0.009954102337360382,\n  0.015001860447227955,\n  0.008833250030875206,\n  0.00954190269112587],\n 'val_pr_auc': [1.0,\n  1.0,\n  0.9997788667678833,\n  0.9993264675140381,\n  0.9987378120422363,\n  0.9999999403953552,\n  0.99921053647995,\n  0.9986212253570557,\n  0.99837327003479,\n  0.998368501663208,\n  0.9983766078948975,\n  0.9977501630783081,\n  0.9988601207733154,\n  0.9982548952102661,\n  0.9989773035049438,\n  0.9954026937484741,\n  0.9986212253570557,\n  0.9980033040046692,\n  0.9965464472770691,\n  0.9936571717262268,\n  0.9966924786567688,\n  0.9972306489944458,\n  0.998378574848175,\n  0.9973663091659546,\n  0.9995543956756592,\n  0.997360110282898,\n  0.9968307614326477,\n  0.9982480406761169,\n  0.9795652627944946,\n  0.9798793792724609],\n 'val_precision': [1.0,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9848484992980957,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9850746393203735,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9850746393203735,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9848484992980957,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9850746393203735,\n  0.9852941036224365,\n  0.9852941036224365,\n  0.9852941036224365],\n 'val_recall': [1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  0.9701492786407471,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  0.9850746393203735,\n  1.0,\n  1.0,\n  1.0,\n  0.9850746393203735,\n  1.0,\n  1.0,\n  0.9701492786407471,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  0.9850746393203735,\n  1.0,\n  1.0,\n  1.0]}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Get predictions","metadata":{}},{"cell_type":"code","source":"# Transform the data\nX_test_selected = mi_selector.fit_transform(X_test, y_test)\n\n# Predict using the test set\npreds = model.predict(X_test_selected)\nprint(preds)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:47:04.109358Z","iopub.execute_input":"2024-04-10T08:47:04.110159Z","iopub.status.idle":"2024-04-10T08:47:29.420919Z","shell.execute_reply.started":"2024-04-10T08:47:04.110121Z","shell.execute_reply":"2024-04-10T08:47:29.418937Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n[[1.0000000e+00]\n [2.7957553e-16]\n [4.8432826e-19]\n ...\n [3.2819112e-17]\n [1.4730660e-03]\n [9.5622599e-01]]\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = (preds >= 0.5).astype(np.int32)[:,0]\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:47:43.328985Z","iopub.execute_input":"2024-04-10T08:47:43.329649Z","iopub.status.idle":"2024-04-10T08:47:43.338678Z","shell.execute_reply.started":"2024-04-10T08:47:43.329594Z","shell.execute_reply":"2024-04-10T08:47:43.337269Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[1 0 0 ... 0 0 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize=None)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, fmt=\"d\", annot=True, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Calculate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:47:44.604650Z","iopub.execute_input":"2024-04-10T08:47:44.605630Z","iopub.status.idle":"2024-04-10T08:47:45.317059Z","shell.execute_reply.started":"2024-04-10T08:47:44.605536Z","shell.execute_reply":"2024-04-10T08:47:45.315323Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.89      0.52      0.65      1808\n           1       0.08      0.39      0.13       192\n\n    accuracy                           0.50      2000\n   macro avg       0.48      0.45      0.39      2000\nweighted avg       0.81      0.50      0.60      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_test, y_pred)\ntest_precision = precision_score(y_test, y_pred, average='macro')\ntest_recall = recall_score(y_test, y_pred, average='macro')\ntest_f1 = f1_score(y_test, y_pred, average='macro')\ntest_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"Accuracy:\", test_accuracy)\nprint('Precison:', test_precision)\nprint('Recall:', test_recall)\nprint('F1 Score:', test_f1)\nprint('AUC:', test_auc)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:47:49.166645Z","iopub.execute_input":"2024-04-10T08:47:49.167875Z","iopub.status.idle":"2024-04-10T08:47:49.193351Z","shell.execute_reply.started":"2024-04-10T08:47:49.167826Z","shell.execute_reply":"2024-04-10T08:47:49.191744Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Accuracy: 0.5045\nPrecison: 0.4838540042650935\nRecall: 0.45360896017699115\nF1 Score: 0.3924194672236053\nAUC: 0.45360896017699115\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}