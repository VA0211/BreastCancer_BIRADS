{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7822434,"sourceType":"datasetVersion","datasetId":4583334},{"sourceId":8075321,"sourceType":"datasetVersion","datasetId":4765536}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\nfrom sklearn.utils import shuffle\nfrom skimage.io import imread\nimport PIL\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision import models\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# from torchsampler import ImbalancedDatasetSampler\n# from torchmetrics.functional import auroc, precision, recall, f1_score, precision_recall_curve\nimport albumentations as albu\nimport albumentations.pytorch\nimport matplotlib.pyplot as plt\nimport torchmetrics\nimport timm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess data","metadata":{}},{"cell_type":"code","source":"def preprocess_df(data_dir):\n    df = pd.read_csv(os.path.join(data_dir,'breast-level_annotations.csv'))\n    \n#     df['img_path'] = f\"{data_dir}/png/png/{df['study_id']}/{df['image_id']}.png\"\n    \n    df['malignancy_label'] = df['breast_birads']\n    # Define positive and negatives based on BI-RADS categories\n    df.loc[df['malignancy_label'] == 'BI-RADS 1', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 2', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 3', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 4', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 5', 'malignancy_label'] = 1\n\n    # Use pre-defined splits to separate data into development and testing\n    train_df = df[df['split'] == 'training']\n    test_df = df[df['split'] == 'test']\n    \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n\ndef show_image_pair(image1, image2):\n    fig = plt.figure(figsize=(10, 20))\n    fig.add_subplot(1,2,1)\n    plt.imshow(image1)\n    fig.add_subplot(1,2, 2)\n    plt.imshow(image2)\n    plt.show()\n\ndef test_dataset(df, idx=0):\n    dataset = Dataset(df, data_dir)\n    \n    img_path = os.path.join(data_dir, 'png/png', dataset.df.iloc[idx]['study_id'], dataset.df.iloc[idx]['image_id'] + '.png')\n    image1 = PIL.Image.open(img_path).convert('RGB')\n\n    tensor = dataset[idx].squeeze()\n    image2 = torchvision.transforms.ToPILImage()(tensor)\n\n    show_image_pair(image1, image2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/full-fullsize/'\n\ntrain_df, test_df = preprocess_df(data_dir)\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx in [random.choice(range(100)) for i in range(3)]:\n#     test_dataset(train_df, idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract feature","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\n\nclass Img2Vec():\n    RESNET_OUTPUT_SIZES = {\n        'resnet18': 512,\n        'resnet34': 512,\n        'resnet50': 2048,\n        'resnet101': 2048,\n        'resnet152': 2048\n    }\n\n    EFFICIENTNET_OUTPUT_SIZES = {\n        'efficientnet_b0': 1280,\n        'efficientnet_b1': 1280,\n        'efficientnet_b2': 1408,\n        'efficientnet_b3': 1536,\n        'efficientnet_b4': 1792,\n        'efficientnet_b5': 2048,\n        'efficientnet_b6': 2304,\n        'efficientnet_b7': 2560\n    }\n\n    def __init__(self, model='resnet-18', layer='default', layer_output_size=512):\n       \n        self.layer_output_size = layer_output_size\n        self.model_name = model\n\n        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n\n        self.model = self.model.to(device)\n\n        self.model.eval()\n\n        self.scaler = transforms.Resize((224, 224))\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n\n    def get_vec(self, img, tensor=False):\n        \"\"\" Get vector embedding from PIL image\n        :param img: PIL Image or list of PIL Images\n        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n        :returns: Numpy ndarray\n        \"\"\"\n        if type(img) == list:\n            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n            images = torch.stack(a).to(device)\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(len(img), self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(images)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[:, :]\n                elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[:, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[:, :, 0, 0]\n        else:\n            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(device)\n\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(1, self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(1, self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(image)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[0, :]\n                elif self.model_name == 'densenet':\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[0, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[0, :, 0, 0]\n\n    def _get_model_and_layer(self, model_name, layer):\n        \"\"\" Internal method for getting layer from model\n        :param model_name: model name such as 'resnet-18'\n        :param layer: layer as a string for resnet-18 or int for alexnet\n        :returns: pytorch model, selected layer\n        \"\"\"\n\n        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n            model = getattr(models, model_name)(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n            else:\n                layer = model._modules.get(layer)\n            return model, layer\n        elif model_name == 'resnet-18':\n            model = models.resnet18(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = 512\n            else:\n                layer = model._modules.get(layer)\n\n            return model, layer\n\n        elif model_name == 'alexnet':\n            model = models.alexnet(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'vgg':\n            # VGG-11\n            model = models.vgg11_bn(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = model.classifier[-1].in_features # should be 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'densenet':\n            # Densenet-121\n            model = models.densenet121(pretrained=True)\n            if layer == 'default':\n                layer = model.features[-1]\n                self.layer_output_size = model.classifier.in_features # should be 1024\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        elif \"efficientnet\" in model_name:\n            # efficientnet-b0 ~ efficientnet-b7\n            if model_name == \"efficientnet_b0\":\n                model = models.efficientnet_b0(pretrained=True)\n            elif model_name == \"efficientnet_b1\":\n                model = models.efficientnet_b1(pretrained=True)\n            elif model_name == \"efficientnet_b2\":\n                model = models.efficientnet_b2(pretrained=True)\n            elif model_name == \"efficientnet_b3\":\n                model = models.efficientnet_b3(pretrained=True)\n            elif model_name == \"efficientnet_b4\":\n                model = models.efficientnet_b4(pretrained=True)\n            elif model_name == \"efficientnet_b5\":\n                model = models.efficientnet_b5(pretrained=True)\n            elif model_name == \"efficientnet_b6\":\n                model = models.efficientnet_b6(pretrained=True)\n            elif model_name == \"efficientnet_b7\":\n                model = models.efficientnet_b7(pretrained=True)\n            else:\n                raise KeyError('Un support %s.' % model_name)\n\n            if layer == 'default':\n                layer = model.features\n                self.layer_output_size = self.EFFICIENTNET_OUTPUT_SIZES[model_name]\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        else:\n            raise KeyError('Model %s was not found' % model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_img_feature(df, data_dir, model, vec_length):\n    img2vec = Img2Vec(model=model, \n                      layer_output_size=vec_length)\n    \n    vec_mat = np.zeros((len(df) , vec_length))\n\n    for idx, row in df.iterrows():\n        img_path = os.path.join(data_dir, 'png/png', row['study_id'], row['image_id'] + '.png')\n        img = PIL.Image.open(img_path).convert('RGB')\n        if row['laterality'] == 'L':\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n        vec = img2vec.get_vec(img)\n        vec_mat[idx, :] = vec\n        \n    features_df = pd.DataFrame(vec_mat)\n    features_df = features_df.add_prefix('feature_')\n    features_df['label'] = df['malignancy_label']\n    features_df['view_position'] = df['view_position']\n    features_df['laterality'] = df['laterality']\n    features_df['study_id'] = df['study_id']\n    \n    return features_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'efficientnet_b0'\nnum_features = 1280\n\nfeatures_train = extract_img_feature(df=train_df, \n                                     data_dir=data_dir,\n                                     model=model_name, \n                                     vec_length=num_features\n                                     )\n\nfeatures_test = extract_img_feature(df=test_df, \n                                    data_dir=data_dir,\n                                    model=model_name, \n                                    vec_length=num_features\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_CC = features_train[features_train['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntrain_MLO = features_train[features_train['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\ntest_CC = features_test[features_test['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntest_MLO = features_test[features_test['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\nconcat_features_train = train_CC.merge(train_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))\nconcat_features_test = test_CC.merge(test_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_features_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat_features_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nconcat_features_train.to_csv(f'{save_dir}concat_features_train_{model_name}.csv', index=False)\nconcat_features_test.to_csv(f'{save_dir}concat_features_test_{model_name}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NaN\nprint(concat_features_train.isna().any().any())\nprint(concat_features_test.isna().any().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classify Model","metadata":{}},{"cell_type":"code","source":"!pip install scikit-fuzzy","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:21:43.660093Z","iopub.execute_input":"2024-04-10T09:21:43.660524Z","iopub.status.idle":"2024-04-10T09:22:05.940254Z","shell.execute_reply.started":"2024-04-10T09:21:43.660490Z","shell.execute_reply":"2024-04-10T09:22:05.938983Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting scikit-fuzzy\n  Downloading scikit-fuzzy-0.4.2.tar.gz (993 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (1.26.4)\nRequirement already satisfied: scipy>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (1.11.4)\nRequirement already satisfied: networkx>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (3.2.1)\nBuilding wheels for collected packages: scikit-fuzzy\n  Building wheel for scikit-fuzzy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894077 sha256=186321e247d134795b92d1b3e1f84362077636cb8ed2a0ab33333ed9e415cdee\n  Stored in directory: /root/.cache/pip/wheels/4f/86/1b/dfd97134a2c8313e519bcebd95d3fedc7be7944db022094bc8\nSuccessfully built scikit-fuzzy\nInstalling collected packages: scikit-fuzzy\nSuccessfully installed scikit-fuzzy-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# import skfuzzy as fuzz\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss, TomekLinks, RandomUnderSampler\nfrom sklearn.metrics import roc_curve,precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\nimport os\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:22:36.467148Z","iopub.execute_input":"2024-04-10T15:22:36.467988Z","iopub.status.idle":"2024-04-10T15:22:39.918169Z","shell.execute_reply.started":"2024-04-10T15:22:36.467949Z","shell.execute_reply":"2024-04-10T15:22:39.917005Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"concat_features_train = pd.read_csv('/kaggle/input/vin-feature/concat_features_train_efficientnet_b0.csv')\nconcat_features_test = pd.read_csv('/kaggle/input/vin-feature/concat_features_test_efficientnet_b0.csv')\n\nconcat_features_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:22:44.154977Z","iopub.execute_input":"2024-04-10T15:22:44.155620Z","iopub.status.idle":"2024-04-10T15:22:58.893550Z","shell.execute_reply.started":"2024-04-10T15:22:44.155585Z","shell.execute_reply":"2024-04-10T15:22:58.892262Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1271_MLO  feature_1272_MLO  feature_1273_MLO  \\\n0     ...         -0.227731         -0.069607         -0.245696   \n1     ...         -0.267062         -0.045241          0.110348   \n2     ...         -0.229044         -0.099550         -0.273775   \n3     ...         -0.215473         -0.081372         -0.271739   \n4     ...         -0.171533         -0.156897         -0.103236   \n...   ...               ...               ...               ...   \n7994  ...         -0.224987         -0.070554         -0.268184   \n7995  ...         -0.248899         -0.061061         -0.270626   \n7996  ...         -0.119826         -0.057346         -0.202150   \n7997  ...         -0.125756         -0.156107         -0.262996   \n7998  ...         -0.056127         -0.057476         -0.211795   \n\n      feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  feature_1277_MLO  \\\n0            -0.261152          0.777141         -0.277788          0.419119   \n1            -0.278391          1.332054         -0.129526         -0.214615   \n2            -0.276369         -0.198178         -0.078316         -0.242274   \n3            -0.271640         -0.221536         -0.068043         -0.256326   \n4             0.013147         -0.208144         -0.277218         -0.196322   \n...                ...               ...               ...               ...   \n7994         -0.209462         -0.272872         -0.250021         -0.247301   \n7995          0.856457         -0.182576         -0.219728         -0.203649   \n7996         -0.243588         -0.253669         -0.193268         -0.087486   \n7997         -0.273179          1.193740         -0.097256         -0.161601   \n7998         -0.274628          0.907126         -0.173844         -0.074086   \n\n      feature_1278_MLO  feature_1279_MLO  label  \n0            -0.137479         -0.125389      0  \n1            -0.171379         -0.265228      0  \n2            -0.114382         -0.211536      0  \n3            -0.115784         -0.235606      0  \n4            -0.128612         -0.212763      0  \n...                ...               ...    ...  \n7994         -0.076688         -0.267579      0  \n7995         -0.137060         -0.168686      0  \n7996         -0.161627         -0.272322      0  \n7997         -0.047568         -0.126142      0  \n7998         -0.075687         -0.064506      0  \n\n[7999 rows x 2563 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2563 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = concat_features_train.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_train = np.array(concat_features_train['label']).astype(int)\nX_test = concat_features_test.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_test = np.array(concat_features_test['label']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:22:58.895600Z","iopub.execute_input":"2024-04-10T15:22:58.895968Z","iopub.status.idle":"2024-04-10T15:22:59.067365Z","shell.execute_reply.started":"2024-04-10T15:22:58.895938Z","shell.execute_reply":"2024-04-10T15:22:59.066071Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:22:59.068933Z","iopub.execute_input":"2024-04-10T15:22:59.069379Z","iopub.status.idle":"2024-04-10T15:22:59.105106Z","shell.execute_reply.started":"2024-04-10T15:22:59.069341Z","shell.execute_reply":"2024-04-10T15:22:59.103786Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1270_MLO  feature_1271_MLO  feature_1272_MLO  \\\n0     ...         -0.219428         -0.227731         -0.069607   \n1     ...         -0.066708         -0.267062         -0.045241   \n2     ...         -0.269503         -0.229044         -0.099550   \n3     ...         -0.274468         -0.215473         -0.081372   \n4     ...         -0.238900         -0.171533         -0.156897   \n...   ...               ...               ...               ...   \n7994  ...         -0.276627         -0.224987         -0.070554   \n7995  ...         -0.278353         -0.248899         -0.061061   \n7996  ...         -0.278436         -0.119826         -0.057346   \n7997  ...         -0.251418         -0.125756         -0.156107   \n7998  ...         -0.166142         -0.056127         -0.057476   \n\n      feature_1273_MLO  feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  \\\n0            -0.245696         -0.261152          0.777141         -0.277788   \n1             0.110348         -0.278391          1.332054         -0.129526   \n2            -0.273775         -0.276369         -0.198178         -0.078316   \n3            -0.271739         -0.271640         -0.221536         -0.068043   \n4            -0.103236          0.013147         -0.208144         -0.277218   \n...                ...               ...               ...               ...   \n7994         -0.268184         -0.209462         -0.272872         -0.250021   \n7995         -0.270626          0.856457         -0.182576         -0.219728   \n7996         -0.202150         -0.243588         -0.253669         -0.193268   \n7997         -0.262996         -0.273179          1.193740         -0.097256   \n7998         -0.211795         -0.274628          0.907126         -0.173844   \n\n      feature_1277_MLO  feature_1278_MLO  feature_1279_MLO  \n0             0.419119         -0.137479         -0.125389  \n1            -0.214615         -0.171379         -0.265228  \n2            -0.242274         -0.114382         -0.211536  \n3            -0.256326         -0.115784         -0.235606  \n4            -0.196322         -0.128612         -0.212763  \n...                ...               ...               ...  \n7994         -0.247301         -0.076688         -0.267579  \n7995         -0.203649         -0.137060         -0.168686  \n7996         -0.087486         -0.161627         -0.272322  \n7997         -0.161601         -0.047568         -0.126142  \n7998         -0.074086         -0.075687         -0.064506  \n\n[7999 rows x 2560 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1270_MLO</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.219428</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.066708</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.269503</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.274468</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.238900</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.276627</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.278353</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.278436</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.251418</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.166142</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2560 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Define the SMOTETomek resampling technique\nsmote_tomek = SMOTETomek(sampling_strategy=0.3,\n                         random_state=42)\n\n# Resample the training data using SMOTETomek\nX_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:22:59.107636Z","iopub.execute_input":"2024-04-10T15:22:59.108053Z","iopub.status.idle":"2024-04-10T15:23:08.089352Z","shell.execute_reply.started":"2024-04-10T15:22:59.108022Z","shell.execute_reply":"2024-04-10T15:23:08.088016Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"num_select_feature = int(X_train.shape[1]*0.05)\nmi_selector = SelectKBest(mutual_info_classif, k=num_select_feature)\n\n# Transform the data\nX_selected = mi_selector.fit_transform(X_resampled, y_resampled)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:23:08.090888Z","iopub.execute_input":"2024-04-10T15:23:08.091364Z","iopub.status.idle":"2024-04-10T15:24:58.319730Z","shell.execute_reply.started":"2024-04-10T15:23:08.091333Z","shell.execute_reply":"2024-04-10T15:24:58.318578Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_selected.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:24:58.321058Z","iopub.execute_input":"2024-04-10T15:24:58.321404Z","iopub.status.idle":"2024-04-10T15:24:58.328177Z","shell.execute_reply.started":"2024-04-10T15:24:58.321376Z","shell.execute_reply":"2024-04-10T15:24:58.327220Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(9381, 128)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=5, \n                           metric='minkowski', \n                           weights='distance',\n                           p=2)\nclf.fit(X_selected, y_resampled)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:25:03.939989Z","iopub.execute_input":"2024-04-10T15:25:03.940446Z","iopub.status.idle":"2024-04-10T15:25:03.958026Z","shell.execute_reply.started":"2024-04-10T15:25:03.940412Z","shell.execute_reply":"2024-04-10T15:25:03.957032Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"KNeighborsClassifier(weights='distance')","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(weights=&#x27;distance&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(weights=&#x27;distance&#x27;)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Get predictions","metadata":{}},{"cell_type":"code","source":"# Transform the data\nX_test_selected = mi_selector.fit_transform(X_test, y_test)\n\n# Predict on the test set\ny_pred = clf.predict(X_test_selected)\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:26:21.412592Z","iopub.execute_input":"2024-04-10T15:26:21.413496Z","iopub.status.idle":"2024-04-10T15:26:46.049648Z","shell.execute_reply.started":"2024-04-10T15:26:21.413437Z","shell.execute_reply":"2024-04-10T15:26:46.048551Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[0 0 0 ... 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize=None)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, fmt=\"d\", annot=True, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Calculate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:26:59.741922Z","iopub.execute_input":"2024-04-10T15:26:59.742655Z","iopub.status.idle":"2024-04-10T15:27:00.488090Z","shell.execute_reply.started":"2024-04-10T15:26:59.742614Z","shell.execute_reply":"2024-04-10T15:27:00.486787Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.91      0.95      0.93      1808\n           1       0.16      0.09      0.12       192\n\n    accuracy                           0.87      2000\n   macro avg       0.53      0.52      0.52      2000\nweighted avg       0.84      0.87      0.85      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_test, y_pred)\ntest_precision = precision_score(y_test, y_pred, average='macro')\ntest_recall = recall_score(y_test, y_pred, average='macro')\ntest_f1 = f1_score(y_test, y_pred, average='macro')\ntest_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"Accuracy:\", test_accuracy)\nprint('Precison:', test_precision)\nprint('Recall:', test_recall)\nprint('F1 Score:', test_f1)\nprint('AUC:', test_auc)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:27:07.314597Z","iopub.execute_input":"2024-04-10T15:27:07.315072Z","iopub.status.idle":"2024-04-10T15:27:07.341351Z","shell.execute_reply.started":"2024-04-10T15:27:07.315041Z","shell.execute_reply":"2024-04-10T15:27:07.339570Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy: 0.8655\nPrecison: 0.5335410892412454\nRecall: 0.5206028761061947\nF1 Score: 0.5226158521706339\nAUC: 0.5206028761061947\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}