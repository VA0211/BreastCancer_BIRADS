{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7822434,"sourceType":"datasetVersion","datasetId":4583334},{"sourceId":8075321,"sourceType":"datasetVersion","datasetId":4765536}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\nfrom sklearn.utils import shuffle\nfrom skimage.io import imread\nimport PIL\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision import models\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# from torchsampler import ImbalancedDatasetSampler\n# from torchmetrics.functional import auroc, precision, recall, f1_score, precision_recall_curve\nimport albumentations as albu\nimport albumentations.pytorch\nimport matplotlib.pyplot as plt\nimport torchmetrics\nimport timm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess data","metadata":{}},{"cell_type":"code","source":"def preprocess_df(data_dir):\n    df = pd.read_csv(os.path.join(data_dir,'breast-level_annotations.csv'))\n    \n#     df['img_path'] = f\"{data_dir}/png/png/{df['study_id']}/{df['image_id']}.png\"\n    \n    df['malignancy_label'] = df['breast_birads']\n    # Define positive and negatives based on BI-RADS categories\n    df.loc[df['malignancy_label'] == 'BI-RADS 1', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 2', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 3', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 4', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 5', 'malignancy_label'] = 1\n\n    # Use pre-defined splits to separate data into development and testing\n    train_df = df[df['split'] == 'training']\n    test_df = df[df['split'] == 'test']\n    \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n\ndef show_image_pair(image1, image2):\n    fig = plt.figure(figsize=(10, 20))\n    fig.add_subplot(1,2,1)\n    plt.imshow(image1)\n    fig.add_subplot(1,2, 2)\n    plt.imshow(image2)\n    plt.show()\n\ndef test_dataset(df, idx=0):\n    dataset = Dataset(df, data_dir)\n    \n    img_path = os.path.join(data_dir, 'png/png', dataset.df.iloc[idx]['study_id'], dataset.df.iloc[idx]['image_id'] + '.png')\n    image1 = PIL.Image.open(img_path).convert('RGB')\n\n    tensor = dataset[idx].squeeze()\n    image2 = torchvision.transforms.ToPILImage()(tensor)\n\n    show_image_pair(image1, image2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/full-fullsize/'\n\ntrain_df, test_df = preprocess_df(data_dir)\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx in [random.choice(range(100)) for i in range(3)]:\n#     test_dataset(train_df, idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract feature","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\n\nclass Img2Vec():\n    RESNET_OUTPUT_SIZES = {\n        'resnet18': 512,\n        'resnet34': 512,\n        'resnet50': 2048,\n        'resnet101': 2048,\n        'resnet152': 2048\n    }\n\n    EFFICIENTNET_OUTPUT_SIZES = {\n        'efficientnet_b0': 1280,\n        'efficientnet_b1': 1280,\n        'efficientnet_b2': 1408,\n        'efficientnet_b3': 1536,\n        'efficientnet_b4': 1792,\n        'efficientnet_b5': 2048,\n        'efficientnet_b6': 2304,\n        'efficientnet_b7': 2560\n    }\n\n    def __init__(self, model='resnet-18', layer='default', layer_output_size=512):\n       \n        self.layer_output_size = layer_output_size\n        self.model_name = model\n\n        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n\n        self.model = self.model.to(device)\n\n        self.model.eval()\n\n        self.scaler = transforms.Resize((224, 224))\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n\n    def get_vec(self, img, tensor=False):\n        \"\"\" Get vector embedding from PIL image\n        :param img: PIL Image or list of PIL Images\n        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n        :returns: Numpy ndarray\n        \"\"\"\n        if type(img) == list:\n            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n            images = torch.stack(a).to(device)\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(len(img), self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(images)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[:, :]\n                elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[:, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[:, :, 0, 0]\n        else:\n            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(device)\n\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(1, self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(1, self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(image)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[0, :]\n                elif self.model_name == 'densenet':\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[0, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[0, :, 0, 0]\n\n    def _get_model_and_layer(self, model_name, layer):\n        \"\"\" Internal method for getting layer from model\n        :param model_name: model name such as 'resnet-18'\n        :param layer: layer as a string for resnet-18 or int for alexnet\n        :returns: pytorch model, selected layer\n        \"\"\"\n\n        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n            model = getattr(models, model_name)(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n            else:\n                layer = model._modules.get(layer)\n            return model, layer\n        elif model_name == 'resnet-18':\n            model = models.resnet18(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = 512\n            else:\n                layer = model._modules.get(layer)\n\n            return model, layer\n\n        elif model_name == 'alexnet':\n            model = models.alexnet(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'vgg':\n            # VGG-11\n            model = models.vgg11_bn(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = model.classifier[-1].in_features # should be 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'densenet':\n            # Densenet-121\n            model = models.densenet121(pretrained=True)\n            if layer == 'default':\n                layer = model.features[-1]\n                self.layer_output_size = model.classifier.in_features # should be 1024\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        elif \"efficientnet\" in model_name:\n            # efficientnet-b0 ~ efficientnet-b7\n            if model_name == \"efficientnet_b0\":\n                model = models.efficientnet_b0(pretrained=True)\n            elif model_name == \"efficientnet_b1\":\n                model = models.efficientnet_b1(pretrained=True)\n            elif model_name == \"efficientnet_b2\":\n                model = models.efficientnet_b2(pretrained=True)\n            elif model_name == \"efficientnet_b3\":\n                model = models.efficientnet_b3(pretrained=True)\n            elif model_name == \"efficientnet_b4\":\n                model = models.efficientnet_b4(pretrained=True)\n            elif model_name == \"efficientnet_b5\":\n                model = models.efficientnet_b5(pretrained=True)\n            elif model_name == \"efficientnet_b6\":\n                model = models.efficientnet_b6(pretrained=True)\n            elif model_name == \"efficientnet_b7\":\n                model = models.efficientnet_b7(pretrained=True)\n            else:\n                raise KeyError('Un support %s.' % model_name)\n\n            if layer == 'default':\n                layer = model.features\n                self.layer_output_size = self.EFFICIENTNET_OUTPUT_SIZES[model_name]\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        else:\n            raise KeyError('Model %s was not found' % model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_img_feature(df, data_dir, model, vec_length):\n    img2vec = Img2Vec(model=model, \n                      layer_output_size=vec_length)\n    \n    vec_mat = np.zeros((len(df) , vec_length))\n\n    for idx, row in df.iterrows():\n        img_path = os.path.join(data_dir, 'png/png', row['study_id'], row['image_id'] + '.png')\n        img = PIL.Image.open(img_path).convert('RGB')\n        if row['laterality'] == 'L':\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n        vec = img2vec.get_vec(img)\n        vec_mat[idx, :] = vec\n        \n    features_df = pd.DataFrame(vec_mat)\n    features_df = features_df.add_prefix('feature_')\n    features_df['label'] = df['malignancy_label']\n    features_df['view_position'] = df['view_position']\n    features_df['laterality'] = df['laterality']\n    features_df['study_id'] = df['study_id']\n    \n    return features_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'efficientnet_b0'\nnum_features = 1280\n\nfeatures_train = extract_img_feature(df=train_df, \n                                     data_dir=data_dir,\n                                     model=model_name, \n                                     vec_length=num_features\n                                     )\n\nfeatures_test = extract_img_feature(df=test_df, \n                                    data_dir=data_dir,\n                                    model=model_name, \n                                    vec_length=num_features\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_CC = features_train[features_train['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntrain_MLO = features_train[features_train['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\ntest_CC = features_test[features_test['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntest_MLO = features_test[features_test['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\nconcat_features_train = train_CC.merge(train_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))\nconcat_features_test = test_CC.merge(test_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_features_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat_features_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nconcat_features_train.to_csv(f'{save_dir}concat_features_train_{model_name}.csv', index=False)\nconcat_features_test.to_csv(f'{save_dir}concat_features_test_{model_name}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NaN\nprint(concat_features_train.isna().any().any())\nprint(concat_features_test.isna().any().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classify Model","metadata":{}},{"cell_type":"code","source":"!pip install scikit-fuzzy","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:05:32.400405Z","iopub.execute_input":"2024-04-10T09:05:32.400819Z","iopub.status.idle":"2024-04-10T09:05:53.541464Z","shell.execute_reply.started":"2024-04-10T09:05:32.400776Z","shell.execute_reply":"2024-04-10T09:05:53.540164Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting scikit-fuzzy\n  Downloading scikit-fuzzy-0.4.2.tar.gz (993 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (1.26.4)\nRequirement already satisfied: scipy>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (1.11.4)\nRequirement already satisfied: networkx>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (3.2.1)\nBuilding wheels for collected packages: scikit-fuzzy\n  Building wheel for scikit-fuzzy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894077 sha256=82a05b3c82451c05eb1435700e0b5142de9f34b6e512a011d359aab6442eb3e7\n  Stored in directory: /root/.cache/pip/wheels/4f/86/1b/dfd97134a2c8313e519bcebd95d3fedc7be7944db022094bc8\nSuccessfully built scikit-fuzzy\nInstalling collected packages: scikit-fuzzy\nSuccessfully installed scikit-fuzzy-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import skfuzzy as fuzz\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss, TomekLinks\nfrom sklearn.metrics import roc_curve,precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay\n\nimport os\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:06:00.277724Z","iopub.execute_input":"2024-04-10T09:06:00.278109Z","iopub.status.idle":"2024-04-10T09:06:03.709814Z","shell.execute_reply.started":"2024-04-10T09:06:00.278077Z","shell.execute_reply":"2024-04-10T09:06:03.708688Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"concat_features_train = pd.read_csv('/kaggle/input/vin-feature/concat_features_train_efficientnet_b0.csv')\nconcat_features_test = pd.read_csv('/kaggle/input/vin-feature/concat_features_test_efficientnet_b0.csv')\n\nconcat_features_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:06:59.597780Z","iopub.execute_input":"2024-04-10T09:06:59.598531Z","iopub.status.idle":"2024-04-10T09:07:13.575625Z","shell.execute_reply.started":"2024-04-10T09:06:59.598497Z","shell.execute_reply":"2024-04-10T09:07:13.574236Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1271_MLO  feature_1272_MLO  feature_1273_MLO  \\\n0     ...         -0.227731         -0.069607         -0.245696   \n1     ...         -0.267062         -0.045241          0.110348   \n2     ...         -0.229044         -0.099550         -0.273775   \n3     ...         -0.215473         -0.081372         -0.271739   \n4     ...         -0.171533         -0.156897         -0.103236   \n...   ...               ...               ...               ...   \n7994  ...         -0.224987         -0.070554         -0.268184   \n7995  ...         -0.248899         -0.061061         -0.270626   \n7996  ...         -0.119826         -0.057346         -0.202150   \n7997  ...         -0.125756         -0.156107         -0.262996   \n7998  ...         -0.056127         -0.057476         -0.211795   \n\n      feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  feature_1277_MLO  \\\n0            -0.261152          0.777141         -0.277788          0.419119   \n1            -0.278391          1.332054         -0.129526         -0.214615   \n2            -0.276369         -0.198178         -0.078316         -0.242274   \n3            -0.271640         -0.221536         -0.068043         -0.256326   \n4             0.013147         -0.208144         -0.277218         -0.196322   \n...                ...               ...               ...               ...   \n7994         -0.209462         -0.272872         -0.250021         -0.247301   \n7995          0.856457         -0.182576         -0.219728         -0.203649   \n7996         -0.243588         -0.253669         -0.193268         -0.087486   \n7997         -0.273179          1.193740         -0.097256         -0.161601   \n7998         -0.274628          0.907126         -0.173844         -0.074086   \n\n      feature_1278_MLO  feature_1279_MLO  label  \n0            -0.137479         -0.125389      0  \n1            -0.171379         -0.265228      0  \n2            -0.114382         -0.211536      0  \n3            -0.115784         -0.235606      0  \n4            -0.128612         -0.212763      0  \n...                ...               ...    ...  \n7994         -0.076688         -0.267579      0  \n7995         -0.137060         -0.168686      0  \n7996         -0.161627         -0.272322      0  \n7997         -0.047568         -0.126142      0  \n7998         -0.075687         -0.064506      0  \n\n[7999 rows x 2563 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2563 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = concat_features_train.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_train = np.array(concat_features_train['label']).astype(int)\nX_test = concat_features_test.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_test = np.array(concat_features_test['label']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:07:42.220701Z","iopub.execute_input":"2024-04-10T09:07:42.221190Z","iopub.status.idle":"2024-04-10T09:07:42.405807Z","shell.execute_reply.started":"2024-04-10T09:07:42.221148Z","shell.execute_reply":"2024-04-10T09:07:42.404667Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:07:44.036804Z","iopub.execute_input":"2024-04-10T09:07:44.038107Z","iopub.status.idle":"2024-04-10T09:07:44.074595Z","shell.execute_reply.started":"2024-04-10T09:07:44.038060Z","shell.execute_reply":"2024-04-10T09:07:44.073359Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1270_MLO  feature_1271_MLO  feature_1272_MLO  \\\n0     ...         -0.219428         -0.227731         -0.069607   \n1     ...         -0.066708         -0.267062         -0.045241   \n2     ...         -0.269503         -0.229044         -0.099550   \n3     ...         -0.274468         -0.215473         -0.081372   \n4     ...         -0.238900         -0.171533         -0.156897   \n...   ...               ...               ...               ...   \n7994  ...         -0.276627         -0.224987         -0.070554   \n7995  ...         -0.278353         -0.248899         -0.061061   \n7996  ...         -0.278436         -0.119826         -0.057346   \n7997  ...         -0.251418         -0.125756         -0.156107   \n7998  ...         -0.166142         -0.056127         -0.057476   \n\n      feature_1273_MLO  feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  \\\n0            -0.245696         -0.261152          0.777141         -0.277788   \n1             0.110348         -0.278391          1.332054         -0.129526   \n2            -0.273775         -0.276369         -0.198178         -0.078316   \n3            -0.271739         -0.271640         -0.221536         -0.068043   \n4            -0.103236          0.013147         -0.208144         -0.277218   \n...                ...               ...               ...               ...   \n7994         -0.268184         -0.209462         -0.272872         -0.250021   \n7995         -0.270626          0.856457         -0.182576         -0.219728   \n7996         -0.202150         -0.243588         -0.253669         -0.193268   \n7997         -0.262996         -0.273179          1.193740         -0.097256   \n7998         -0.211795         -0.274628          0.907126         -0.173844   \n\n      feature_1277_MLO  feature_1278_MLO  feature_1279_MLO  \n0             0.419119         -0.137479         -0.125389  \n1            -0.214615         -0.171379         -0.265228  \n2            -0.242274         -0.114382         -0.211536  \n3            -0.256326         -0.115784         -0.235606  \n4            -0.196322         -0.128612         -0.212763  \n...                ...               ...               ...  \n7994         -0.247301         -0.076688         -0.267579  \n7995         -0.203649         -0.137060         -0.168686  \n7996         -0.087486         -0.161627         -0.272322  \n7997         -0.161601         -0.047568         -0.126142  \n7998         -0.074086         -0.075687         -0.064506  \n\n[7999 rows x 2560 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1270_MLO</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.219428</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.066708</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.269503</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.274468</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.238900</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.276627</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.278353</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.278436</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.251418</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.166142</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2560 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"num_select_feature = int(X_train.shape[1]*0.25)\nmi_selector = SelectKBest(mutual_info_classif, k=num_select_feature)\n\n# Transform the data\nX_selected = mi_selector.fit_transform(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:07:44.872471Z","iopub.execute_input":"2024-04-10T09:07:44.872892Z","iopub.status.idle":"2024-04-10T09:09:18.932242Z","shell.execute_reply.started":"2024-04-10T09:07:44.872862Z","shell.execute_reply":"2024-04-10T09:09:18.930902Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_selected.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:10:13.753946Z","iopub.execute_input":"2024-04-10T09:10:13.754830Z","iopub.status.idle":"2024-04-10T09:10:13.762700Z","shell.execute_reply.started":"2024-04-10T09:10:13.754784Z","shell.execute_reply":"2024-04-10T09:10:13.761166Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(7999, 640)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Classifier","metadata":{}},{"cell_type":"code","source":"# Perform Fuzzy C-Means clustering using skfuzzy\ncntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(X_selected.T, c=2,\n                                                 m=2.5, error=0.005, maxiter=5000)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:10:17.988683Z","iopub.execute_input":"2024-04-10T09:10:17.989527Z","iopub.status.idle":"2024-04-10T09:10:19.136824Z","shell.execute_reply.started":"2024-04-10T09:10:17.989493Z","shell.execute_reply":"2024-04-10T09:10:19.135216Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Get predictions","metadata":{}},{"cell_type":"code","source":"# Transform the data\nX_test_selected = mi_selector.fit_transform(X_test, y_test)\n# Predict using the test set\nu_test, _, _, _, _, _ = fuzz.cluster.cmeans_predict(X_test_selected.T, cntr, m=2.5, error=0.005, maxiter=5000)\nprint(u_test)\n\n# Get the most likely class for each sample\ny_pred = np.argmax(u_test, axis=0)\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:10:27.450597Z","iopub.execute_input":"2024-04-10T09:10:27.450984Z","iopub.status.idle":"2024-04-10T09:10:52.045887Z","shell.execute_reply.started":"2024-04-10T09:10:27.450955Z","shell.execute_reply":"2024-04-10T09:10:52.045010Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[[0.49993201 0.49988511 0.49989022 ... 0.49998204 0.49979097 0.49979846]\n [0.50006799 0.50011489 0.50010978 ... 0.50001796 0.50020903 0.50020154]]\n[1 1 1 ... 1 1 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize=None)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, fmt=\"d\", annot=True, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Calculate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:11:11.724601Z","iopub.execute_input":"2024-04-10T09:11:11.725049Z","iopub.status.idle":"2024-04-10T09:11:12.409594Z","shell.execute_reply.started":"2024-04-10T09:11:11.725010Z","shell.execute_reply":"2024-04-10T09:11:12.408076Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.85      0.03      0.05      1808\n           1       0.09      0.95      0.17       192\n\n    accuracy                           0.12      2000\n   macro avg       0.47      0.49      0.11      2000\nweighted avg       0.78      0.12      0.07      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_test, y_pred)\ntest_precision = precision_score(y_test, y_pred, average='macro')\ntest_recall = recall_score(y_test, y_pred, average='macro')\ntest_f1 = f1_score(y_test, y_pred, average='macro')\ntest_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"Accuracy:\", test_accuracy)\nprint('Precison:', test_precision)\nprint('Recall:', test_recall)\nprint('F1 Score:', test_f1)\nprint('AUC:', test_auc)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:12:15.639367Z","iopub.execute_input":"2024-04-10T09:12:15.639802Z","iopub.status.idle":"2024-04-10T09:12:15.662643Z","shell.execute_reply.started":"2024-04-10T09:12:15.639768Z","shell.execute_reply":"2024-04-10T09:12:15.661079Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Accuracy: 0.117\nPrecison: 0.47216494845360824\nRecall: 0.49066648230088494\nF1 Score: 0.11313682400536737\nAUC: 0.49066648230088494\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}