{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7822434,"sourceType":"datasetVersion","datasetId":4583334},{"sourceId":8080013,"sourceType":"datasetVersion","datasetId":4765536}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\nfrom sklearn.utils import shuffle\nfrom skimage.io import imread\nimport PIL\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision import models\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# from torchsampler import ImbalancedDatasetSampler\n# from torchmetrics.functional import auroc, precision, recall, f1_score, precision_recall_curve\nimport albumentations as albu\nimport albumentations.pytorch\nimport matplotlib.pyplot as plt\nimport torchmetrics\nimport timm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-11T08:02:45.036137Z","iopub.execute_input":"2024-04-11T08:02:45.036623Z","iopub.status.idle":"2024-04-11T08:03:07.862989Z","shell.execute_reply.started":"2024-04-11T08:02:45.036585Z","shell.execute_reply":"2024-04-11T08:03:07.861115Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:10.250958Z","iopub.execute_input":"2024-04-11T08:03:10.251428Z","iopub.status.idle":"2024-04-11T08:03:10.262830Z","shell.execute_reply.started":"2024-04-11T08:03:10.251385Z","shell.execute_reply":"2024-04-11T08:03:10.260879Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocess data","metadata":{}},{"cell_type":"code","source":"def preprocess_df(data_dir):\n    df = pd.read_csv(os.path.join(data_dir,'breast-level_annotations.csv'))\n    \n#     df['img_path'] = f\"{data_dir}/png/png/{df['study_id']}/{df['image_id']}.png\"\n    \n    df['malignancy_label'] = df['breast_birads']\n    # Define positive and negatives based on BI-RADS categories\n    df.loc[df['malignancy_label'] == 'BI-RADS 1', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 2', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 3', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 4', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 5', 'malignancy_label'] = 1\n\n    # Use pre-defined splits to separate data into development and testing\n    train_df = df[df['split'] == 'training']\n    test_df = df[df['split'] == 'test']\n    \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n\ndef show_image_pair(image1, image2):\n    fig = plt.figure(figsize=(10, 20))\n    fig.add_subplot(1,2,1)\n    plt.imshow(image1)\n    fig.add_subplot(1,2, 2)\n    plt.imshow(image2)\n    plt.show()\n\ndef test_dataset(df, idx=0):\n    dataset = Dataset(df, data_dir)\n    \n    img_path = os.path.join(data_dir, 'png/png', dataset.df.iloc[idx]['study_id'], dataset.df.iloc[idx]['image_id'] + '.png')\n    image1 = PIL.Image.open(img_path).convert('RGB')\n\n    tensor = dataset[idx].squeeze()\n    image2 = torchvision.transforms.ToPILImage()(tensor)\n\n    show_image_pair(image1, image2)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:11.645246Z","iopub.execute_input":"2024-04-11T08:03:11.645694Z","iopub.status.idle":"2024-04-11T08:03:11.661404Z","shell.execute_reply.started":"2024-04-11T08:03:11.645630Z","shell.execute_reply":"2024-04-11T08:03:11.660226Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/full-fullsize/'\n\ntrain_df, test_df = preprocess_df(data_dir)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:12.746231Z","iopub.execute_input":"2024-04-11T08:03:12.746818Z","iopub.status.idle":"2024-04-11T08:03:13.018234Z","shell.execute_reply.started":"2024-04-11T08:03:12.746777Z","shell.execute_reply":"2024-04-11T08:03:13.016157Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                               study_id                         series_id  \\\n0      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n1      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n2      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n3      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n4      8269f5971eaca3e5d3772d1796e6bd7a  d931832a0815df082c085b6e09d20aac   \n...                                 ...                               ...   \n15995  f2093a752e6b44df5990f5fd38c99dd2  2b1b2b8f48abab9819c0b3d091e152ee   \n15996  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n15997  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n15998  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n15999  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n\n                               image_id laterality view_position  height  \\\n0      d8125545210c08e1b1793a5af6458ee2          L            CC    3518   \n1      290c658f4e75a3f83ec78a847414297c          L           MLO    3518   \n2      cd0fc7bc53ac632a11643ac4cc91002a          R            CC    3518   \n3      71638b1e853799f227492bfb08a01491          R           MLO    3518   \n4      dd9ce3288c0773e006a294188aadba8e          L            CC    3518   \n...                                 ...        ...           ...     ...   \n15995  ea732154d149f619b20070b78060ae65          R            CC    2812   \n15996  4689616c3d0b46fcba7a771107730791          R            CC    3580   \n15997  3c22491bcf1d0b004715c28d80981cdd          L            CC    3580   \n15998  d443b9725e331b8b27589aa725597801          R           MLO    3580   \n15999  45c1239cc36b0e672f0072707fd05c6f          L           MLO    3580   \n\n       width breast_birads breast_density     split malignancy_label  \n0       2800     BI-RADS 2      DENSITY C  training                0  \n1       2800     BI-RADS 2      DENSITY C  training                0  \n2       2800     BI-RADS 2      DENSITY C  training                0  \n3       2800     BI-RADS 2      DENSITY C  training                0  \n4       2800     BI-RADS 1      DENSITY C  training                0  \n...      ...           ...            ...       ...              ...  \n15995   2012     BI-RADS 2      DENSITY C  training                0  \n15996   2702     BI-RADS 2      DENSITY C  training                0  \n15997   2702     BI-RADS 2      DENSITY C  training                0  \n15998   2686     BI-RADS 2      DENSITY C  training                0  \n15999   2670     BI-RADS 2      DENSITY C  training                0  \n\n[16000 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>series_id</th>\n      <th>image_id</th>\n      <th>laterality</th>\n      <th>view_position</th>\n      <th>height</th>\n      <th>width</th>\n      <th>breast_birads</th>\n      <th>breast_density</th>\n      <th>split</th>\n      <th>malignancy_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b8d273e8601f348d3664778dae0e7e0b</td>\n      <td>b36517b9cbbcfd286a7ae04f643af97a</td>\n      <td>d8125545210c08e1b1793a5af6458ee2</td>\n      <td>L</td>\n      <td>CC</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b8d273e8601f348d3664778dae0e7e0b</td>\n      <td>b36517b9cbbcfd286a7ae04f643af97a</td>\n      <td>290c658f4e75a3f83ec78a847414297c</td>\n      <td>L</td>\n      <td>MLO</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b8d273e8601f348d3664778dae0e7e0b</td>\n      <td>b36517b9cbbcfd286a7ae04f643af97a</td>\n      <td>cd0fc7bc53ac632a11643ac4cc91002a</td>\n      <td>R</td>\n      <td>CC</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b8d273e8601f348d3664778dae0e7e0b</td>\n      <td>b36517b9cbbcfd286a7ae04f643af97a</td>\n      <td>71638b1e853799f227492bfb08a01491</td>\n      <td>R</td>\n      <td>MLO</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8269f5971eaca3e5d3772d1796e6bd7a</td>\n      <td>d931832a0815df082c085b6e09d20aac</td>\n      <td>dd9ce3288c0773e006a294188aadba8e</td>\n      <td>L</td>\n      <td>CC</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 1</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15995</th>\n      <td>f2093a752e6b44df5990f5fd38c99dd2</td>\n      <td>2b1b2b8f48abab9819c0b3d091e152ee</td>\n      <td>ea732154d149f619b20070b78060ae65</td>\n      <td>R</td>\n      <td>CC</td>\n      <td>2812</td>\n      <td>2012</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15996</th>\n      <td>b3c8969cd2accfa4dbb2aece1f7158ab</td>\n      <td>69d7f07ea04572dad5e5aa62fbcfc4b7</td>\n      <td>4689616c3d0b46fcba7a771107730791</td>\n      <td>R</td>\n      <td>CC</td>\n      <td>3580</td>\n      <td>2702</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15997</th>\n      <td>b3c8969cd2accfa4dbb2aece1f7158ab</td>\n      <td>69d7f07ea04572dad5e5aa62fbcfc4b7</td>\n      <td>3c22491bcf1d0b004715c28d80981cdd</td>\n      <td>L</td>\n      <td>CC</td>\n      <td>3580</td>\n      <td>2702</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15998</th>\n      <td>b3c8969cd2accfa4dbb2aece1f7158ab</td>\n      <td>69d7f07ea04572dad5e5aa62fbcfc4b7</td>\n      <td>d443b9725e331b8b27589aa725597801</td>\n      <td>R</td>\n      <td>MLO</td>\n      <td>3580</td>\n      <td>2686</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15999</th>\n      <td>b3c8969cd2accfa4dbb2aece1f7158ab</td>\n      <td>69d7f07ea04572dad5e5aa62fbcfc4b7</td>\n      <td>45c1239cc36b0e672f0072707fd05c6f</td>\n      <td>L</td>\n      <td>MLO</td>\n      <td>3580</td>\n      <td>2670</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>16000 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# for idx in [random.choice(range(100)) for i in range(3)]:\n#     test_dataset(train_df, idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract feature","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\n\nclass Img2Vec():\n    RESNET_OUTPUT_SIZES = {\n        'resnet18': 512,\n        'resnet34': 512,\n        'resnet50': 2048,\n        'resnet101': 2048,\n        'resnet152': 2048\n    }\n\n    EFFICIENTNET_OUTPUT_SIZES = {\n        'efficientnet_b0': 1280,\n        'efficientnet_b1': 1280,\n        'efficientnet_b2': 1408,\n        'efficientnet_b3': 1536,\n        'efficientnet_b4': 1792,\n        'efficientnet_b5': 2048,\n        'efficientnet_b6': 2304,\n        'efficientnet_b7': 2560\n    }\n\n    def __init__(self, model='resnet-18', layer='default', layer_output_size=512):\n       \n        self.layer_output_size = layer_output_size\n        self.model_name = model\n\n        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n\n        self.model = self.model.to(device)\n\n        self.model.eval()\n\n        self.scaler = transforms.Resize((224, 224))\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n\n    def get_vec(self, img, tensor=False):\n        \"\"\" Get vector embedding from PIL image\n        :param img: PIL Image or list of PIL Images\n        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n        :returns: Numpy ndarray\n        \"\"\"\n        if type(img) == list:\n            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n            images = torch.stack(a).to(device)\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(len(img), self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(images)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[:, :]\n                elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[:, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[:, :, 0, 0]\n        else:\n            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(device)\n\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(1, self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(1, self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(image)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[0, :]\n                elif self.model_name == 'densenet':\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[0, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[0, :, 0, 0]\n\n    def _get_model_and_layer(self, model_name, layer):\n        \"\"\" Internal method for getting layer from model\n        :param model_name: model name such as 'resnet-18'\n        :param layer: layer as a string for resnet-18 or int for alexnet\n        :returns: pytorch model, selected layer\n        \"\"\"\n\n        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n            model = getattr(models, model_name)(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n            else:\n                layer = model._modules.get(layer)\n            return model, layer\n        elif model_name == 'resnet-18':\n            model = models.resnet18(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = 512\n            else:\n                layer = model._modules.get(layer)\n\n            return model, layer\n\n        elif model_name == 'alexnet':\n            model = models.alexnet(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'vgg':\n            # VGG-11\n            model = models.vgg11_bn(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = model.classifier[-1].in_features # should be 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'densenet':\n            # Densenet-121\n            model = models.densenet121(pretrained=True)\n            if layer == 'default':\n                layer = model.features[-1]\n                self.layer_output_size = model.classifier.in_features # should be 1024\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        elif \"efficientnet\" in model_name:\n            # efficientnet-b0 ~ efficientnet-b7\n            if model_name == \"efficientnet_b0\":\n                model = models.efficientnet_b0(pretrained=True)\n            elif model_name == \"efficientnet_b1\":\n                model = models.efficientnet_b1(pretrained=True)\n            elif model_name == \"efficientnet_b2\":\n                model = models.efficientnet_b2(pretrained=True)\n            elif model_name == \"efficientnet_b3\":\n                model = models.efficientnet_b3(pretrained=True)\n            elif model_name == \"efficientnet_b4\":\n                model = models.efficientnet_b4(pretrained=True)\n            elif model_name == \"efficientnet_b5\":\n                model = models.efficientnet_b5(pretrained=True)\n            elif model_name == \"efficientnet_b6\":\n                model = models.efficientnet_b6(pretrained=True)\n            elif model_name == \"efficientnet_b7\":\n                model = models.efficientnet_b7(pretrained=True)\n            else:\n                raise KeyError('Un support %s.' % model_name)\n\n            if layer == 'default':\n                layer = model.features\n                self.layer_output_size = self.EFFICIENTNET_OUTPUT_SIZES[model_name]\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        else:\n            raise KeyError('Model %s was not found' % model_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:16.242949Z","iopub.execute_input":"2024-04-11T08:03:16.243477Z","iopub.status.idle":"2024-04-11T08:03:16.677721Z","shell.execute_reply.started":"2024-04-11T08:03:16.243440Z","shell.execute_reply":"2024-04-11T08:03:16.675272Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def extract_img_feature(df, data_dir, model, vec_length):\n    img2vec = Img2Vec(model=model, \n                      layer_output_size=vec_length)\n    \n    vec_mat = np.zeros((len(df) , vec_length))\n\n    for idx, row in df.iterrows():\n        img_path = os.path.join(data_dir, 'png/png', row['study_id'], row['image_id'] + '.png')\n        img = PIL.Image.open(img_path).convert('RGB')\n        if row['laterality'] == 'L':\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n        vec = img2vec.get_vec(img)\n        vec_mat[idx, :] = vec\n        \n    features_df = pd.DataFrame(vec_mat)\n    features_df = features_df.add_prefix('feature_')\n    features_df['label'] = df['malignancy_label']\n    features_df['view_position'] = df['view_position']\n    features_df['laterality'] = df['laterality']\n    features_df['study_id'] = df['study_id']\n    \n    return features_df","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:17.312846Z","iopub.execute_input":"2024-04-11T08:03:17.313314Z","iopub.status.idle":"2024-04-11T08:03:17.326585Z","shell.execute_reply.started":"2024-04-11T08:03:17.313282Z","shell.execute_reply":"2024-04-11T08:03:17.324690Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model_name = 'efficientnet_b0'\nnum_features = 1280\n\nfeatures_train = extract_img_feature(df=train_df, \n                                     data_dir=data_dir,\n                                     model=model_name, \n                                     vec_length=num_features\n                                     )\n\nfeatures_test = extract_img_feature(df=test_df, \n                                    data_dir=data_dir,\n                                    model=model_name, \n                                    vec_length=num_features\n                                    )","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:18.847031Z","iopub.execute_input":"2024-04-11T08:03:18.847546Z","iopub.status.idle":"2024-04-11T08:48:40.911977Z","shell.execute_reply.started":"2024-04-11T08:03:18.847508Z","shell.execute_reply":"2024-04-11T08:48:40.910465Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 63.5MB/s]\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nfeatures_train.to_csv(f'{save_dir}features_train_{model_name}.csv', index=False)\nfeatures_test.to_csv(f'{save_dir}features_test_{model_name}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:58:41.783425Z","iopub.execute_input":"2024-04-11T08:58:41.784021Z","iopub.status.idle":"2024-04-11T08:59:48.326204Z","shell.execute_reply.started":"2024-04-11T08:58:41.783979Z","shell.execute_reply":"2024-04-11T08:59:48.324907Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_CC = features_train[features_train['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntrain_MLO = features_train[features_train['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\ntest_CC = features_test[features_test['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntest_MLO = features_test[features_test['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\nconcat_features_train = train_CC.merge(train_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))\nconcat_features_test = test_CC.merge(test_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_features_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat_features_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nconcat_features_train.to_csv(f'{save_dir}concat_features_train_{model_name}.csv', index=False)\nconcat_features_test.to_csv(f'{save_dir}concat_features_test_{model_name}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NaN\nprint(concat_features_train.isna().any().any())\nprint(concat_features_test.isna().any().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classify Model","metadata":{}},{"cell_type":"code","source":"!pip install scikit-fuzzy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import skfuzzy as fuzz\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss, TomekLinks, RandomUnderSampler\nfrom sklearn.metrics import roc_curve,precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n\nimport os\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:23:28.675515Z","iopub.execute_input":"2024-04-11T09:23:28.676063Z","iopub.status.idle":"2024-04-11T09:23:30.957339Z","shell.execute_reply.started":"2024-04-11T09:23:28.676017Z","shell.execute_reply":"2024-04-11T09:23:30.950751Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"concat_features_train = pd.read_csv('/kaggle/input/vin-feature/concat_features_train_efficientnet_b0.csv')\nconcat_features_test = pd.read_csv('/kaggle/input/vin-feature/concat_features_test_efficientnet_b0.csv')\n\nconcat_features_train","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:23:32.440562Z","iopub.execute_input":"2024-04-11T09:23:32.441341Z","iopub.status.idle":"2024-04-11T09:23:41.855000Z","shell.execute_reply.started":"2024-04-11T09:23:32.441294Z","shell.execute_reply":"2024-04-11T09:23:41.853748Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1271_MLO  feature_1272_MLO  feature_1273_MLO  \\\n0     ...         -0.227731         -0.069607         -0.245696   \n1     ...         -0.267062         -0.045241          0.110348   \n2     ...         -0.229044         -0.099550         -0.273775   \n3     ...         -0.215473         -0.081372         -0.271739   \n4     ...         -0.171533         -0.156897         -0.103236   \n...   ...               ...               ...               ...   \n7994  ...         -0.224987         -0.070554         -0.268184   \n7995  ...         -0.248899         -0.061061         -0.270626   \n7996  ...         -0.119826         -0.057346         -0.202150   \n7997  ...         -0.125756         -0.156107         -0.262996   \n7998  ...         -0.056127         -0.057476         -0.211795   \n\n      feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  feature_1277_MLO  \\\n0            -0.261152          0.777141         -0.277788          0.419119   \n1            -0.278391          1.332054         -0.129526         -0.214615   \n2            -0.276369         -0.198178         -0.078316         -0.242274   \n3            -0.271640         -0.221536         -0.068043         -0.256326   \n4             0.013147         -0.208144         -0.277218         -0.196322   \n...                ...               ...               ...               ...   \n7994         -0.209462         -0.272872         -0.250021         -0.247301   \n7995          0.856457         -0.182576         -0.219728         -0.203649   \n7996         -0.243588         -0.253669         -0.193268         -0.087486   \n7997         -0.273179          1.193740         -0.097256         -0.161601   \n7998         -0.274628          0.907126         -0.173844         -0.074086   \n\n      feature_1278_MLO  feature_1279_MLO  label  \n0            -0.137479         -0.125389      0  \n1            -0.171379         -0.265228      0  \n2            -0.114382         -0.211536      0  \n3            -0.115784         -0.235606      0  \n4            -0.128612         -0.212763      0  \n...                ...               ...    ...  \n7994         -0.076688         -0.267579      0  \n7995         -0.137060         -0.168686      0  \n7996         -0.161627         -0.272322      0  \n7997         -0.047568         -0.126142      0  \n7998         -0.075687         -0.064506      0  \n\n[7999 rows x 2563 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2563 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = concat_features_train.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_train = np.array(concat_features_train['label']).astype(int)\nX_test = concat_features_test.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_test = np.array(concat_features_test['label']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:23:51.199134Z","iopub.execute_input":"2024-04-11T09:23:51.199564Z","iopub.status.idle":"2024-04-11T09:23:51.937030Z","shell.execute_reply.started":"2024-04-11T09:23:51.199521Z","shell.execute_reply":"2024-04-11T09:23:51.934381Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:23:53.303137Z","iopub.execute_input":"2024-04-11T09:23:53.304114Z","iopub.status.idle":"2024-04-11T09:23:53.342627Z","shell.execute_reply.started":"2024-04-11T09:23:53.304073Z","shell.execute_reply":"2024-04-11T09:23:53.341722Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1270_MLO  feature_1271_MLO  feature_1272_MLO  \\\n0     ...         -0.219428         -0.227731         -0.069607   \n1     ...         -0.066708         -0.267062         -0.045241   \n2     ...         -0.269503         -0.229044         -0.099550   \n3     ...         -0.274468         -0.215473         -0.081372   \n4     ...         -0.238900         -0.171533         -0.156897   \n...   ...               ...               ...               ...   \n7994  ...         -0.276627         -0.224987         -0.070554   \n7995  ...         -0.278353         -0.248899         -0.061061   \n7996  ...         -0.278436         -0.119826         -0.057346   \n7997  ...         -0.251418         -0.125756         -0.156107   \n7998  ...         -0.166142         -0.056127         -0.057476   \n\n      feature_1273_MLO  feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  \\\n0            -0.245696         -0.261152          0.777141         -0.277788   \n1             0.110348         -0.278391          1.332054         -0.129526   \n2            -0.273775         -0.276369         -0.198178         -0.078316   \n3            -0.271739         -0.271640         -0.221536         -0.068043   \n4            -0.103236          0.013147         -0.208144         -0.277218   \n...                ...               ...               ...               ...   \n7994         -0.268184         -0.209462         -0.272872         -0.250021   \n7995         -0.270626          0.856457         -0.182576         -0.219728   \n7996         -0.202150         -0.243588         -0.253669         -0.193268   \n7997         -0.262996         -0.273179          1.193740         -0.097256   \n7998         -0.211795         -0.274628          0.907126         -0.173844   \n\n      feature_1277_MLO  feature_1278_MLO  feature_1279_MLO  \n0             0.419119         -0.137479         -0.125389  \n1            -0.214615         -0.171379         -0.265228  \n2            -0.242274         -0.114382         -0.211536  \n3            -0.256326         -0.115784         -0.235606  \n4            -0.196322         -0.128612         -0.212763  \n...                ...               ...               ...  \n7994         -0.247301         -0.076688         -0.267579  \n7995         -0.203649         -0.137060         -0.168686  \n7996         -0.087486         -0.161627         -0.272322  \n7997         -0.161601         -0.047568         -0.126142  \n7998         -0.074086         -0.075687         -0.064506  \n\n[7999 rows x 2560 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1270_MLO</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.219428</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.066708</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.269503</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.274468</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.238900</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.276627</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.278353</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.278436</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.251418</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.166142</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2560 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Define the SMOTETomek resampling technique\nsmote_tomek = SMOTETomek(sampling_strategy=0.5,\n                         random_state=42)\n\n# Resample the training data using SMOTETomek\nX_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:23:54.273318Z","iopub.execute_input":"2024-04-11T09:23:54.274106Z","iopub.status.idle":"2024-04-11T09:24:12.521647Z","shell.execute_reply.started":"2024-04-11T09:23:54.274058Z","shell.execute_reply":"2024-04-11T09:24:12.520522Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"num_select_feature = int(X_train.shape[1]*0.1)\nmi_selector = SelectKBest(mutual_info_classif, k=num_select_feature)\n\n# Transform the data\nX_selected = mi_selector.fit_transform(X_resampled, y_resampled)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:24:55.801393Z","iopub.execute_input":"2024-04-11T09:24:55.801876Z","iopub.status.idle":"2024-04-11T09:27:10.107479Z","shell.execute_reply.started":"2024-04-11T09:24:55.801840Z","shell.execute_reply":"2024-04-11T09:27:10.106106Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_selected.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:27:12.579794Z","iopub.execute_input":"2024-04-11T09:27:12.580316Z","iopub.status.idle":"2024-04-11T09:27:12.589554Z","shell.execute_reply.started":"2024-04-11T09:27:12.580274Z","shell.execute_reply":"2024-04-11T09:27:12.588025Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(10838, 256)"},"metadata":{}}]},{"cell_type":"code","source":"X_train_full = X_selected\ny_train_full = y_resampled\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:27:28.619869Z","iopub.execute_input":"2024-04-11T09:27:28.621432Z","iopub.status.idle":"2024-04-11T09:27:28.685720Z","shell.execute_reply.started":"2024-04-11T09:27:28.621381Z","shell.execute_reply":"2024-04-11T09:27:28.684231Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Classifier","metadata":{}},{"cell_type":"code","source":"def get_models():\n    models = list()\n    models.append(('lr', LogisticRegression(max_iter=5000)))\n    models.append(('rf', RandomForestClassifier(random_state=42)))\n    models.append(('bayes', GaussianNB()))\n    return models\n\n\n# evaluate each base model\ndef evaluate_models(models, X_train, X_val, y_train, y_val):\n    # fit and evaluate the models\n    scores = list()\n    for name, model in models:\n        # fit the model\n        model.fit(X_train, y_train)\n        # evaluate the model\n        yhat = model.predict(X_val)\n        auc = roc_auc_score(y_val, yhat)\n        # store the performance\n        scores.append(auc)\n        # report model performance\n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:27:31.659381Z","iopub.execute_input":"2024-04-11T09:27:31.662889Z","iopub.status.idle":"2024-04-11T09:27:31.673058Z","shell.execute_reply.started":"2024-04-11T09:27:31.662829Z","shell.execute_reply":"2024-04-11T09:27:31.671306Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"models = get_models()\n# fit and evaluate each model\nscores = evaluate_models(models, X_train, X_val, y_train, y_val)\nscores","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:27:32.974193Z","iopub.execute_input":"2024-04-11T09:27:32.974608Z","iopub.status.idle":"2024-04-11T09:27:49.579378Z","shell.execute_reply.started":"2024-04-11T09:27:32.974577Z","shell.execute_reply":"2024-04-11T09:27:49.578185Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"[0.5622206363315816, 0.891478101688903, 0.5601345801811997]"},"metadata":{}}]},{"cell_type":"code","source":"# create the ensemble\nensemble = VotingClassifier(estimators=models, voting='soft', weights=scores)\n# fit the ensemble on the training dataset\nensemble.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:27:49.581394Z","iopub.execute_input":"2024-04-11T09:27:49.581788Z","iopub.status.idle":"2024-04-11T09:28:06.202035Z","shell.execute_reply.started":"2024-04-11T09:27:49.581756Z","shell.execute_reply":"2024-04-11T09:28:06.200778Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"VotingClassifier(estimators=[('lr', LogisticRegression(max_iter=5000)),\n                             ('rf', RandomForestClassifier(random_state=42)),\n                             ('bayes', GaussianNB())],\n                 voting='soft',\n                 weights=[0.5622206363315816, 0.891478101688903,\n                          0.5601345801811997])","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=5000)),\n                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n                             (&#x27;bayes&#x27;, GaussianNB())],\n                 voting=&#x27;soft&#x27;,\n                 weights=[0.5622206363315816, 0.891478101688903,\n                          0.5601345801811997])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=5000)),\n                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n                             (&#x27;bayes&#x27;, GaussianNB())],\n                 voting=&#x27;soft&#x27;,\n                 weights=[0.5622206363315816, 0.891478101688903,\n                          0.5601345801811997])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=5000)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>bayes</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Get predictions","metadata":{}},{"cell_type":"code","source":"# # Transform the data\nX_test_selected = mi_selector.fit_transform(X_test, y_test)\n\n# Predict on the test set\ny_pred = ensemble.predict(X_test_selected)\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:29:29.833911Z","iopub.execute_input":"2024-04-11T09:29:29.834431Z","iopub.status.idle":"2024-04-11T09:29:53.987703Z","shell.execute_reply.started":"2024-04-11T09:29:29.834391Z","shell.execute_reply":"2024-04-11T09:29:53.983503Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[0 0 0 ... 0 1 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize=None)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, fmt=\"d\", annot=True, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Calculate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:29:57.204212Z","iopub.execute_input":"2024-04-11T09:29:57.206238Z","iopub.status.idle":"2024-04-11T09:29:57.708266Z","shell.execute_reply.started":"2024-04-11T09:29:57.206187Z","shell.execute_reply":"2024-04-11T09:29:57.706307Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.90      0.99      0.94      1808\n           1       0.00      0.00      0.00       192\n\n    accuracy                           0.89      2000\n   macro avg       0.45      0.49      0.47      2000\nweighted avg       0.82      0.89      0.85      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_test, y_pred)\ntest_precision = precision_score(y_test, y_pred, average='macro')\ntest_recall = recall_score(y_test, y_pred, average='macro')\ntest_f1 = f1_score(y_test, y_pred, average='macro')\ntest_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"Accuracy:\", test_accuracy)\nprint('Precison:', test_precision)\nprint('Recall:', test_recall)\nprint('F1 Score:', test_f1)\nprint('AUC:', test_auc)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:30:03.624502Z","iopub.execute_input":"2024-04-11T09:30:03.625009Z","iopub.status.idle":"2024-04-11T09:30:03.646723Z","shell.execute_reply.started":"2024-04-11T09:30:03.624972Z","shell.execute_reply":"2024-04-11T09:30:03.645284Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy: 0.8905\nPrecison: 0.4513431322858591\nRecall: 0.492533185840708\nF1 Score: 0.47103940756413654\nAUC: 0.492533185840708\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}