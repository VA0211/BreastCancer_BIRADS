{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7822434,"sourceType":"datasetVersion","datasetId":4583334},{"sourceId":8075321,"sourceType":"datasetVersion","datasetId":4765536}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\nfrom sklearn.utils import shuffle\nfrom skimage.io import imread\nimport PIL\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision import models\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# from torchsampler import ImbalancedDatasetSampler\n# from torchmetrics.functional import auroc, precision, recall, f1_score, precision_recall_curve\nimport albumentations as albu\nimport albumentations.pytorch\nimport matplotlib.pyplot as plt\nimport torchmetrics\nimport timm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess data","metadata":{}},{"cell_type":"code","source":"def preprocess_df(data_dir):\n    df = pd.read_csv(os.path.join(data_dir,'breast-level_annotations.csv'))\n    \n#     df['img_path'] = f\"{data_dir}/png/png/{df['study_id']}/{df['image_id']}.png\"\n    \n    df['malignancy_label'] = df['breast_birads']\n    # Define positive and negatives based on BI-RADS categories\n    df.loc[df['malignancy_label'] == 'BI-RADS 1', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 2', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 3', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 4', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 5', 'malignancy_label'] = 1\n\n    # Use pre-defined splits to separate data into development and testing\n    train_df = df[df['split'] == 'training']\n    test_df = df[df['split'] == 'test']\n    \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n\ndef show_image_pair(image1, image2):\n    fig = plt.figure(figsize=(10, 20))\n    fig.add_subplot(1,2,1)\n    plt.imshow(image1)\n    fig.add_subplot(1,2, 2)\n    plt.imshow(image2)\n    plt.show()\n\ndef test_dataset(df, idx=0):\n    dataset = Dataset(df, data_dir)\n    \n    img_path = os.path.join(data_dir, 'png/png', dataset.df.iloc[idx]['study_id'], dataset.df.iloc[idx]['image_id'] + '.png')\n    image1 = PIL.Image.open(img_path).convert('RGB')\n\n    tensor = dataset[idx].squeeze()\n    image2 = torchvision.transforms.ToPILImage()(tensor)\n\n    show_image_pair(image1, image2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/full-fullsize/'\n\ntrain_df, test_df = preprocess_df(data_dir)\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx in [random.choice(range(100)) for i in range(3)]:\n#     test_dataset(train_df, idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract feature","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\n\nclass Img2Vec():\n    RESNET_OUTPUT_SIZES = {\n        'resnet18': 512,\n        'resnet34': 512,\n        'resnet50': 2048,\n        'resnet101': 2048,\n        'resnet152': 2048\n    }\n\n    EFFICIENTNET_OUTPUT_SIZES = {\n        'efficientnet_b0': 1280,\n        'efficientnet_b1': 1280,\n        'efficientnet_b2': 1408,\n        'efficientnet_b3': 1536,\n        'efficientnet_b4': 1792,\n        'efficientnet_b5': 2048,\n        'efficientnet_b6': 2304,\n        'efficientnet_b7': 2560\n    }\n\n    def __init__(self, model='resnet-18', layer='default', layer_output_size=512):\n       \n        self.layer_output_size = layer_output_size\n        self.model_name = model\n\n        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n\n        self.model = self.model.to(device)\n\n        self.model.eval()\n\n        self.scaler = transforms.Resize((224, 224))\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n\n    def get_vec(self, img, tensor=False):\n        \"\"\" Get vector embedding from PIL image\n        :param img: PIL Image or list of PIL Images\n        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n        :returns: Numpy ndarray\n        \"\"\"\n        if type(img) == list:\n            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n            images = torch.stack(a).to(device)\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(len(img), self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(images)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[:, :]\n                elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[:, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[:, :, 0, 0]\n        else:\n            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(device)\n\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(1, self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(1, self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(image)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[0, :]\n                elif self.model_name == 'densenet':\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[0, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[0, :, 0, 0]\n\n    def _get_model_and_layer(self, model_name, layer):\n        \"\"\" Internal method for getting layer from model\n        :param model_name: model name such as 'resnet-18'\n        :param layer: layer as a string for resnet-18 or int for alexnet\n        :returns: pytorch model, selected layer\n        \"\"\"\n\n        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n            model = getattr(models, model_name)(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n            else:\n                layer = model._modules.get(layer)\n            return model, layer\n        elif model_name == 'resnet-18':\n            model = models.resnet18(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = 512\n            else:\n                layer = model._modules.get(layer)\n\n            return model, layer\n\n        elif model_name == 'alexnet':\n            model = models.alexnet(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'vgg':\n            # VGG-11\n            model = models.vgg11_bn(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = model.classifier[-1].in_features # should be 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'densenet':\n            # Densenet-121\n            model = models.densenet121(pretrained=True)\n            if layer == 'default':\n                layer = model.features[-1]\n                self.layer_output_size = model.classifier.in_features # should be 1024\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        elif \"efficientnet\" in model_name:\n            # efficientnet-b0 ~ efficientnet-b7\n            if model_name == \"efficientnet_b0\":\n                model = models.efficientnet_b0(pretrained=True)\n            elif model_name == \"efficientnet_b1\":\n                model = models.efficientnet_b1(pretrained=True)\n            elif model_name == \"efficientnet_b2\":\n                model = models.efficientnet_b2(pretrained=True)\n            elif model_name == \"efficientnet_b3\":\n                model = models.efficientnet_b3(pretrained=True)\n            elif model_name == \"efficientnet_b4\":\n                model = models.efficientnet_b4(pretrained=True)\n            elif model_name == \"efficientnet_b5\":\n                model = models.efficientnet_b5(pretrained=True)\n            elif model_name == \"efficientnet_b6\":\n                model = models.efficientnet_b6(pretrained=True)\n            elif model_name == \"efficientnet_b7\":\n                model = models.efficientnet_b7(pretrained=True)\n            else:\n                raise KeyError('Un support %s.' % model_name)\n\n            if layer == 'default':\n                layer = model.features\n                self.layer_output_size = self.EFFICIENTNET_OUTPUT_SIZES[model_name]\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        else:\n            raise KeyError('Model %s was not found' % model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_img_feature(df, data_dir, model, vec_length):\n    img2vec = Img2Vec(model=model, \n                      layer_output_size=vec_length)\n    \n    vec_mat = np.zeros((len(df) , vec_length))\n\n    for idx, row in df.iterrows():\n        img_path = os.path.join(data_dir, 'png/png', row['study_id'], row['image_id'] + '.png')\n        img = PIL.Image.open(img_path).convert('RGB')\n        if row['laterality'] == 'L':\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n        vec = img2vec.get_vec(img)\n        vec_mat[idx, :] = vec\n        \n    features_df = pd.DataFrame(vec_mat)\n    features_df = features_df.add_prefix('feature_')\n    features_df['label'] = df['malignancy_label']\n    features_df['view_position'] = df['view_position']\n    features_df['laterality'] = df['laterality']\n    features_df['study_id'] = df['study_id']\n    \n    return features_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'efficientnet_b0'\nnum_features = 1280\n\nfeatures_train = extract_img_feature(df=train_df, \n                                     data_dir=data_dir,\n                                     model=model_name, \n                                     vec_length=num_features\n                                     )\n\nfeatures_test = extract_img_feature(df=test_df, \n                                    data_dir=data_dir,\n                                    model=model_name, \n                                    vec_length=num_features\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_CC = features_train[features_train['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntrain_MLO = features_train[features_train['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\ntest_CC = features_test[features_test['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntest_MLO = features_test[features_test['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\nconcat_features_train = train_CC.merge(train_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))\nconcat_features_test = test_CC.merge(test_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_features_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat_features_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nconcat_features_train.to_csv(f'{save_dir}concat_features_train_{model_name}.csv', index=False)\nconcat_features_test.to_csv(f'{save_dir}concat_features_test_{model_name}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NaN\nprint(concat_features_train.isna().any().any())\nprint(concat_features_test.isna().any().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classify Model","metadata":{}},{"cell_type":"code","source":"!pip install scikit-fuzzy","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:21:43.660093Z","iopub.execute_input":"2024-04-10T09:21:43.660524Z","iopub.status.idle":"2024-04-10T09:22:05.940254Z","shell.execute_reply.started":"2024-04-10T09:21:43.660490Z","shell.execute_reply":"2024-04-10T09:22:05.938983Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting scikit-fuzzy\n  Downloading scikit-fuzzy-0.4.2.tar.gz (993 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (1.26.4)\nRequirement already satisfied: scipy>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (1.11.4)\nRequirement already satisfied: networkx>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (3.2.1)\nBuilding wheels for collected packages: scikit-fuzzy\n  Building wheel for scikit-fuzzy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894077 sha256=186321e247d134795b92d1b3e1f84362077636cb8ed2a0ab33333ed9e415cdee\n  Stored in directory: /root/.cache/pip/wheels/4f/86/1b/dfd97134a2c8313e519bcebd95d3fedc7be7944db022094bc8\nSuccessfully built scikit-fuzzy\nInstalling collected packages: scikit-fuzzy\nSuccessfully installed scikit-fuzzy-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# import skfuzzy as fuzz\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss, TomekLinks, RandomUnderSampler\nfrom sklearn.metrics import roc_curve,precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\nimport os\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:22:36.467148Z","iopub.execute_input":"2024-04-10T15:22:36.467988Z","iopub.status.idle":"2024-04-10T15:22:39.918169Z","shell.execute_reply.started":"2024-04-10T15:22:36.467949Z","shell.execute_reply":"2024-04-10T15:22:39.917005Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"concat_features_train = pd.read_csv('/kaggle/input/vin-feature/concat_features_train_efficientnet_b0.csv')\nconcat_features_test = pd.read_csv('/kaggle/input/vin-feature/concat_features_test_efficientnet_b0.csv')\n\nconcat_features_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:19:44.188829Z","iopub.execute_input":"2024-04-10T16:19:44.189335Z","iopub.status.idle":"2024-04-10T16:19:53.483086Z","shell.execute_reply.started":"2024-04-10T16:19:44.189305Z","shell.execute_reply":"2024-04-10T16:19:53.481471Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1271_MLO  feature_1272_MLO  feature_1273_MLO  \\\n0     ...         -0.227731         -0.069607         -0.245696   \n1     ...         -0.267062         -0.045241          0.110348   \n2     ...         -0.229044         -0.099550         -0.273775   \n3     ...         -0.215473         -0.081372         -0.271739   \n4     ...         -0.171533         -0.156897         -0.103236   \n...   ...               ...               ...               ...   \n7994  ...         -0.224987         -0.070554         -0.268184   \n7995  ...         -0.248899         -0.061061         -0.270626   \n7996  ...         -0.119826         -0.057346         -0.202150   \n7997  ...         -0.125756         -0.156107         -0.262996   \n7998  ...         -0.056127         -0.057476         -0.211795   \n\n      feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  feature_1277_MLO  \\\n0            -0.261152          0.777141         -0.277788          0.419119   \n1            -0.278391          1.332054         -0.129526         -0.214615   \n2            -0.276369         -0.198178         -0.078316         -0.242274   \n3            -0.271640         -0.221536         -0.068043         -0.256326   \n4             0.013147         -0.208144         -0.277218         -0.196322   \n...                ...               ...               ...               ...   \n7994         -0.209462         -0.272872         -0.250021         -0.247301   \n7995          0.856457         -0.182576         -0.219728         -0.203649   \n7996         -0.243588         -0.253669         -0.193268         -0.087486   \n7997         -0.273179          1.193740         -0.097256         -0.161601   \n7998         -0.274628          0.907126         -0.173844         -0.074086   \n\n      feature_1278_MLO  feature_1279_MLO  label  \n0            -0.137479         -0.125389      0  \n1            -0.171379         -0.265228      0  \n2            -0.114382         -0.211536      0  \n3            -0.115784         -0.235606      0  \n4            -0.128612         -0.212763      0  \n...                ...               ...    ...  \n7994         -0.076688         -0.267579      0  \n7995         -0.137060         -0.168686      0  \n7996         -0.161627         -0.272322      0  \n7997         -0.047568         -0.126142      0  \n7998         -0.075687         -0.064506      0  \n\n[7999 rows x 2563 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2563 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = concat_features_train.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_train = np.array(concat_features_train['label']).astype(int)\nX_test = concat_features_test.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_test = np.array(concat_features_test['label']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:20:28.110334Z","iopub.execute_input":"2024-04-10T16:20:28.111372Z","iopub.status.idle":"2024-04-10T16:20:28.262473Z","shell.execute_reply.started":"2024-04-10T16:20:28.111332Z","shell.execute_reply":"2024-04-10T16:20:28.261107Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:20:29.108242Z","iopub.execute_input":"2024-04-10T16:20:29.108715Z","iopub.status.idle":"2024-04-10T16:20:29.145305Z","shell.execute_reply.started":"2024-04-10T16:20:29.108670Z","shell.execute_reply":"2024-04-10T16:20:29.144041Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1270_MLO  feature_1271_MLO  feature_1272_MLO  \\\n0     ...         -0.219428         -0.227731         -0.069607   \n1     ...         -0.066708         -0.267062         -0.045241   \n2     ...         -0.269503         -0.229044         -0.099550   \n3     ...         -0.274468         -0.215473         -0.081372   \n4     ...         -0.238900         -0.171533         -0.156897   \n...   ...               ...               ...               ...   \n7994  ...         -0.276627         -0.224987         -0.070554   \n7995  ...         -0.278353         -0.248899         -0.061061   \n7996  ...         -0.278436         -0.119826         -0.057346   \n7997  ...         -0.251418         -0.125756         -0.156107   \n7998  ...         -0.166142         -0.056127         -0.057476   \n\n      feature_1273_MLO  feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  \\\n0            -0.245696         -0.261152          0.777141         -0.277788   \n1             0.110348         -0.278391          1.332054         -0.129526   \n2            -0.273775         -0.276369         -0.198178         -0.078316   \n3            -0.271739         -0.271640         -0.221536         -0.068043   \n4            -0.103236          0.013147         -0.208144         -0.277218   \n...                ...               ...               ...               ...   \n7994         -0.268184         -0.209462         -0.272872         -0.250021   \n7995         -0.270626          0.856457         -0.182576         -0.219728   \n7996         -0.202150         -0.243588         -0.253669         -0.193268   \n7997         -0.262996         -0.273179          1.193740         -0.097256   \n7998         -0.211795         -0.274628          0.907126         -0.173844   \n\n      feature_1277_MLO  feature_1278_MLO  feature_1279_MLO  \n0             0.419119         -0.137479         -0.125389  \n1            -0.214615         -0.171379         -0.265228  \n2            -0.242274         -0.114382         -0.211536  \n3            -0.256326         -0.115784         -0.235606  \n4            -0.196322         -0.128612         -0.212763  \n...                ...               ...               ...  \n7994         -0.247301         -0.076688         -0.267579  \n7995         -0.203649         -0.137060         -0.168686  \n7996         -0.087486         -0.161627         -0.272322  \n7997         -0.161601         -0.047568         -0.126142  \n7998         -0.074086         -0.075687         -0.064506  \n\n[7999 rows x 2560 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1270_MLO</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.219428</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.066708</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.269503</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.274468</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.238900</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.276627</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.278353</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.278436</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.251418</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.166142</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2560 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# # Define the SMOTETomek resampling technique\n# smote_tomek = SMOTETomek(sampling_strategy=0.5,\n#                          random_state=42)\n\n# # Resample the training data using SMOTETomek\n# X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:12:11.168804Z","iopub.execute_input":"2024-04-10T16:12:11.169256Z","iopub.status.idle":"2024-04-10T16:12:11.817807Z","shell.execute_reply.started":"2024-04-10T16:12:11.169223Z","shell.execute_reply":"2024-04-10T16:12:11.816802Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# num_select_feature = int(X_train.shape[1]*0.1)\n# mi_selector = SelectKBest(mutual_info_classif, k=num_select_feature)\n\n# # Transform the data\n# X_selected = mi_selector.fit_transform(X_resampled, y_resampled)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:12:13.800544Z","iopub.execute_input":"2024-04-10T16:12:13.801016Z","iopub.status.idle":"2024-04-10T16:12:32.702559Z","shell.execute_reply.started":"2024-04-10T16:12:13.800984Z","shell.execute_reply":"2024-04-10T16:12:32.701260Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# X_selected.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:13:28.959737Z","iopub.execute_input":"2024-04-10T16:13:28.960171Z","iopub.status.idle":"2024-04-10T16:13:28.967898Z","shell.execute_reply.started":"2024-04-10T16:13:28.960142Z","shell.execute_reply":"2024-04-10T16:13:28.966377Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"(1534, 256)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Classifier","metadata":{}},{"cell_type":"code","source":"clf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:20:32.689263Z","iopub.execute_input":"2024-04-10T16:20:32.689762Z","iopub.status.idle":"2024-04-10T16:23:12.644767Z","shell.execute_reply.started":"2024-04-10T16:20:32.689721Z","shell.execute_reply":"2024-04-10T16:23:12.643857Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(random_state=42)","text/html":"<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Get predictions","metadata":{}},{"cell_type":"code","source":"# # Transform the data\n# X_test_selected = mi_selector.fit_transform(X_test, y_test)\n\n# Predict on the test set\ny_pred = clf.predict(X_test)\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:24:06.068566Z","iopub.execute_input":"2024-04-10T16:24:06.069072Z","iopub.status.idle":"2024-04-10T16:24:06.223070Z","shell.execute_reply.started":"2024-04-10T16:24:06.069041Z","shell.execute_reply":"2024-04-10T16:24:06.221681Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"[0 0 0 ... 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize=None)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, fmt=\"d\", annot=True, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Calculate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:24:07.223142Z","iopub.execute_input":"2024-04-10T16:24:07.224025Z","iopub.status.idle":"2024-04-10T16:24:07.579355Z","shell.execute_reply.started":"2024-04-10T16:24:07.223983Z","shell.execute_reply":"2024-04-10T16:24:07.578012Z"},"trusted":true},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.91      1.00      0.95      1808\n           1       1.00      0.06      0.12       192\n\n    accuracy                           0.91      2000\n   macro avg       0.95      0.53      0.54      2000\nweighted avg       0.92      0.91      0.87      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_test, y_pred)\ntest_precision = precision_score(y_test, y_pred, average='macro')\ntest_recall = recall_score(y_test, y_pred, average='macro')\ntest_f1 = f1_score(y_test, y_pred, average='macro')\ntest_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"Accuracy:\", test_accuracy)\nprint('Precison:', test_precision)\nprint('Recall:', test_recall)\nprint('F1 Score:', test_f1)\nprint('AUC:', test_auc)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T16:14:02.169369Z","iopub.execute_input":"2024-04-10T16:14:02.170057Z","iopub.status.idle":"2024-04-10T16:14:02.190559Z","shell.execute_reply.started":"2024-04-10T16:14:02.170014Z","shell.execute_reply":"2024-04-10T16:14:02.188853Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Accuracy: 0.2905\nPrecison: 0.5058542627392006\nRecall: 0.5121450958702065\nF1 Score: 0.2764189315119555\nAUC: 0.5121450958702065\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}