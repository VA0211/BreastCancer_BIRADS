{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7822434,"sourceType":"datasetVersion","datasetId":4583334},{"sourceId":8075321,"sourceType":"datasetVersion","datasetId":4765536}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\nfrom sklearn.utils import shuffle\nfrom skimage.io import imread\nimport PIL\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision import models\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# from torchsampler import ImbalancedDatasetSampler\n# from torchmetrics.functional import auroc, precision, recall, f1_score, precision_recall_curve\nimport albumentations as albu\nimport albumentations.pytorch\nimport matplotlib.pyplot as plt\nimport torchmetrics\nimport timm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess data","metadata":{}},{"cell_type":"code","source":"def preprocess_df(data_dir):\n    df = pd.read_csv(os.path.join(data_dir,'breast-level_annotations.csv'))\n    \n#     df['img_path'] = f\"{data_dir}/png/png/{df['study_id']}/{df['image_id']}.png\"\n    \n    df['malignancy_label'] = df['breast_birads']\n    # Define positive and negatives based on BI-RADS categories\n    df.loc[df['malignancy_label'] == 'BI-RADS 1', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 2', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 3', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 4', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 5', 'malignancy_label'] = 1\n\n    # Use pre-defined splits to separate data into development and testing\n    train_df = df[df['split'] == 'training']\n    test_df = df[df['split'] == 'test']\n    \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n\ndef show_image_pair(image1, image2):\n    fig = plt.figure(figsize=(10, 20))\n    fig.add_subplot(1,2,1)\n    plt.imshow(image1)\n    fig.add_subplot(1,2, 2)\n    plt.imshow(image2)\n    plt.show()\n\ndef test_dataset(df, idx=0):\n    dataset = Dataset(df, data_dir)\n    \n    img_path = os.path.join(data_dir, 'png/png', dataset.df.iloc[idx]['study_id'], dataset.df.iloc[idx]['image_id'] + '.png')\n    image1 = PIL.Image.open(img_path).convert('RGB')\n\n    tensor = dataset[idx].squeeze()\n    image2 = torchvision.transforms.ToPILImage()(tensor)\n\n    show_image_pair(image1, image2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/full-fullsize/'\n\ntrain_df, test_df = preprocess_df(data_dir)\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx in [random.choice(range(100)) for i in range(3)]:\n#     test_dataset(train_df, idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract feature","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\n\nclass Img2Vec():\n    RESNET_OUTPUT_SIZES = {\n        'resnet18': 512,\n        'resnet34': 512,\n        'resnet50': 2048,\n        'resnet101': 2048,\n        'resnet152': 2048\n    }\n\n    EFFICIENTNET_OUTPUT_SIZES = {\n        'efficientnet_b0': 1280,\n        'efficientnet_b1': 1280,\n        'efficientnet_b2': 1408,\n        'efficientnet_b3': 1536,\n        'efficientnet_b4': 1792,\n        'efficientnet_b5': 2048,\n        'efficientnet_b6': 2304,\n        'efficientnet_b7': 2560\n    }\n\n    def __init__(self, model='resnet-18', layer='default', layer_output_size=512):\n       \n        self.layer_output_size = layer_output_size\n        self.model_name = model\n\n        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n\n        self.model = self.model.to(device)\n\n        self.model.eval()\n\n        self.scaler = transforms.Resize((224, 224))\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n\n    def get_vec(self, img, tensor=False):\n        \"\"\" Get vector embedding from PIL image\n        :param img: PIL Image or list of PIL Images\n        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n        :returns: Numpy ndarray\n        \"\"\"\n        if type(img) == list:\n            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n            images = torch.stack(a).to(device)\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(len(img), self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(images)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[:, :]\n                elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[:, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[:, :, 0, 0]\n        else:\n            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(device)\n\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(1, self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(1, self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(image)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[0, :]\n                elif self.model_name == 'densenet':\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[0, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[0, :, 0, 0]\n\n    def _get_model_and_layer(self, model_name, layer):\n        \"\"\" Internal method for getting layer from model\n        :param model_name: model name such as 'resnet-18'\n        :param layer: layer as a string for resnet-18 or int for alexnet\n        :returns: pytorch model, selected layer\n        \"\"\"\n\n        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n            model = getattr(models, model_name)(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n            else:\n                layer = model._modules.get(layer)\n            return model, layer\n        elif model_name == 'resnet-18':\n            model = models.resnet18(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = 512\n            else:\n                layer = model._modules.get(layer)\n\n            return model, layer\n\n        elif model_name == 'alexnet':\n            model = models.alexnet(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'vgg':\n            # VGG-11\n            model = models.vgg11_bn(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = model.classifier[-1].in_features # should be 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'densenet':\n            # Densenet-121\n            model = models.densenet121(pretrained=True)\n            if layer == 'default':\n                layer = model.features[-1]\n                self.layer_output_size = model.classifier.in_features # should be 1024\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        elif \"efficientnet\" in model_name:\n            # efficientnet-b0 ~ efficientnet-b7\n            if model_name == \"efficientnet_b0\":\n                model = models.efficientnet_b0(pretrained=True)\n            elif model_name == \"efficientnet_b1\":\n                model = models.efficientnet_b1(pretrained=True)\n            elif model_name == \"efficientnet_b2\":\n                model = models.efficientnet_b2(pretrained=True)\n            elif model_name == \"efficientnet_b3\":\n                model = models.efficientnet_b3(pretrained=True)\n            elif model_name == \"efficientnet_b4\":\n                model = models.efficientnet_b4(pretrained=True)\n            elif model_name == \"efficientnet_b5\":\n                model = models.efficientnet_b5(pretrained=True)\n            elif model_name == \"efficientnet_b6\":\n                model = models.efficientnet_b6(pretrained=True)\n            elif model_name == \"efficientnet_b7\":\n                model = models.efficientnet_b7(pretrained=True)\n            else:\n                raise KeyError('Un support %s.' % model_name)\n\n            if layer == 'default':\n                layer = model.features\n                self.layer_output_size = self.EFFICIENTNET_OUTPUT_SIZES[model_name]\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        else:\n            raise KeyError('Model %s was not found' % model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_img_feature(df, data_dir, model, vec_length):\n    img2vec = Img2Vec(model=model, \n                      layer_output_size=vec_length)\n    \n    vec_mat = np.zeros((len(df) , vec_length))\n\n    for idx, row in df.iterrows():\n        img_path = os.path.join(data_dir, 'png/png', row['study_id'], row['image_id'] + '.png')\n        img = PIL.Image.open(img_path).convert('RGB')\n        if row['laterality'] == 'L':\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n        vec = img2vec.get_vec(img)\n        vec_mat[idx, :] = vec\n        \n    features_df = pd.DataFrame(vec_mat)\n    features_df = features_df.add_prefix('feature_')\n    features_df['label'] = df['malignancy_label']\n    features_df['view_position'] = df['view_position']\n    features_df['laterality'] = df['laterality']\n    features_df['study_id'] = df['study_id']\n    \n    return features_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'efficientnet_b0'\nnum_features = 1280\n\nfeatures_train = extract_img_feature(df=train_df, \n                                     data_dir=data_dir,\n                                     model=model_name, \n                                     vec_length=num_features\n                                     )\n\nfeatures_test = extract_img_feature(df=test_df, \n                                    data_dir=data_dir,\n                                    model=model_name, \n                                    vec_length=num_features\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_CC = features_train[features_train['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntrain_MLO = features_train[features_train['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\ntest_CC = features_test[features_test['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntest_MLO = features_test[features_test['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\nconcat_features_train = train_CC.merge(train_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))\nconcat_features_test = test_CC.merge(test_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_features_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat_features_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nconcat_features_train.to_csv(f'{save_dir}concat_features_train_{model_name}.csv', index=False)\nconcat_features_test.to_csv(f'{save_dir}concat_features_test_{model_name}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NaN\nprint(concat_features_train.isna().any().any())\nprint(concat_features_test.isna().any().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classify Model","metadata":{}},{"cell_type":"code","source":"!pip install scikit-fuzzy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import skfuzzy as fuzz\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss, TomekLinks\nfrom sklearn.metrics import roc_curve,precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay\n\nimport os\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:08:33.887567Z","iopub.execute_input":"2024-04-10T08:08:33.888287Z","iopub.status.idle":"2024-04-10T08:08:37.362627Z","shell.execute_reply.started":"2024-04-10T08:08:33.888239Z","shell.execute_reply":"2024-04-10T08:08:37.361457Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"concat_features_train = pd.read_csv('/kaggle/input/vin-feature/concat_features_train_efficientnet_b0.csv')\nconcat_features_test = pd.read_csv('/kaggle/input/vin-feature/concat_features_test_efficientnet_b0.csv')\n\nconcat_features_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:08:37.364910Z","iopub.execute_input":"2024-04-10T08:08:37.366062Z","iopub.status.idle":"2024-04-10T08:08:51.035321Z","shell.execute_reply.started":"2024-04-10T08:08:37.366027Z","shell.execute_reply":"2024-04-10T08:08:51.034381Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1271_MLO  feature_1272_MLO  feature_1273_MLO  \\\n0     ...         -0.227731         -0.069607         -0.245696   \n1     ...         -0.267062         -0.045241          0.110348   \n2     ...         -0.229044         -0.099550         -0.273775   \n3     ...         -0.215473         -0.081372         -0.271739   \n4     ...         -0.171533         -0.156897         -0.103236   \n...   ...               ...               ...               ...   \n7994  ...         -0.224987         -0.070554         -0.268184   \n7995  ...         -0.248899         -0.061061         -0.270626   \n7996  ...         -0.119826         -0.057346         -0.202150   \n7997  ...         -0.125756         -0.156107         -0.262996   \n7998  ...         -0.056127         -0.057476         -0.211795   \n\n      feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  feature_1277_MLO  \\\n0            -0.261152          0.777141         -0.277788          0.419119   \n1            -0.278391          1.332054         -0.129526         -0.214615   \n2            -0.276369         -0.198178         -0.078316         -0.242274   \n3            -0.271640         -0.221536         -0.068043         -0.256326   \n4             0.013147         -0.208144         -0.277218         -0.196322   \n...                ...               ...               ...               ...   \n7994         -0.209462         -0.272872         -0.250021         -0.247301   \n7995          0.856457         -0.182576         -0.219728         -0.203649   \n7996         -0.243588         -0.253669         -0.193268         -0.087486   \n7997         -0.273179          1.193740         -0.097256         -0.161601   \n7998         -0.274628          0.907126         -0.173844         -0.074086   \n\n      feature_1278_MLO  feature_1279_MLO  label  \n0            -0.137479         -0.125389      0  \n1            -0.171379         -0.265228      0  \n2            -0.114382         -0.211536      0  \n3            -0.115784         -0.235606      0  \n4            -0.128612         -0.212763      0  \n...                ...               ...    ...  \n7994         -0.076688         -0.267579      0  \n7995         -0.137060         -0.168686      0  \n7996         -0.161627         -0.272322      0  \n7997         -0.047568         -0.126142      0  \n7998         -0.075687         -0.064506      0  \n\n[7999 rows x 2563 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2563 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = concat_features_train.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_train = np.array(concat_features_train['label']).astype(int)\nX_test = concat_features_test.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_test = np.array(concat_features_test['label']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:08:51.036428Z","iopub.execute_input":"2024-04-10T08:08:51.036957Z","iopub.status.idle":"2024-04-10T08:08:51.220669Z","shell.execute_reply.started":"2024-04-10T08:08:51.036925Z","shell.execute_reply":"2024-04-10T08:08:51.219405Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:08:51.223847Z","iopub.execute_input":"2024-04-10T08:08:51.224266Z","iopub.status.idle":"2024-04-10T08:08:51.261051Z","shell.execute_reply.started":"2024-04-10T08:08:51.224229Z","shell.execute_reply":"2024-04-10T08:08:51.259780Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1270_MLO  feature_1271_MLO  feature_1272_MLO  \\\n0     ...         -0.219428         -0.227731         -0.069607   \n1     ...         -0.066708         -0.267062         -0.045241   \n2     ...         -0.269503         -0.229044         -0.099550   \n3     ...         -0.274468         -0.215473         -0.081372   \n4     ...         -0.238900         -0.171533         -0.156897   \n...   ...               ...               ...               ...   \n7994  ...         -0.276627         -0.224987         -0.070554   \n7995  ...         -0.278353         -0.248899         -0.061061   \n7996  ...         -0.278436         -0.119826         -0.057346   \n7997  ...         -0.251418         -0.125756         -0.156107   \n7998  ...         -0.166142         -0.056127         -0.057476   \n\n      feature_1273_MLO  feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  \\\n0            -0.245696         -0.261152          0.777141         -0.277788   \n1             0.110348         -0.278391          1.332054         -0.129526   \n2            -0.273775         -0.276369         -0.198178         -0.078316   \n3            -0.271739         -0.271640         -0.221536         -0.068043   \n4            -0.103236          0.013147         -0.208144         -0.277218   \n...                ...               ...               ...               ...   \n7994         -0.268184         -0.209462         -0.272872         -0.250021   \n7995         -0.270626          0.856457         -0.182576         -0.219728   \n7996         -0.202150         -0.243588         -0.253669         -0.193268   \n7997         -0.262996         -0.273179          1.193740         -0.097256   \n7998         -0.211795         -0.274628          0.907126         -0.173844   \n\n      feature_1277_MLO  feature_1278_MLO  feature_1279_MLO  \n0             0.419119         -0.137479         -0.125389  \n1            -0.214615         -0.171379         -0.265228  \n2            -0.242274         -0.114382         -0.211536  \n3            -0.256326         -0.115784         -0.235606  \n4            -0.196322         -0.128612         -0.212763  \n...                ...               ...               ...  \n7994         -0.247301         -0.076688         -0.267579  \n7995         -0.203649         -0.137060         -0.168686  \n7996         -0.087486         -0.161627         -0.272322  \n7997         -0.161601         -0.047568         -0.126142  \n7998         -0.074086         -0.075687         -0.064506  \n\n[7999 rows x 2560 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1270_MLO</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.219428</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.066708</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.269503</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.274468</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.238900</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.276627</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.278353</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.278436</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.251418</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.166142</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2560 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"num_select_feature = int(X_train.shape[1])\n# mi_selector = SelectKBest(mutual_info_classif, k=num_select_feature)\n\n# # Transform the data\n# X_selected = mi_selector.fit_transform(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:08:51.262904Z","iopub.execute_input":"2024-04-10T08:08:51.263287Z","iopub.status.idle":"2024-04-10T08:08:51.268965Z","shell.execute_reply.started":"2024-04-10T08:08:51.263254Z","shell.execute_reply":"2024-04-10T08:08:51.267583Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### NN Classifier","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import KFold\n\n# Define the model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(num_select_feature,)),\n    tf.keras.layers.Dense(96, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=0.005),\n              loss='binary_crossentropy',\n              metrics= [\n                        tf.keras.metrics.Recall(name='recall'),\n                        tf.keras.metrics.Precision(name='precision'),\n                        tf.keras.metrics.AUC(curve='ROC', name='auc'),\n                        tf.keras.metrics.AUC(curve='PR', name='pr_auc'),\n                      ])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:08:51.270602Z","iopub.execute_input":"2024-04-10T08:08:51.271146Z","iopub.status.idle":"2024-04-10T08:09:06.853680Z","shell.execute_reply.started":"2024-04-10T08:08:51.271103Z","shell.execute_reply":"2024-04-10T08:09:06.852375Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-04-10 08:08:53.741716: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-10 08:08:53.741896: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-10 08:08:53.939897: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │       \u001b[38;5;34m245,856\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m97\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">245,856</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m245,953\u001b[0m (960.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">245,953</span> (960.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m245,953\u001b[0m (960.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">245,953</span> (960.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)\nori_class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nori_class_weights","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:09:06.855584Z","iopub.execute_input":"2024-04-10T08:09:06.856040Z","iopub.status.idle":"2024-04-10T08:09:06.873282Z","shell.execute_reply.started":"2024-04-10T08:09:06.855995Z","shell.execute_reply":"2024-04-10T08:09:06.872068Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([0.55302821, 5.21447197])"},"metadata":{}}]},{"cell_type":"code","source":"# Define k-fold cross-validation\nkfold = KFold(n_splits=10, shuffle=True)\n\n# Perform k-fold cross-validation\nfold_history = []\nfor train_index, val_index in kfold.split(X_train):\n    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n\n    # Train the model\n    history = model.fit(X_train_fold, y_train_fold, \n                        epochs=30, verbose=0,\n                        validation_data=(X_val_fold, y_val_fold),\n                        class_weight=dict(enumerate(ori_class_weights)),\n#                         callbacks=[early_stopping]\n                       )\n    \n    fold_history.append(history.history)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:09:06.875072Z","iopub.execute_input":"2024-04-10T08:09:06.875827Z","iopub.status.idle":"2024-04-10T08:14:51.165959Z","shell.execute_reply.started":"2024-04-10T08:09:06.875793Z","shell.execute_reply":"2024-04-10T08:14:51.164778Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"fold_history[9]","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:18:43.127724Z","iopub.execute_input":"2024-04-10T08:18:43.128162Z","iopub.status.idle":"2024-04-10T08:18:43.141133Z","shell.execute_reply.started":"2024-04-10T08:18:43.128127Z","shell.execute_reply":"2024-04-10T08:18:43.139754Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'auc': [0.9999106526374817,\n  0.9999942183494568,\n  0.9992398619651794,\n  0.9998345375061035,\n  0.999918520450592,\n  0.9999162554740906,\n  0.9999160766601562,\n  0.9998782277107239,\n  0.9999929666519165,\n  0.9999030232429504,\n  0.9999184012413025,\n  0.9999141693115234,\n  0.999918520450592,\n  0.9999147057533264,\n  0.9999150633811951,\n  0.9996957182884216,\n  0.9996299743652344,\n  0.999997615814209,\n  0.9999958276748657,\n  0.9999197721481323,\n  0.9999942779541016,\n  0.999916672706604,\n  0.9999195337295532,\n  0.9998400807380676,\n  0.999993622303009,\n  0.9999153017997742,\n  0.9998558163642883,\n  0.9995591044425964,\n  0.9999948143959045,\n  0.9999158978462219],\n 'loss': [0.0030143787153065205,\n  0.0017048877198249102,\n  0.008340253494679928,\n  0.003598032984882593,\n  0.001734767109155655,\n  0.0018461705185472965,\n  0.0019999113865196705,\n  0.009345347993075848,\n  0.002909964183345437,\n  0.006181635893881321,\n  0.002205457305535674,\n  0.002172238891944289,\n  0.0018236561445519328,\n  0.0020012487657368183,\n  0.0019300788408145308,\n  0.016578879207372665,\n  0.008906829170882702,\n  0.002457612892612815,\n  0.001705615664832294,\n  0.0015113207045942545,\n  0.003643697826191783,\n  0.0020132779609411955,\n  0.002242016838863492,\n  0.0017875372432172298,\n  0.0015272097662091255,\n  0.00389975612051785,\n  0.011168465949594975,\n  0.019734496250748634,\n  0.0025533020962029696,\n  0.0021147478837519884],\n 'pr_auc': [0.9983913898468018,\n  0.9999456405639648,\n  0.9988530874252319,\n  0.9969061017036438,\n  0.9984830021858215,\n  0.9984610080718994,\n  0.9984566569328308,\n  0.9980778694152832,\n  0.9999343752861023,\n  0.9983370304107666,\n  0.9984739422798157,\n  0.9984431266784668,\n  0.9984908699989319,\n  0.998448371887207,\n  0.9984560012817383,\n  0.9957684874534607,\n  0.9934266209602356,\n  0.999978244304657,\n  0.9999604225158691,\n  0.9985000491142273,\n  0.9999485611915588,\n  0.9984685778617859,\n  0.9984979629516602,\n  0.9970129132270813,\n  0.9999404549598694,\n  0.9984447956085205,\n  0.9979026317596436,\n  0.9936268329620361,\n  0.9999510049819946,\n  0.998458981513977],\n 'precision': [0.9914407730102539,\n  0.9971305727958679,\n  0.9928469061851501,\n  0.9886202216148376,\n  0.9971305727958679,\n  0.9971305727958679,\n  0.9957020282745361,\n  0.9719495177268982,\n  0.9942775368690491,\n  0.991428554058075,\n  0.9971305727958679,\n  0.9971305727958679,\n  0.9971305727958679,\n  0.9971305727958679,\n  0.9971305727958679,\n  0.9531680345535278,\n  0.9774647951126099,\n  0.9971305727958679,\n  0.9971305727958679,\n  0.9971305727958679,\n  0.9942693114280701,\n  0.9971305727958679,\n  0.9957020282745361,\n  0.9971305727958679,\n  0.9971305727958679,\n  0.9928571581840515,\n  0.9678321480751038,\n  0.9374149441719055,\n  0.9957020282745361,\n  0.9971305727958679],\n 'recall': [1.0,\n  1.0,\n  0.9985611438751221,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  0.9971222877502441,\n  1.0,\n  0.9985611438751221,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  0.9956834316253662,\n  0.9985611438751221,\n  1.0,\n  1.0,\n  1.0,\n  0.9985611438751221,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  0.9956834316253662,\n  0.9913669228553772,\n  1.0,\n  1.0],\n 'val_auc': [1.0,\n  0.999923586845398,\n  0.9999808669090271,\n  1.0,\n  1.0,\n  0.9999809265136719,\n  0.9995797276496887,\n  0.9997421503067017,\n  0.9930174350738525,\n  1.0,\n  1.0,\n  0.9999999403953552,\n  1.0,\n  1.0,\n  0.999961793422699,\n  0.9983952641487122,\n  0.9992166757583618,\n  0.9999426603317261,\n  0.9999427199363708,\n  0.9997134804725647,\n  0.9997612833976746,\n  0.9998758435249329,\n  0.9996083974838257,\n  0.9998090267181396,\n  0.9998567700386047,\n  0.9995988011360168,\n  0.9964083433151245,\n  0.9919283986091614,\n  0.9989397525787354,\n  0.9991689324378967],\n 'val_loss': [0.002159497933462262,\n  0.0067979274317622185,\n  0.004375504795461893,\n  0.0030105197802186012,\n  0.004536469466984272,\n  0.003530379617586732,\n  0.011940372176468372,\n  0.011768889613449574,\n  0.008620483800768852,\n  0.0012328646844252944,\n  0.0039365594275295734,\n  0.004169126506894827,\n  0.003622537013143301,\n  0.003809248097240925,\n  0.005361116491258144,\n  0.03962532430887222,\n  0.01867886260151863,\n  0.009339257143437862,\n  0.00898743886500597,\n  0.008495679125189781,\n  0.007431477308273315,\n  0.005791849922388792,\n  0.008050902746617794,\n  0.006606241688132286,\n  0.005920012015849352,\n  0.011998586356639862,\n  0.05629453808069229,\n  0.05510619282722473,\n  0.055292487144470215,\n  0.049105968326330185],\n 'val_pr_auc': [1.0,\n  0.9992679357528687,\n  0.9998084902763367,\n  1.0,\n  1.0,\n  0.9998084902763367,\n  0.9967319965362549,\n  0.9976100921630859,\n  0.9893479347229004,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  0.9996233582496643,\n  0.9689449667930603,\n  0.9930258989334106,\n  0.9994406700134277,\n  0.9994406700134277,\n  0.9975922703742981,\n  0.9979334473609924,\n  0.9988428950309753,\n  0.9969082474708557,\n  0.9982966780662537,\n  0.9986817836761475,\n  0.9968474507331848,\n  0.9576193690299988,\n  0.9730261564254761,\n  0.982265055179596,\n  0.9839563965797424],\n 'val_precision': [1.0,\n  1.0,\n  0.9863013625144958,\n  1.0,\n  1.0,\n  1.0,\n  0.9726027250289917,\n  0.9857142567634583,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  0.9113923907279968,\n  0.98591548204422,\n  0.9726027250289917,\n  0.9726027250289917,\n  0.9861111044883728,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  1.0,\n  0.9726027250289917,\n  0.8554216623306274,\n  0.9861111044883728,\n  0.9594594836235046,\n  0.9861111044883728],\n 'val_recall': [0.9861111044883728,\n  0.9861111044883728,\n  1.0,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9583333134651184,\n  0.9861111044883728,\n  1.0,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  1.0,\n  0.9722222089767456,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728,\n  0.9861111044883728]}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Get predictions","metadata":{}},{"cell_type":"code","source":"# # Transform the data\n# X_test_selected = mi_selector.fit_transform(X_test, y_test)\n\n# Predict using the test set\npreds = model.predict(X_test)\nprint(preds)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:18:55.722079Z","iopub.execute_input":"2024-04-10T08:18:55.722470Z","iopub.status.idle":"2024-04-10T08:18:56.046179Z","shell.execute_reply.started":"2024-04-10T08:18:55.722441Z","shell.execute_reply":"2024-04-10T08:18:56.044935Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n[[1.3000995e-07]\n [3.5770381e-07]\n [1.3914620e-06]\n ...\n [4.2976246e-35]\n [1.7253288e-04]\n [6.3755743e-02]]\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = (preds >= 0.5).astype(np.int32)[:,0]\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:18:57.086739Z","iopub.execute_input":"2024-04-10T08:18:57.087172Z","iopub.status.idle":"2024-04-10T08:18:57.094148Z","shell.execute_reply.started":"2024-04-10T08:18:57.087138Z","shell.execute_reply":"2024-04-10T08:18:57.092719Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[0 0 0 ... 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize=None)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, fmt=\"d\", annot=True, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Calculate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:18:58.079302Z","iopub.execute_input":"2024-04-10T08:18:58.079686Z","iopub.status.idle":"2024-04-10T08:18:58.789134Z","shell.execute_reply.started":"2024-04-10T08:18:58.079655Z","shell.execute_reply":"2024-04-10T08:18:58.787926Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.91      0.92      0.92      1808\n           1       0.21      0.19      0.20       192\n\n    accuracy                           0.85      2000\n   macro avg       0.56      0.56      0.56      2000\nweighted avg       0.85      0.85      0.85      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_test, y_pred)\ntest_precision = precision_score(y_test, y_pred, average='macro')\ntest_recall = recall_score(y_test, y_pred, average='macro')\ntest_f1 = f1_score(y_test, y_pred, average='macro')\ntest_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"Accuracy:\", test_accuracy)\nprint('Precison:', test_precision)\nprint('Recall:', test_recall)\nprint('F1 Score:', test_f1)\nprint('AUC:', test_auc)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T08:19:01.095109Z","iopub.execute_input":"2024-04-10T08:19:01.095642Z","iopub.status.idle":"2024-04-10T08:19:01.124276Z","shell.execute_reply.started":"2024-04-10T08:19:01.095607Z","shell.execute_reply":"2024-04-10T08:19:01.123254Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Accuracy: 0.8535\nPrecison: 0.5613533035299031\nRecall: 0.5558628318584071\nF1 Score: 0.5583275235062464\nAUC: 0.555862831858407\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}