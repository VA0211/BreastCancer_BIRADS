{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7822434,"sourceType":"datasetVersion","datasetId":4583334},{"sourceId":8075321,"sourceType":"datasetVersion","datasetId":4765536}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\nfrom sklearn.utils import shuffle\nfrom skimage.io import imread\nimport PIL\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision import models\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# from torchsampler import ImbalancedDatasetSampler\n# from torchmetrics.functional import auroc, precision, recall, f1_score, precision_recall_curve\nimport albumentations as albu\nimport albumentations.pytorch\nimport matplotlib.pyplot as plt\nimport torchmetrics\nimport timm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess data","metadata":{}},{"cell_type":"code","source":"def preprocess_df(data_dir):\n    df = pd.read_csv(os.path.join(data_dir,'breast-level_annotations.csv'))\n    \n#     df['img_path'] = f\"{data_dir}/png/png/{df['study_id']}/{df['image_id']}.png\"\n    \n    df['malignancy_label'] = df['breast_birads']\n    # Define positive and negatives based on BI-RADS categories\n    df.loc[df['malignancy_label'] == 'BI-RADS 1', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 2', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 3', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 4', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 5', 'malignancy_label'] = 1\n\n    # Use pre-defined splits to separate data into development and testing\n    train_df = df[df['split'] == 'training']\n    test_df = df[df['split'] == 'test']\n    \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n\ndef show_image_pair(image1, image2):\n    fig = plt.figure(figsize=(10, 20))\n    fig.add_subplot(1,2,1)\n    plt.imshow(image1)\n    fig.add_subplot(1,2, 2)\n    plt.imshow(image2)\n    plt.show()\n\ndef test_dataset(df, idx=0):\n    dataset = Dataset(df, data_dir)\n    \n    img_path = os.path.join(data_dir, 'png/png', dataset.df.iloc[idx]['study_id'], dataset.df.iloc[idx]['image_id'] + '.png')\n    image1 = PIL.Image.open(img_path).convert('RGB')\n\n    tensor = dataset[idx].squeeze()\n    image2 = torchvision.transforms.ToPILImage()(tensor)\n\n    show_image_pair(image1, image2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/full-fullsize/'\n\ntrain_df, test_df = preprocess_df(data_dir)\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for idx in [random.choice(range(100)) for i in range(3)]:\n#     test_dataset(train_df, idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract feature","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\n\nclass Img2Vec():\n    RESNET_OUTPUT_SIZES = {\n        'resnet18': 512,\n        'resnet34': 512,\n        'resnet50': 2048,\n        'resnet101': 2048,\n        'resnet152': 2048\n    }\n\n    EFFICIENTNET_OUTPUT_SIZES = {\n        'efficientnet_b0': 1280,\n        'efficientnet_b1': 1280,\n        'efficientnet_b2': 1408,\n        'efficientnet_b3': 1536,\n        'efficientnet_b4': 1792,\n        'efficientnet_b5': 2048,\n        'efficientnet_b6': 2304,\n        'efficientnet_b7': 2560\n    }\n\n    def __init__(self, model='resnet-18', layer='default', layer_output_size=512):\n       \n        self.layer_output_size = layer_output_size\n        self.model_name = model\n\n        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n\n        self.model = self.model.to(device)\n\n        self.model.eval()\n\n        self.scaler = transforms.Resize((224, 224))\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n\n    def get_vec(self, img, tensor=False):\n        \"\"\" Get vector embedding from PIL image\n        :param img: PIL Image or list of PIL Images\n        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n        :returns: Numpy ndarray\n        \"\"\"\n        if type(img) == list:\n            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n            images = torch.stack(a).to(device)\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(len(img), self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(images)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[:, :]\n                elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[:, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[:, :, 0, 0]\n        else:\n            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(device)\n\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(1, self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(1, self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(image)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[0, :]\n                elif self.model_name == 'densenet':\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[0, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[0, :, 0, 0]\n\n    def _get_model_and_layer(self, model_name, layer):\n        \"\"\" Internal method for getting layer from model\n        :param model_name: model name such as 'resnet-18'\n        :param layer: layer as a string for resnet-18 or int for alexnet\n        :returns: pytorch model, selected layer\n        \"\"\"\n\n        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n            model = getattr(models, model_name)(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n            else:\n                layer = model._modules.get(layer)\n            return model, layer\n        elif model_name == 'resnet-18':\n            model = models.resnet18(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = 512\n            else:\n                layer = model._modules.get(layer)\n\n            return model, layer\n\n        elif model_name == 'alexnet':\n            model = models.alexnet(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'vgg':\n            # VGG-11\n            model = models.vgg11_bn(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = model.classifier[-1].in_features # should be 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'densenet':\n            # Densenet-121\n            model = models.densenet121(pretrained=True)\n            if layer == 'default':\n                layer = model.features[-1]\n                self.layer_output_size = model.classifier.in_features # should be 1024\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        elif \"efficientnet\" in model_name:\n            # efficientnet-b0 ~ efficientnet-b7\n            if model_name == \"efficientnet_b0\":\n                model = models.efficientnet_b0(pretrained=True)\n            elif model_name == \"efficientnet_b1\":\n                model = models.efficientnet_b1(pretrained=True)\n            elif model_name == \"efficientnet_b2\":\n                model = models.efficientnet_b2(pretrained=True)\n            elif model_name == \"efficientnet_b3\":\n                model = models.efficientnet_b3(pretrained=True)\n            elif model_name == \"efficientnet_b4\":\n                model = models.efficientnet_b4(pretrained=True)\n            elif model_name == \"efficientnet_b5\":\n                model = models.efficientnet_b5(pretrained=True)\n            elif model_name == \"efficientnet_b6\":\n                model = models.efficientnet_b6(pretrained=True)\n            elif model_name == \"efficientnet_b7\":\n                model = models.efficientnet_b7(pretrained=True)\n            else:\n                raise KeyError('Un support %s.' % model_name)\n\n            if layer == 'default':\n                layer = model.features\n                self.layer_output_size = self.EFFICIENTNET_OUTPUT_SIZES[model_name]\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        else:\n            raise KeyError('Model %s was not found' % model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_img_feature(df, data_dir, model, vec_length):\n    img2vec = Img2Vec(model=model, \n                      layer_output_size=vec_length)\n    \n    vec_mat = np.zeros((len(df) , vec_length))\n\n    for idx, row in df.iterrows():\n        img_path = os.path.join(data_dir, 'png/png', row['study_id'], row['image_id'] + '.png')\n        img = PIL.Image.open(img_path).convert('RGB')\n        if row['laterality'] == 'L':\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n        vec = img2vec.get_vec(img)\n        vec_mat[idx, :] = vec\n        \n    features_df = pd.DataFrame(vec_mat)\n    features_df = features_df.add_prefix('feature_')\n    features_df['label'] = df['malignancy_label']\n    features_df['view_position'] = df['view_position']\n    features_df['laterality'] = df['laterality']\n    features_df['study_id'] = df['study_id']\n    \n    return features_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'efficientnet_b0'\nnum_features = 1280\n\nfeatures_train = extract_img_feature(df=train_df, \n                                     data_dir=data_dir,\n                                     model=model_name, \n                                     vec_length=num_features\n                                     )\n\nfeatures_test = extract_img_feature(df=test_df, \n                                    data_dir=data_dir,\n                                    model=model_name, \n                                    vec_length=num_features\n                                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_CC = features_train[features_train['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntrain_MLO = features_train[features_train['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\ntest_CC = features_test[features_test['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntest_MLO = features_test[features_test['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\nconcat_features_train = train_CC.merge(train_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))\nconcat_features_test = test_CC.merge(test_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_features_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat_features_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nconcat_features_train.to_csv(f'{save_dir}concat_features_train_{model_name}.csv', index=False)\nconcat_features_test.to_csv(f'{save_dir}concat_features_test_{model_name}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NaN\nprint(concat_features_train.isna().any().any())\nprint(concat_features_test.isna().any().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classify Model","metadata":{}},{"cell_type":"code","source":"!pip install scikit-fuzzy","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:05:32.400405Z","iopub.execute_input":"2024-04-10T09:05:32.400819Z","iopub.status.idle":"2024-04-10T09:05:53.541464Z","shell.execute_reply.started":"2024-04-10T09:05:32.400776Z","shell.execute_reply":"2024-04-10T09:05:53.540164Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting scikit-fuzzy\n  Downloading scikit-fuzzy-0.4.2.tar.gz (993 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (1.26.4)\nRequirement already satisfied: scipy>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (1.11.4)\nRequirement already satisfied: networkx>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit-fuzzy) (3.2.1)\nBuilding wheels for collected packages: scikit-fuzzy\n  Building wheel for scikit-fuzzy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894077 sha256=82a05b3c82451c05eb1435700e0b5142de9f34b6e512a011d359aab6442eb3e7\n  Stored in directory: /root/.cache/pip/wheels/4f/86/1b/dfd97134a2c8313e519bcebd95d3fedc7be7944db022094bc8\nSuccessfully built scikit-fuzzy\nInstalling collected packages: scikit-fuzzy\nSuccessfully installed scikit-fuzzy-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import skfuzzy as fuzz\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss, TomekLinks\nfrom sklearn.metrics import roc_curve,precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay\n\nimport os\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:06:00.277724Z","iopub.execute_input":"2024-04-10T09:06:00.278109Z","iopub.status.idle":"2024-04-10T09:06:03.709814Z","shell.execute_reply.started":"2024-04-10T09:06:00.278077Z","shell.execute_reply":"2024-04-10T09:06:03.708688Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"concat_features_train = pd.read_csv('/kaggle/input/vin-feature/concat_features_train_efficientnet_b0.csv')\nconcat_features_test = pd.read_csv('/kaggle/input/vin-feature/concat_features_test_efficientnet_b0.csv')\n\nconcat_features_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:13:56.832871Z","iopub.execute_input":"2024-04-10T09:13:56.833577Z","iopub.status.idle":"2024-04-10T09:14:06.081819Z","shell.execute_reply.started":"2024-04-10T09:13:56.833532Z","shell.execute_reply":"2024-04-10T09:14:06.080510Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1271_MLO  feature_1272_MLO  feature_1273_MLO  \\\n0     ...         -0.227731         -0.069607         -0.245696   \n1     ...         -0.267062         -0.045241          0.110348   \n2     ...         -0.229044         -0.099550         -0.273775   \n3     ...         -0.215473         -0.081372         -0.271739   \n4     ...         -0.171533         -0.156897         -0.103236   \n...   ...               ...               ...               ...   \n7994  ...         -0.224987         -0.070554         -0.268184   \n7995  ...         -0.248899         -0.061061         -0.270626   \n7996  ...         -0.119826         -0.057346         -0.202150   \n7997  ...         -0.125756         -0.156107         -0.262996   \n7998  ...         -0.056127         -0.057476         -0.211795   \n\n      feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  feature_1277_MLO  \\\n0            -0.261152          0.777141         -0.277788          0.419119   \n1            -0.278391          1.332054         -0.129526         -0.214615   \n2            -0.276369         -0.198178         -0.078316         -0.242274   \n3            -0.271640         -0.221536         -0.068043         -0.256326   \n4             0.013147         -0.208144         -0.277218         -0.196322   \n...                ...               ...               ...               ...   \n7994         -0.209462         -0.272872         -0.250021         -0.247301   \n7995          0.856457         -0.182576         -0.219728         -0.203649   \n7996         -0.243588         -0.253669         -0.193268         -0.087486   \n7997         -0.273179          1.193740         -0.097256         -0.161601   \n7998         -0.274628          0.907126         -0.173844         -0.074086   \n\n      feature_1278_MLO  feature_1279_MLO  label  \n0            -0.137479         -0.125389      0  \n1            -0.171379         -0.265228      0  \n2            -0.114382         -0.211536      0  \n3            -0.115784         -0.235606      0  \n4            -0.128612         -0.212763      0  \n...                ...               ...    ...  \n7994         -0.076688         -0.267579      0  \n7995         -0.137060         -0.168686      0  \n7996         -0.161627         -0.272322      0  \n7997         -0.047568         -0.126142      0  \n7998         -0.075687         -0.064506      0  \n\n[7999 rows x 2563 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2563 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = concat_features_train.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_train = np.array(concat_features_train['label']).astype(int)\nX_test = concat_features_test.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_test = np.array(concat_features_test['label']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:14:10.335797Z","iopub.execute_input":"2024-04-10T09:14:10.336213Z","iopub.status.idle":"2024-04-10T09:14:10.499272Z","shell.execute_reply.started":"2024-04-10T09:14:10.336182Z","shell.execute_reply":"2024-04-10T09:14:10.497947Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:14:12.453034Z","iopub.execute_input":"2024-04-10T09:14:12.453459Z","iopub.status.idle":"2024-04-10T09:14:12.488468Z","shell.execute_reply.started":"2024-04-10T09:14:12.453425Z","shell.execute_reply":"2024-04-10T09:14:12.487252Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1270_MLO  feature_1271_MLO  feature_1272_MLO  \\\n0     ...         -0.219428         -0.227731         -0.069607   \n1     ...         -0.066708         -0.267062         -0.045241   \n2     ...         -0.269503         -0.229044         -0.099550   \n3     ...         -0.274468         -0.215473         -0.081372   \n4     ...         -0.238900         -0.171533         -0.156897   \n...   ...               ...               ...               ...   \n7994  ...         -0.276627         -0.224987         -0.070554   \n7995  ...         -0.278353         -0.248899         -0.061061   \n7996  ...         -0.278436         -0.119826         -0.057346   \n7997  ...         -0.251418         -0.125756         -0.156107   \n7998  ...         -0.166142         -0.056127         -0.057476   \n\n      feature_1273_MLO  feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  \\\n0            -0.245696         -0.261152          0.777141         -0.277788   \n1             0.110348         -0.278391          1.332054         -0.129526   \n2            -0.273775         -0.276369         -0.198178         -0.078316   \n3            -0.271739         -0.271640         -0.221536         -0.068043   \n4            -0.103236          0.013147         -0.208144         -0.277218   \n...                ...               ...               ...               ...   \n7994         -0.268184         -0.209462         -0.272872         -0.250021   \n7995         -0.270626          0.856457         -0.182576         -0.219728   \n7996         -0.202150         -0.243588         -0.253669         -0.193268   \n7997         -0.262996         -0.273179          1.193740         -0.097256   \n7998         -0.211795         -0.274628          0.907126         -0.173844   \n\n      feature_1277_MLO  feature_1278_MLO  feature_1279_MLO  \n0             0.419119         -0.137479         -0.125389  \n1            -0.214615         -0.171379         -0.265228  \n2            -0.242274         -0.114382         -0.211536  \n3            -0.256326         -0.115784         -0.235606  \n4            -0.196322         -0.128612         -0.212763  \n...                ...               ...               ...  \n7994         -0.247301         -0.076688         -0.267579  \n7995         -0.203649         -0.137060         -0.168686  \n7996         -0.087486         -0.161627         -0.272322  \n7997         -0.161601         -0.047568         -0.126142  \n7998         -0.074086         -0.075687         -0.064506  \n\n[7999 rows x 2560 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1270_MLO</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.219428</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.066708</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.269503</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.274468</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.238900</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.276627</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.278353</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.278436</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.251418</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.166142</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2560 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"num_select_feature = int(X_train.shape[1]*0.1)\nmi_selector = SelectKBest(mutual_info_classif, k=num_select_feature)\n\n# Transform the data\nX_selected = mi_selector.fit_transform(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:14:13.399946Z","iopub.execute_input":"2024-04-10T09:14:13.400372Z","iopub.status.idle":"2024-04-10T09:15:47.514323Z","shell.execute_reply.started":"2024-04-10T09:14:13.400339Z","shell.execute_reply":"2024-04-10T09:15:47.513138Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"X_selected.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:16:47.485234Z","iopub.execute_input":"2024-04-10T09:16:47.485654Z","iopub.status.idle":"2024-04-10T09:16:47.495679Z","shell.execute_reply.started":"2024-04-10T09:16:47.485624Z","shell.execute_reply":"2024-04-10T09:16:47.494694Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(7999, 256)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Classifier","metadata":{}},{"cell_type":"code","source":"# Perform Fuzzy C-Means clustering using skfuzzy\ncntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(X_selected.T, c=2,\n                                                 m=2.5, error=0.005, maxiter=10000)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:19:27.671075Z","iopub.execute_input":"2024-04-10T09:19:27.671506Z","iopub.status.idle":"2024-04-10T09:19:27.898788Z","shell.execute_reply.started":"2024-04-10T09:19:27.671473Z","shell.execute_reply":"2024-04-10T09:19:27.897136Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Get predictions","metadata":{}},{"cell_type":"code","source":"# Transform the data\nX_test_selected = mi_selector.fit_transform(X_test, y_test)\n\n# Predict using the test set\nu_test, _, _, _, _, _ = fuzz.cluster.cmeans_predict(X_test_selected.T, cntr, \n                                                    m=2.5, error=0.005, maxiter=10000)\nprint(u_test)\n\n# Get the most likely class for each sample\ny_pred = np.argmax(u_test, axis=0)\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:19:29.101759Z","iopub.execute_input":"2024-04-10T09:19:29.102230Z","iopub.status.idle":"2024-04-10T09:19:29.118217Z","shell.execute_reply.started":"2024-04-10T09:19:29.102193Z","shell.execute_reply":"2024-04-10T09:19:29.117324Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[[0.499934   0.49990528 0.49989871 ... 0.49997143 0.49978226 0.49978799]\n [0.500066   0.50009472 0.50010129 ... 0.50002857 0.50021774 0.50021201]]\n[1 1 1 ... 1 1 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize=None)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, fmt=\"d\", annot=True, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Calculate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:19:30.031736Z","iopub.execute_input":"2024-04-10T09:19:30.032712Z","iopub.status.idle":"2024-04-10T09:19:30.317245Z","shell.execute_reply.started":"2024-04-10T09:19:30.032647Z","shell.execute_reply":"2024-04-10T09:19:30.316129Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.89      0.06      0.11      1808\n           1       0.10      0.93      0.17       192\n\n    accuracy                           0.14      2000\n   macro avg       0.49      0.50      0.14      2000\nweighted avg       0.81      0.14      0.12      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_test, y_pred)\ntest_precision = precision_score(y_test, y_pred, average='macro')\ntest_recall = recall_score(y_test, y_pred, average='macro')\ntest_f1 = f1_score(y_test, y_pred, average='macro')\ntest_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"Accuracy:\", test_accuracy)\nprint('Precison:', test_precision)\nprint('Recall:', test_recall)\nprint('F1 Score:', test_f1)\nprint('AUC:', test_auc)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:19:04.610250Z","iopub.execute_input":"2024-04-10T09:19:04.610720Z","iopub.status.idle":"2024-04-10T09:19:04.629539Z","shell.execute_reply.started":"2024-04-10T09:19:04.610684Z","shell.execute_reply":"2024-04-10T09:19:04.628352Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Accuracy: 0.142\nPrecison: 0.49247104594823393\nRecall: 0.49518344395280234\nF1 Score: 0.1408237877654509\nAUC: 0.49518344395280234\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}