{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7822434,"sourceType":"datasetVersion","datasetId":4583334},{"sourceId":8080013,"sourceType":"datasetVersion","datasetId":4765536}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\nfrom sklearn.utils import shuffle\nfrom skimage.io import imread\nimport PIL\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as T\nfrom torchvision import models\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# from torchsampler import ImbalancedDatasetSampler\n# from torchmetrics.functional import auroc, precision, recall, f1_score, precision_recall_curve\nimport albumentations as albu\nimport albumentations.pytorch\nimport matplotlib.pyplot as plt\nimport torchmetrics\nimport timm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-11T08:02:45.036137Z","iopub.execute_input":"2024-04-11T08:02:45.036623Z","iopub.status.idle":"2024-04-11T08:03:07.862989Z","shell.execute_reply.started":"2024-04-11T08:02:45.036585Z","shell.execute_reply":"2024-04-11T08:03:07.861115Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# setting device on GPU if available, else CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))\n    print('Memory Usage:')\n    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:10.250958Z","iopub.execute_input":"2024-04-11T08:03:10.251428Z","iopub.status.idle":"2024-04-11T08:03:10.262830Z","shell.execute_reply.started":"2024-04-11T08:03:10.251385Z","shell.execute_reply":"2024-04-11T08:03:10.260879Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocess data","metadata":{}},{"cell_type":"code","source":"def preprocess_df(data_dir):\n    df = pd.read_csv(os.path.join(data_dir,'breast-level_annotations.csv'))\n    \n#     df['img_path'] = f\"{data_dir}/png/png/{df['study_id']}/{df['image_id']}.png\"\n    \n    df['malignancy_label'] = df['breast_birads']\n    # Define positive and negatives based on BI-RADS categories\n    df.loc[df['malignancy_label'] == 'BI-RADS 1', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 2', 'malignancy_label'] = 0\n    df.loc[df['malignancy_label'] == 'BI-RADS 3', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 4', 'malignancy_label'] = 1\n    df.loc[df['malignancy_label'] == 'BI-RADS 5', 'malignancy_label'] = 1\n\n    # Use pre-defined splits to separate data into development and testing\n    train_df = df[df['split'] == 'training']\n    test_df = df[df['split'] == 'test']\n    \n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n\ndef show_image_pair(image1, image2):\n    fig = plt.figure(figsize=(10, 20))\n    fig.add_subplot(1,2,1)\n    plt.imshow(image1)\n    fig.add_subplot(1,2, 2)\n    plt.imshow(image2)\n    plt.show()\n\ndef test_dataset(df, idx=0):\n    dataset = Dataset(df, data_dir)\n    \n    img_path = os.path.join(data_dir, 'png/png', dataset.df.iloc[idx]['study_id'], dataset.df.iloc[idx]['image_id'] + '.png')\n    image1 = PIL.Image.open(img_path).convert('RGB')\n\n    tensor = dataset[idx].squeeze()\n    image2 = torchvision.transforms.ToPILImage()(tensor)\n\n    show_image_pair(image1, image2)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:11.645246Z","iopub.execute_input":"2024-04-11T08:03:11.645694Z","iopub.status.idle":"2024-04-11T08:03:11.661404Z","shell.execute_reply.started":"2024-04-11T08:03:11.645630Z","shell.execute_reply":"2024-04-11T08:03:11.660226Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/full-fullsize/'\n\ntrain_df, test_df = preprocess_df(data_dir)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:12.746231Z","iopub.execute_input":"2024-04-11T08:03:12.746818Z","iopub.status.idle":"2024-04-11T08:03:13.018234Z","shell.execute_reply.started":"2024-04-11T08:03:12.746777Z","shell.execute_reply":"2024-04-11T08:03:13.016157Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                               study_id                         series_id  \\\n0      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n1      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n2      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n3      b8d273e8601f348d3664778dae0e7e0b  b36517b9cbbcfd286a7ae04f643af97a   \n4      8269f5971eaca3e5d3772d1796e6bd7a  d931832a0815df082c085b6e09d20aac   \n...                                 ...                               ...   \n15995  f2093a752e6b44df5990f5fd38c99dd2  2b1b2b8f48abab9819c0b3d091e152ee   \n15996  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n15997  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n15998  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n15999  b3c8969cd2accfa4dbb2aece1f7158ab  69d7f07ea04572dad5e5aa62fbcfc4b7   \n\n                               image_id laterality view_position  height  \\\n0      d8125545210c08e1b1793a5af6458ee2          L            CC    3518   \n1      290c658f4e75a3f83ec78a847414297c          L           MLO    3518   \n2      cd0fc7bc53ac632a11643ac4cc91002a          R            CC    3518   \n3      71638b1e853799f227492bfb08a01491          R           MLO    3518   \n4      dd9ce3288c0773e006a294188aadba8e          L            CC    3518   \n...                                 ...        ...           ...     ...   \n15995  ea732154d149f619b20070b78060ae65          R            CC    2812   \n15996  4689616c3d0b46fcba7a771107730791          R            CC    3580   \n15997  3c22491bcf1d0b004715c28d80981cdd          L            CC    3580   \n15998  d443b9725e331b8b27589aa725597801          R           MLO    3580   \n15999  45c1239cc36b0e672f0072707fd05c6f          L           MLO    3580   \n\n       width breast_birads breast_density     split malignancy_label  \n0       2800     BI-RADS 2      DENSITY C  training                0  \n1       2800     BI-RADS 2      DENSITY C  training                0  \n2       2800     BI-RADS 2      DENSITY C  training                0  \n3       2800     BI-RADS 2      DENSITY C  training                0  \n4       2800     BI-RADS 1      DENSITY C  training                0  \n...      ...           ...            ...       ...              ...  \n15995   2012     BI-RADS 2      DENSITY C  training                0  \n15996   2702     BI-RADS 2      DENSITY C  training                0  \n15997   2702     BI-RADS 2      DENSITY C  training                0  \n15998   2686     BI-RADS 2      DENSITY C  training                0  \n15999   2670     BI-RADS 2      DENSITY C  training                0  \n\n[16000 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>series_id</th>\n      <th>image_id</th>\n      <th>laterality</th>\n      <th>view_position</th>\n      <th>height</th>\n      <th>width</th>\n      <th>breast_birads</th>\n      <th>breast_density</th>\n      <th>split</th>\n      <th>malignancy_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b8d273e8601f348d3664778dae0e7e0b</td>\n      <td>b36517b9cbbcfd286a7ae04f643af97a</td>\n      <td>d8125545210c08e1b1793a5af6458ee2</td>\n      <td>L</td>\n      <td>CC</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b8d273e8601f348d3664778dae0e7e0b</td>\n      <td>b36517b9cbbcfd286a7ae04f643af97a</td>\n      <td>290c658f4e75a3f83ec78a847414297c</td>\n      <td>L</td>\n      <td>MLO</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b8d273e8601f348d3664778dae0e7e0b</td>\n      <td>b36517b9cbbcfd286a7ae04f643af97a</td>\n      <td>cd0fc7bc53ac632a11643ac4cc91002a</td>\n      <td>R</td>\n      <td>CC</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b8d273e8601f348d3664778dae0e7e0b</td>\n      <td>b36517b9cbbcfd286a7ae04f643af97a</td>\n      <td>71638b1e853799f227492bfb08a01491</td>\n      <td>R</td>\n      <td>MLO</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8269f5971eaca3e5d3772d1796e6bd7a</td>\n      <td>d931832a0815df082c085b6e09d20aac</td>\n      <td>dd9ce3288c0773e006a294188aadba8e</td>\n      <td>L</td>\n      <td>CC</td>\n      <td>3518</td>\n      <td>2800</td>\n      <td>BI-RADS 1</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15995</th>\n      <td>f2093a752e6b44df5990f5fd38c99dd2</td>\n      <td>2b1b2b8f48abab9819c0b3d091e152ee</td>\n      <td>ea732154d149f619b20070b78060ae65</td>\n      <td>R</td>\n      <td>CC</td>\n      <td>2812</td>\n      <td>2012</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15996</th>\n      <td>b3c8969cd2accfa4dbb2aece1f7158ab</td>\n      <td>69d7f07ea04572dad5e5aa62fbcfc4b7</td>\n      <td>4689616c3d0b46fcba7a771107730791</td>\n      <td>R</td>\n      <td>CC</td>\n      <td>3580</td>\n      <td>2702</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15997</th>\n      <td>b3c8969cd2accfa4dbb2aece1f7158ab</td>\n      <td>69d7f07ea04572dad5e5aa62fbcfc4b7</td>\n      <td>3c22491bcf1d0b004715c28d80981cdd</td>\n      <td>L</td>\n      <td>CC</td>\n      <td>3580</td>\n      <td>2702</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15998</th>\n      <td>b3c8969cd2accfa4dbb2aece1f7158ab</td>\n      <td>69d7f07ea04572dad5e5aa62fbcfc4b7</td>\n      <td>d443b9725e331b8b27589aa725597801</td>\n      <td>R</td>\n      <td>MLO</td>\n      <td>3580</td>\n      <td>2686</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15999</th>\n      <td>b3c8969cd2accfa4dbb2aece1f7158ab</td>\n      <td>69d7f07ea04572dad5e5aa62fbcfc4b7</td>\n      <td>45c1239cc36b0e672f0072707fd05c6f</td>\n      <td>L</td>\n      <td>MLO</td>\n      <td>3580</td>\n      <td>2670</td>\n      <td>BI-RADS 2</td>\n      <td>DENSITY C</td>\n      <td>training</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>16000 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# for idx in [random.choice(range(100)) for i in range(3)]:\n#     test_dataset(train_df, idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract feature","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\n\nclass Img2Vec():\n    RESNET_OUTPUT_SIZES = {\n        'resnet18': 512,\n        'resnet34': 512,\n        'resnet50': 2048,\n        'resnet101': 2048,\n        'resnet152': 2048\n    }\n\n    EFFICIENTNET_OUTPUT_SIZES = {\n        'efficientnet_b0': 1280,\n        'efficientnet_b1': 1280,\n        'efficientnet_b2': 1408,\n        'efficientnet_b3': 1536,\n        'efficientnet_b4': 1792,\n        'efficientnet_b5': 2048,\n        'efficientnet_b6': 2304,\n        'efficientnet_b7': 2560\n    }\n\n    def __init__(self, model='resnet-18', layer='default', layer_output_size=512):\n       \n        self.layer_output_size = layer_output_size\n        self.model_name = model\n\n        self.model, self.extraction_layer = self._get_model_and_layer(model, layer)\n\n        self.model = self.model.to(device)\n\n        self.model.eval()\n\n        self.scaler = transforms.Resize((224, 224))\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                              std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n\n    def get_vec(self, img, tensor=False):\n        \"\"\" Get vector embedding from PIL image\n        :param img: PIL Image or list of PIL Images\n        :param tensor: If True, get_vec will return a FloatTensor instead of Numpy array\n        :returns: Numpy ndarray\n        \"\"\"\n        if type(img) == list:\n            a = [self.normalize(self.to_tensor(self.scaler(im))) for im in img]\n            images = torch.stack(a).to(device)\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(len(img), self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(len(img), self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(images)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[:, :]\n                elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[:, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[:, :, 0, 0]\n        else:\n            image = self.normalize(self.to_tensor(self.scaler(img))).unsqueeze(0).to(device)\n\n            if self.model_name in ['alexnet', 'vgg']:\n                my_embedding = torch.zeros(1, self.layer_output_size)\n            elif self.model_name == 'densenet' or 'efficientnet' in self.model_name:\n                my_embedding = torch.zeros(1, self.layer_output_size, 7, 7)\n            else:\n                my_embedding = torch.zeros(1, self.layer_output_size, 1, 1)\n\n            def copy_data(m, i, o):\n                my_embedding.copy_(o.data)\n\n            h = self.extraction_layer.register_forward_hook(copy_data)\n            with torch.no_grad():\n                h_x = self.model(image)\n            h.remove()\n\n            if tensor:\n                return my_embedding\n            else:\n                if self.model_name in ['alexnet', 'vgg']:\n                    return my_embedding.numpy()[0, :]\n                elif self.model_name == 'densenet':\n                    return torch.mean(my_embedding, (2, 3), True).numpy()[0, :, 0, 0]\n                else:\n                    return my_embedding.numpy()[0, :, 0, 0]\n\n    def _get_model_and_layer(self, model_name, layer):\n        \"\"\" Internal method for getting layer from model\n        :param model_name: model name such as 'resnet-18'\n        :param layer: layer as a string for resnet-18 or int for alexnet\n        :returns: pytorch model, selected layer\n        \"\"\"\n\n        if model_name.startswith('resnet') and not model_name.startswith('resnet-'):\n            model = getattr(models, model_name)(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = self.RESNET_OUTPUT_SIZES[model_name]\n            else:\n                layer = model._modules.get(layer)\n            return model, layer\n        elif model_name == 'resnet-18':\n            model = models.resnet18(pretrained=True)\n            if layer == 'default':\n                layer = model._modules.get('avgpool')\n                self.layer_output_size = 512\n            else:\n                layer = model._modules.get(layer)\n\n            return model, layer\n\n        elif model_name == 'alexnet':\n            model = models.alexnet(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'vgg':\n            # VGG-11\n            model = models.vgg11_bn(pretrained=True)\n            if layer == 'default':\n                layer = model.classifier[-2]\n                self.layer_output_size = model.classifier[-1].in_features # should be 4096\n            else:\n                layer = model.classifier[-layer]\n\n            return model, layer\n\n        elif model_name == 'densenet':\n            # Densenet-121\n            model = models.densenet121(pretrained=True)\n            if layer == 'default':\n                layer = model.features[-1]\n                self.layer_output_size = model.classifier.in_features # should be 1024\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        elif \"efficientnet\" in model_name:\n            # efficientnet-b0 ~ efficientnet-b7\n            if model_name == \"efficientnet_b0\":\n                model = models.efficientnet_b0(pretrained=True)\n            elif model_name == \"efficientnet_b1\":\n                model = models.efficientnet_b1(pretrained=True)\n            elif model_name == \"efficientnet_b2\":\n                model = models.efficientnet_b2(pretrained=True)\n            elif model_name == \"efficientnet_b3\":\n                model = models.efficientnet_b3(pretrained=True)\n            elif model_name == \"efficientnet_b4\":\n                model = models.efficientnet_b4(pretrained=True)\n            elif model_name == \"efficientnet_b5\":\n                model = models.efficientnet_b5(pretrained=True)\n            elif model_name == \"efficientnet_b6\":\n                model = models.efficientnet_b6(pretrained=True)\n            elif model_name == \"efficientnet_b7\":\n                model = models.efficientnet_b7(pretrained=True)\n            else:\n                raise KeyError('Un support %s.' % model_name)\n\n            if layer == 'default':\n                layer = model.features\n                self.layer_output_size = self.EFFICIENTNET_OUTPUT_SIZES[model_name]\n            else:\n                raise KeyError('Un support %s for layer parameters' % model_name)\n\n            return model, layer\n\n        else:\n            raise KeyError('Model %s was not found' % model_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:16.242949Z","iopub.execute_input":"2024-04-11T08:03:16.243477Z","iopub.status.idle":"2024-04-11T08:03:16.677721Z","shell.execute_reply.started":"2024-04-11T08:03:16.243440Z","shell.execute_reply":"2024-04-11T08:03:16.675272Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def extract_img_feature(df, data_dir, model, vec_length):\n    img2vec = Img2Vec(model=model, \n                      layer_output_size=vec_length)\n    \n    vec_mat = np.zeros((len(df) , vec_length))\n\n    for idx, row in df.iterrows():\n        img_path = os.path.join(data_dir, 'png/png', row['study_id'], row['image_id'] + '.png')\n        img = PIL.Image.open(img_path).convert('RGB')\n        if row['laterality'] == 'L':\n            img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n        vec = img2vec.get_vec(img)\n        vec_mat[idx, :] = vec\n        \n    features_df = pd.DataFrame(vec_mat)\n    features_df = features_df.add_prefix('feature_')\n    features_df['label'] = df['malignancy_label']\n    features_df['view_position'] = df['view_position']\n    features_df['laterality'] = df['laterality']\n    features_df['study_id'] = df['study_id']\n    \n    return features_df","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:17.312846Z","iopub.execute_input":"2024-04-11T08:03:17.313314Z","iopub.status.idle":"2024-04-11T08:03:17.326585Z","shell.execute_reply.started":"2024-04-11T08:03:17.313282Z","shell.execute_reply":"2024-04-11T08:03:17.324690Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model_name = 'efficientnet_b0'\nnum_features = 1280\n\nfeatures_train = extract_img_feature(df=train_df, \n                                     data_dir=data_dir,\n                                     model=model_name, \n                                     vec_length=num_features\n                                     )\n\nfeatures_test = extract_img_feature(df=test_df, \n                                    data_dir=data_dir,\n                                    model=model_name, \n                                    vec_length=num_features\n                                    )","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:03:18.847031Z","iopub.execute_input":"2024-04-11T08:03:18.847546Z","iopub.status.idle":"2024-04-11T08:48:40.911977Z","shell.execute_reply.started":"2024-04-11T08:03:18.847508Z","shell.execute_reply":"2024-04-11T08:48:40.910465Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 63.5MB/s]\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nfeatures_train.to_csv(f'{save_dir}features_train_{model_name}.csv', index=False)\nfeatures_test.to_csv(f'{save_dir}features_test_{model_name}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T08:58:41.783425Z","iopub.execute_input":"2024-04-11T08:58:41.784021Z","iopub.status.idle":"2024-04-11T08:59:48.326204Z","shell.execute_reply.started":"2024-04-11T08:58:41.783979Z","shell.execute_reply":"2024-04-11T08:59:48.324907Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_CC = features_train[features_train['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntrain_MLO = features_train[features_train['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\ntest_CC = features_test[features_test['view_position'] == 'CC'].drop(['label', 'view_position'], axis=1) \ntest_MLO = features_test[features_test['view_position'] == 'MLO'].drop(['view_position'], axis=1) \n\nconcat_features_train = train_CC.merge(train_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))\nconcat_features_test = test_CC.merge(test_MLO, on=['study_id', 'laterality'], suffixes=('_CC', '_MLO'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_features_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat_features_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_dir = '/kaggle/working/'\n\nconcat_features_train.to_csv(f'{save_dir}concat_features_train_{model_name}.csv', index=False)\nconcat_features_test.to_csv(f'{save_dir}concat_features_test_{model_name}.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NaN\nprint(concat_features_train.isna().any().any())\nprint(concat_features_test.isna().any().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classify Model","metadata":{}},{"cell_type":"code","source":"!pip install scikit-fuzzy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import skfuzzy as fuzz\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import NearMiss, TomekLinks, RandomUnderSampler\nfrom sklearn.metrics import roc_curve,precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n\nimport os\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:00:24.548878Z","iopub.execute_input":"2024-04-11T09:00:24.549351Z","iopub.status.idle":"2024-04-11T09:00:24.939288Z","shell.execute_reply.started":"2024-04-11T09:00:24.549316Z","shell.execute_reply":"2024-04-11T09:00:24.938047Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"concat_features_train = pd.read_csv('/kaggle/input/vin-feature/concat_features_train_efficientnet_b0.csv')\nconcat_features_test = pd.read_csv('/kaggle/input/vin-feature/concat_features_test_efficientnet_b0.csv')\n\nconcat_features_train","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:00:34.120699Z","iopub.execute_input":"2024-04-11T09:00:34.121840Z","iopub.status.idle":"2024-04-11T09:00:51.580706Z","shell.execute_reply.started":"2024-04-11T09:00:34.121796Z","shell.execute_reply":"2024-04-11T09:00:51.579286Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1271_MLO  feature_1272_MLO  feature_1273_MLO  \\\n0     ...         -0.227731         -0.069607         -0.245696   \n1     ...         -0.267062         -0.045241          0.110348   \n2     ...         -0.229044         -0.099550         -0.273775   \n3     ...         -0.215473         -0.081372         -0.271739   \n4     ...         -0.171533         -0.156897         -0.103236   \n...   ...               ...               ...               ...   \n7994  ...         -0.224987         -0.070554         -0.268184   \n7995  ...         -0.248899         -0.061061         -0.270626   \n7996  ...         -0.119826         -0.057346         -0.202150   \n7997  ...         -0.125756         -0.156107         -0.262996   \n7998  ...         -0.056127         -0.057476         -0.211795   \n\n      feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  feature_1277_MLO  \\\n0            -0.261152          0.777141         -0.277788          0.419119   \n1            -0.278391          1.332054         -0.129526         -0.214615   \n2            -0.276369         -0.198178         -0.078316         -0.242274   \n3            -0.271640         -0.221536         -0.068043         -0.256326   \n4             0.013147         -0.208144         -0.277218         -0.196322   \n...                ...               ...               ...               ...   \n7994         -0.209462         -0.272872         -0.250021         -0.247301   \n7995          0.856457         -0.182576         -0.219728         -0.203649   \n7996         -0.243588         -0.253669         -0.193268         -0.087486   \n7997         -0.273179          1.193740         -0.097256         -0.161601   \n7998         -0.274628          0.907126         -0.173844         -0.074086   \n\n      feature_1278_MLO  feature_1279_MLO  label  \n0            -0.137479         -0.125389      0  \n1            -0.171379         -0.265228      0  \n2            -0.114382         -0.211536      0  \n3            -0.115784         -0.235606      0  \n4            -0.128612         -0.212763      0  \n...                ...               ...    ...  \n7994         -0.076688         -0.267579      0  \n7995         -0.137060         -0.168686      0  \n7996         -0.161627         -0.272322      0  \n7997         -0.047568         -0.126142      0  \n7998         -0.075687         -0.064506      0  \n\n[7999 rows x 2563 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2563 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train = concat_features_train.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_train = np.array(concat_features_train['label']).astype(int)\nX_test = concat_features_test.copy().drop(['label', 'study_id', 'laterality'], axis=1)  # Features\ny_test = np.array(concat_features_test['label']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:01:24.664174Z","iopub.execute_input":"2024-04-11T09:01:24.664793Z","iopub.status.idle":"2024-04-11T09:01:24.906270Z","shell.execute_reply.started":"2024-04-11T09:01:24.664732Z","shell.execute_reply":"2024-04-11T09:01:24.904834Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:01:29.034291Z","iopub.execute_input":"2024-04-11T09:01:29.034860Z","iopub.status.idle":"2024-04-11T09:01:29.075405Z","shell.execute_reply.started":"2024-04-11T09:01:29.034815Z","shell.execute_reply":"2024-04-11T09:01:29.073886Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"      feature_0_CC  feature_1_CC  feature_2_CC  feature_3_CC  feature_4_CC  \\\n0        -0.101210     -0.174239      0.121421     -0.141098     -0.277888   \n1        -0.058023     -0.144134      0.043329     -0.205208     -0.277090   \n2        -0.133359     -0.116345     -0.090174     -0.152623     -0.274790   \n3        -0.028937     -0.137982     -0.021677     -0.123771     -0.278450   \n4        -0.237625     -0.103890     -0.161415     -0.151816     -0.258543   \n...            ...           ...           ...           ...           ...   \n7994     -0.258254     -0.080930      0.449778     -0.089230     -0.240298   \n7995     -0.065698     -0.121744     -0.142756     -0.103660     -0.277657   \n7996     -0.258811     -0.164959     -0.137258     -0.115478     -0.265617   \n7997     -0.107519     -0.185627      0.245910     -0.104862     -0.278052   \n7998     -0.166349     -0.160051     -0.058604     -0.149733     -0.234989   \n\n      feature_5_CC  feature_6_CC  feature_7_CC  feature_8_CC  feature_9_CC  \\\n0        -0.186682     -0.140499     -0.067732     -0.111569     -0.269176   \n1        -0.239452     -0.136032     -0.068292     -0.111630     -0.277913   \n2        -0.261249     -0.108648     -0.066531     -0.120386     -0.276416   \n3        -0.276996     -0.113804     -0.076267     -0.138271     -0.278445   \n4        -0.255977     -0.122566     -0.094943     -0.183906     -0.223354   \n...            ...           ...           ...           ...           ...   \n7994     -0.196373     -0.084969     -0.056243     -0.064630     -0.259314   \n7995     -0.269506     -0.077604     -0.117808     -0.209393     -0.276863   \n7996     -0.250546     -0.094390     -0.146150     -0.200458     -0.257736   \n7997     -0.061016     -0.138582     -0.048616     -0.064193     -0.246309   \n7998     -0.154209     -0.179032     -0.064121     -0.116352     -0.272534   \n\n      ...  feature_1270_MLO  feature_1271_MLO  feature_1272_MLO  \\\n0     ...         -0.219428         -0.227731         -0.069607   \n1     ...         -0.066708         -0.267062         -0.045241   \n2     ...         -0.269503         -0.229044         -0.099550   \n3     ...         -0.274468         -0.215473         -0.081372   \n4     ...         -0.238900         -0.171533         -0.156897   \n...   ...               ...               ...               ...   \n7994  ...         -0.276627         -0.224987         -0.070554   \n7995  ...         -0.278353         -0.248899         -0.061061   \n7996  ...         -0.278436         -0.119826         -0.057346   \n7997  ...         -0.251418         -0.125756         -0.156107   \n7998  ...         -0.166142         -0.056127         -0.057476   \n\n      feature_1273_MLO  feature_1274_MLO  feature_1275_MLO  feature_1276_MLO  \\\n0            -0.245696         -0.261152          0.777141         -0.277788   \n1             0.110348         -0.278391          1.332054         -0.129526   \n2            -0.273775         -0.276369         -0.198178         -0.078316   \n3            -0.271739         -0.271640         -0.221536         -0.068043   \n4            -0.103236          0.013147         -0.208144         -0.277218   \n...                ...               ...               ...               ...   \n7994         -0.268184         -0.209462         -0.272872         -0.250021   \n7995         -0.270626          0.856457         -0.182576         -0.219728   \n7996         -0.202150         -0.243588         -0.253669         -0.193268   \n7997         -0.262996         -0.273179          1.193740         -0.097256   \n7998         -0.211795         -0.274628          0.907126         -0.173844   \n\n      feature_1277_MLO  feature_1278_MLO  feature_1279_MLO  \n0             0.419119         -0.137479         -0.125389  \n1            -0.214615         -0.171379         -0.265228  \n2            -0.242274         -0.114382         -0.211536  \n3            -0.256326         -0.115784         -0.235606  \n4            -0.196322         -0.128612         -0.212763  \n...                ...               ...               ...  \n7994         -0.247301         -0.076688         -0.267579  \n7995         -0.203649         -0.137060         -0.168686  \n7996         -0.087486         -0.161627         -0.272322  \n7997         -0.161601         -0.047568         -0.126142  \n7998         -0.074086         -0.075687         -0.064506  \n\n[7999 rows x 2560 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature_0_CC</th>\n      <th>feature_1_CC</th>\n      <th>feature_2_CC</th>\n      <th>feature_3_CC</th>\n      <th>feature_4_CC</th>\n      <th>feature_5_CC</th>\n      <th>feature_6_CC</th>\n      <th>feature_7_CC</th>\n      <th>feature_8_CC</th>\n      <th>feature_9_CC</th>\n      <th>...</th>\n      <th>feature_1270_MLO</th>\n      <th>feature_1271_MLO</th>\n      <th>feature_1272_MLO</th>\n      <th>feature_1273_MLO</th>\n      <th>feature_1274_MLO</th>\n      <th>feature_1275_MLO</th>\n      <th>feature_1276_MLO</th>\n      <th>feature_1277_MLO</th>\n      <th>feature_1278_MLO</th>\n      <th>feature_1279_MLO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.101210</td>\n      <td>-0.174239</td>\n      <td>0.121421</td>\n      <td>-0.141098</td>\n      <td>-0.277888</td>\n      <td>-0.186682</td>\n      <td>-0.140499</td>\n      <td>-0.067732</td>\n      <td>-0.111569</td>\n      <td>-0.269176</td>\n      <td>...</td>\n      <td>-0.219428</td>\n      <td>-0.227731</td>\n      <td>-0.069607</td>\n      <td>-0.245696</td>\n      <td>-0.261152</td>\n      <td>0.777141</td>\n      <td>-0.277788</td>\n      <td>0.419119</td>\n      <td>-0.137479</td>\n      <td>-0.125389</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.058023</td>\n      <td>-0.144134</td>\n      <td>0.043329</td>\n      <td>-0.205208</td>\n      <td>-0.277090</td>\n      <td>-0.239452</td>\n      <td>-0.136032</td>\n      <td>-0.068292</td>\n      <td>-0.111630</td>\n      <td>-0.277913</td>\n      <td>...</td>\n      <td>-0.066708</td>\n      <td>-0.267062</td>\n      <td>-0.045241</td>\n      <td>0.110348</td>\n      <td>-0.278391</td>\n      <td>1.332054</td>\n      <td>-0.129526</td>\n      <td>-0.214615</td>\n      <td>-0.171379</td>\n      <td>-0.265228</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.133359</td>\n      <td>-0.116345</td>\n      <td>-0.090174</td>\n      <td>-0.152623</td>\n      <td>-0.274790</td>\n      <td>-0.261249</td>\n      <td>-0.108648</td>\n      <td>-0.066531</td>\n      <td>-0.120386</td>\n      <td>-0.276416</td>\n      <td>...</td>\n      <td>-0.269503</td>\n      <td>-0.229044</td>\n      <td>-0.099550</td>\n      <td>-0.273775</td>\n      <td>-0.276369</td>\n      <td>-0.198178</td>\n      <td>-0.078316</td>\n      <td>-0.242274</td>\n      <td>-0.114382</td>\n      <td>-0.211536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.028937</td>\n      <td>-0.137982</td>\n      <td>-0.021677</td>\n      <td>-0.123771</td>\n      <td>-0.278450</td>\n      <td>-0.276996</td>\n      <td>-0.113804</td>\n      <td>-0.076267</td>\n      <td>-0.138271</td>\n      <td>-0.278445</td>\n      <td>...</td>\n      <td>-0.274468</td>\n      <td>-0.215473</td>\n      <td>-0.081372</td>\n      <td>-0.271739</td>\n      <td>-0.271640</td>\n      <td>-0.221536</td>\n      <td>-0.068043</td>\n      <td>-0.256326</td>\n      <td>-0.115784</td>\n      <td>-0.235606</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.237625</td>\n      <td>-0.103890</td>\n      <td>-0.161415</td>\n      <td>-0.151816</td>\n      <td>-0.258543</td>\n      <td>-0.255977</td>\n      <td>-0.122566</td>\n      <td>-0.094943</td>\n      <td>-0.183906</td>\n      <td>-0.223354</td>\n      <td>...</td>\n      <td>-0.238900</td>\n      <td>-0.171533</td>\n      <td>-0.156897</td>\n      <td>-0.103236</td>\n      <td>0.013147</td>\n      <td>-0.208144</td>\n      <td>-0.277218</td>\n      <td>-0.196322</td>\n      <td>-0.128612</td>\n      <td>-0.212763</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7994</th>\n      <td>-0.258254</td>\n      <td>-0.080930</td>\n      <td>0.449778</td>\n      <td>-0.089230</td>\n      <td>-0.240298</td>\n      <td>-0.196373</td>\n      <td>-0.084969</td>\n      <td>-0.056243</td>\n      <td>-0.064630</td>\n      <td>-0.259314</td>\n      <td>...</td>\n      <td>-0.276627</td>\n      <td>-0.224987</td>\n      <td>-0.070554</td>\n      <td>-0.268184</td>\n      <td>-0.209462</td>\n      <td>-0.272872</td>\n      <td>-0.250021</td>\n      <td>-0.247301</td>\n      <td>-0.076688</td>\n      <td>-0.267579</td>\n    </tr>\n    <tr>\n      <th>7995</th>\n      <td>-0.065698</td>\n      <td>-0.121744</td>\n      <td>-0.142756</td>\n      <td>-0.103660</td>\n      <td>-0.277657</td>\n      <td>-0.269506</td>\n      <td>-0.077604</td>\n      <td>-0.117808</td>\n      <td>-0.209393</td>\n      <td>-0.276863</td>\n      <td>...</td>\n      <td>-0.278353</td>\n      <td>-0.248899</td>\n      <td>-0.061061</td>\n      <td>-0.270626</td>\n      <td>0.856457</td>\n      <td>-0.182576</td>\n      <td>-0.219728</td>\n      <td>-0.203649</td>\n      <td>-0.137060</td>\n      <td>-0.168686</td>\n    </tr>\n    <tr>\n      <th>7996</th>\n      <td>-0.258811</td>\n      <td>-0.164959</td>\n      <td>-0.137258</td>\n      <td>-0.115478</td>\n      <td>-0.265617</td>\n      <td>-0.250546</td>\n      <td>-0.094390</td>\n      <td>-0.146150</td>\n      <td>-0.200458</td>\n      <td>-0.257736</td>\n      <td>...</td>\n      <td>-0.278436</td>\n      <td>-0.119826</td>\n      <td>-0.057346</td>\n      <td>-0.202150</td>\n      <td>-0.243588</td>\n      <td>-0.253669</td>\n      <td>-0.193268</td>\n      <td>-0.087486</td>\n      <td>-0.161627</td>\n      <td>-0.272322</td>\n    </tr>\n    <tr>\n      <th>7997</th>\n      <td>-0.107519</td>\n      <td>-0.185627</td>\n      <td>0.245910</td>\n      <td>-0.104862</td>\n      <td>-0.278052</td>\n      <td>-0.061016</td>\n      <td>-0.138582</td>\n      <td>-0.048616</td>\n      <td>-0.064193</td>\n      <td>-0.246309</td>\n      <td>...</td>\n      <td>-0.251418</td>\n      <td>-0.125756</td>\n      <td>-0.156107</td>\n      <td>-0.262996</td>\n      <td>-0.273179</td>\n      <td>1.193740</td>\n      <td>-0.097256</td>\n      <td>-0.161601</td>\n      <td>-0.047568</td>\n      <td>-0.126142</td>\n    </tr>\n    <tr>\n      <th>7998</th>\n      <td>-0.166349</td>\n      <td>-0.160051</td>\n      <td>-0.058604</td>\n      <td>-0.149733</td>\n      <td>-0.234989</td>\n      <td>-0.154209</td>\n      <td>-0.179032</td>\n      <td>-0.064121</td>\n      <td>-0.116352</td>\n      <td>-0.272534</td>\n      <td>...</td>\n      <td>-0.166142</td>\n      <td>-0.056127</td>\n      <td>-0.057476</td>\n      <td>-0.211795</td>\n      <td>-0.274628</td>\n      <td>0.907126</td>\n      <td>-0.173844</td>\n      <td>-0.074086</td>\n      <td>-0.075687</td>\n      <td>-0.064506</td>\n    </tr>\n  </tbody>\n</table>\n<p>7999 rows × 2560 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# # Define the SMOTETomek resampling technique\n# smote_tomek = SMOTETomek(sampling_strategy=0.5,\n#                          random_state=42)\n\n# # Resample the training data using SMOTETomek\n# X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:01:30.276873Z","iopub.execute_input":"2024-04-11T09:01:30.278271Z","iopub.status.idle":"2024-04-11T09:01:30.283876Z","shell.execute_reply.started":"2024-04-11T09:01:30.278215Z","shell.execute_reply":"2024-04-11T09:01:30.282856Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# num_select_feature = int(X_train.shape[1]*0.1)\n# mi_selector = SelectKBest(mutual_info_classif, k=num_select_feature)\n\n# # Transform the data\n# X_selected = mi_selector.fit_transform(X_resampled, y_resampled)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_selected.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_full = X_train\ny_train_full = y_train\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:01:32.956148Z","iopub.execute_input":"2024-04-11T09:01:32.957130Z","iopub.status.idle":"2024-04-11T09:01:33.099225Z","shell.execute_reply.started":"2024-04-11T09:01:32.957086Z","shell.execute_reply":"2024-04-11T09:01:33.097795Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Classifier","metadata":{}},{"cell_type":"code","source":"def get_models():\n    models = list()\n    models.append(('lr', LogisticRegression(max_iter=5000)))\n    models.append(('cart', DecisionTreeClassifier()))\n    models.append(('bayes', GaussianNB()))\n    return models\n\n\n# evaluate each base model\ndef evaluate_models(models, X_train, X_val, y_train, y_val):\n    # fit and evaluate the models\n    scores = list()\n    for name, model in models:\n        # fit the model\n        model.fit(X_train, y_train)\n        # evaluate the model\n        yhat = model.predict(X_val)\n        auc = roc_auc_score(y_val, yhat)\n        # store the performance\n        scores.append(auc)\n        # report model performance\n    return scores","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:01:35.012819Z","iopub.execute_input":"2024-04-11T09:01:35.013616Z","iopub.status.idle":"2024-04-11T09:01:35.021242Z","shell.execute_reply.started":"2024-04-11T09:01:35.013581Z","shell.execute_reply":"2024-04-11T09:01:35.020326Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"models = get_models()\n# fit and evaluate each model\nscores = evaluate_models(models, X_train, X_val, y_train, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:01:38.780370Z","iopub.execute_input":"2024-04-11T09:01:38.781449Z","iopub.status.idle":"2024-04-11T09:03:39.734218Z","shell.execute_reply.started":"2024-04-11T09:01:38.781408Z","shell.execute_reply":"2024-04-11T09:03:39.732966Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}]},{"cell_type":"code","source":"# create the ensemble\nensemble = VotingClassifier(estimators=models, voting='soft', weights=scores)\n# fit the ensemble on the training dataset\nensemble.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:05:42.141234Z","iopub.execute_input":"2024-04-11T09:05:42.141844Z","iopub.status.idle":"2024-04-11T09:07:36.187463Z","shell.execute_reply.started":"2024-04-11T09:05:42.141803Z","shell.execute_reply":"2024-04-11T09:07:36.186178Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"VotingClassifier(estimators=[('lr', LogisticRegression(max_iter=1000)),\n                             ('cart', DecisionTreeClassifier()),\n                             ('bayes', GaussianNB())],\n                 voting='soft',\n                 weights=[0.5185957755836771, 0.4931009440813363,\n                          0.5305371426216927])","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n                             (&#x27;cart&#x27;, DecisionTreeClassifier()),\n                             (&#x27;bayes&#x27;, GaussianNB())],\n                 voting=&#x27;soft&#x27;,\n                 weights=[0.5185957755836771, 0.4931009440813363,\n                          0.5305371426216927])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(max_iter=1000)),\n                             (&#x27;cart&#x27;, DecisionTreeClassifier()),\n                             (&#x27;bayes&#x27;, GaussianNB())],\n                 voting=&#x27;soft&#x27;,\n                 weights=[0.5185957755836771, 0.4931009440813363,\n                          0.5305371426216927])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>cart</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>bayes</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Get predictions","metadata":{}},{"cell_type":"code","source":"# # Transform the data\n# X_test_selected = mi_selector.fit_transform(X_test, y_test)\n\n# Predict on the test set\ny_pred = ensemble.predict(X_test)\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:08:09.620306Z","iopub.execute_input":"2024-04-11T09:08:09.620744Z","iopub.status.idle":"2024-04-11T09:08:09.974278Z","shell.execute_reply.started":"2024-04-11T09:08:09.620712Z","shell.execute_reply":"2024-04-11T09:08:09.973048Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[0 0 0 ... 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# Create confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize=None)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, fmt=\"d\", annot=True, cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Calculate classification report\nreport = classification_report(y_test, y_pred)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:08:14.573631Z","iopub.execute_input":"2024-04-11T09:08:14.574594Z","iopub.status.idle":"2024-04-11T09:08:15.117826Z","shell.execute_reply.started":"2024-04-11T09:08:14.574554Z","shell.execute_reply":"2024-04-11T09:08:15.116692Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAApIAAAIjCAYAAACwHvu2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABH1ElEQVR4nO3de1yU1b7H8e8gMigKiApIea+8ZWpqRpaXI4nXvLWNJMOy3BlaippZaV5KCu+YRXfdpWXt0l1WJkmKF/KCkWZmmqbtFNAICE1AmfNHhzlNaMKKccD5vM/reb32rGfNM7+Z/arz29/1PAuLzWazCQAAACgjD1cXAAAAgMqJRhIAAABGaCQBAABghEYSAAAARmgkAQAAYIRGEgAAAEZoJAEAAGCERhIAAABGaCQBAABghEYSwF86cOCAevbsKT8/P1ksFq1evbpcr//DDz/IYrFo6dKl5Xrdyqxbt27q1q2bq8sAgIuikQQqge+//17//Oc/1aRJE3l7e8vX11edO3fWokWL9Ntvvzn1s6OiorRnzx49/fTTeuONN9ShQwenft6lNGLECFksFvn6+p73dzxw4IAsFossFovmzp1b5usfO3ZM06dPV1paWjlUCwAVj6erCwDw1z766CP94x//kNVq1d13361rr71WBQUF2rx5syZNmqS9e/fqpZdecspn//bbb0pJSdHjjz+uMWPGOOUzGjZsqN9++01Vq1Z1yvUvxtPTU6dPn9aHH36ooUOHOpxbvny5vL29debMGaNrHzt2TDNmzFCjRo3Utm3bUr9v3bp1Rp8HAJcajSRQgR0+fFgRERFq2LChkpKSVK9ePfu56OhoHTx4UB999JHTPv/EiROSJH9/f6d9hsVikbe3t9OufzFWq1WdO3fWW2+9VaKRXLFihfr27av33nvvktRy+vRpVa9eXV5eXpfk8wDg72JpG6jA4uLilJeXp1dffdWhiSx21VVX6eGHH7a/Pnv2rGbNmqWmTZvKarWqUaNGeuyxx5Sfn+/wvkaNGqlfv37avHmzbrjhBnl7e6tJkyb617/+ZZ8zffp0NWzYUJI0adIkWSwWNWrUSNLvS8LF//mPpk+fLovF4jCWmJiom2++Wf7+/qpRo4aaNWumxx57zH7+QvdIJiUl6ZZbbpGPj4/8/f01YMAA7du377yfd/DgQY0YMUL+/v7y8/PTPffco9OnT1/4h/2TYcOG6ZNPPlF2drZ9bMeOHTpw4ICGDRtWYn5WVpYmTpyo1q1bq0aNGvL19VXv3r311Vdf2eds2LBBHTt2lCTdc8899iXy4u/ZrVs3XXvttUpNTVWXLl1UvXp1++/y53sko6Ki5O3tXeL7h4eHq1atWjp27FipvysAlCcaSaAC+/DDD9WkSRPddNNNpZp/3333adq0abr++uu1YMECde3aVbGxsYqIiCgx9+DBg7r99tt16623at68eapVq5ZGjBihvXv3SpIGDx6sBQsWSJLuvPNOvfHGG1q4cGGZ6t+7d6/69eun/Px8zZw5U/PmzdNtt92mLVu2/OX7PvvsM4WHhyszM1PTp09XTEyMtm7dqs6dO+uHH34oMX/o0KH69ddfFRsbq6FDh2rp0qWaMWNGqescPHiwLBaL3n//ffvYihUr1Lx5c11//fUl5h86dEirV69Wv379NH/+fE2aNEl79uxR165d7U1dixYtNHPmTEnSqFGj9MYbb+iNN95Qly5d7Nf5+eef1bt3b7Vt21YLFy5U9+7dz1vfokWLVLduXUVFRencuXOSpBdffFHr1q3T4sWLFRISUurvCgDlygagQsrJybFJsg0YMKBU89PS0mySbPfdd5/D+MSJE22SbElJSfaxhg0b2iTZkpOT7WOZmZk2q9VqmzBhgn3s8OHDNkm2OXPmOFwzKirK1rBhwxI1PPnkk7Y//mtlwYIFNkm2EydOXLDu4s94/fXX7WNt27a1BQYG2n7++Wf72FdffWXz8PCw3X333SU+795773W45qBBg2y1a9e+4Gf+8Xv4+PjYbDab7fbbb7f16NHDZrPZbOfOnbMFBwfbZsyYcd7f4MyZM7Zz586V+B5Wq9U2c+ZM+9iOHTtKfLdiXbt2tUmyJSQknPdc165dHcY+/fRTmyTbU089ZTt06JCtRo0atoEDB170OwKAM5FIAhVUbm6uJKlmzZqlmv/xxx9LkmJiYhzGJ0yYIEkl7qVs2bKlbrnlFvvrunXrqlmzZjp06JBxzX9WfG/lf/7zHxUVFZXqPcePH1daWppGjBihgIAA+/h1112nW2+91f49/+iBBx5weH3LLbfo559/tv+GpTFs2DBt2LBB6enpSkpKUnp6+nmXtaXf76v08Pj9X5/nzp3Tzz//bF+237VrV6k/02q16p577inV3J49e+qf//ynZs6cqcGDB8vb21svvvhiqT8LAJyBRhKooHx9fSVJv/76a6nmHzlyRB4eHrrqqqscxoODg+Xv768jR444jDdo0KDENWrVqqVffvnFsOKS7rjjDnXu3Fn33XefgoKCFBERoXfeeecvm8riOps1a1biXIsWLXTy5EmdOnXKYfzP36VWrVqSVKbv0qdPH9WsWVMrV67U8uXL1bFjxxK/ZbGioiItWLBAV199taxWq+rUqaO6detq9+7dysnJKfVnXnHFFWV6sGbu3LkKCAhQWlqa4uPjFRgYWOr3AoAz0EgCFZSvr69CQkL09ddfl+l9f37Y5UKqVKly3nGbzWb8GcX37xWrVq2akpOT9dlnn2n48OHavXu37rjjDt16660l5v4df+e7FLNarRo8eLCWLVumVatWXTCNlKTZs2crJiZGXbp00ZtvvqlPP/1UiYmJatWqVamTV+n336csvvzyS2VmZkqS9uzZU6b3AoAz0EgCFVi/fv30/fffKyUl5aJzGzZsqKKiIh04cMBhPCMjQ9nZ2fYnsMtDrVq1HJ5wLvbn1FOSPDw81KNHD82fP1/ffPONnn76aSUlJenzzz8/77WL69y/f3+Jc99++63q1KkjHx+fv/cFLmDYsGH68ssv9euvv573AaVi//73v9W9e3e9+uqrioiIUM+ePRUWFlbiNyltU18ap06d0j333KOWLVtq1KhRiouL044dO8rt+gBggkYSqMAeeeQR+fj46L777lNGRkaJ899//70WLVok6felWUklnqyeP3++JKlv377lVlfTpk2Vk5Oj3bt328eOHz+uVatWOczLysoq8d7ijbn/vCVRsXr16qlt27ZatmyZQ2P29ddfa926dfbv6Qzdu3fXrFmz9Nxzzyk4OPiC86pUqVIi7Xz33Xf1008/OYwVN7zna7rLavLkyTp69KiWLVum+fPnq1GjRoqKirrg7wgAlwIbkgMVWNOmTbVixQrdcccdatGihcNfttm6daveffddjRgxQpLUpk0bRUVF6aWXXlJ2dra6du2q7du3a9myZRo4cOAFt5YxERERocmTJ2vQoEF66KGHdPr0ab3wwgu65pprHB42mTlzppKTk9W3b181bNhQmZmZev7553XllVfq5ptvvuD158yZo969eys0NFQjR47Ub7/9psWLF8vPz0/Tp08vt+/xZx4eHnriiScuOq9fv36aOXOm7rnnHt10003as2ePli9friZNmjjMa9q0qfz9/ZWQkKCaNWvKx8dHnTp1UuPGjctUV1JSkp5//nk9+eST9u2IXn/9dXXr1k1Tp05VXFxcma4HAOWFRBKo4G677Tbt3r1bt99+u/7zn/8oOjpajz76qH744QfNmzdP8fHx9rmvvPKKZsyYoR07dmjcuHFKSkrSlClT9Pbbb5drTbVr19aqVatUvXp1PfLII1q2bJliY2PVv3//ErU3aNBAr732mqKjo7VkyRJ16dJFSUlJ8vPzu+D1w8LCtHbtWtWuXVvTpk3T3LlzdeONN2rLli1lbsKc4bHHHtOECRP06aef6uGHH9auXbv00UcfqX79+g7zqlatqmXLlqlKlSp64IEHdOedd2rjxo1l+qxff/1V9957r9q1a6fHH3/cPn7LLbfo4Ycf1rx58/TFF1+Uy/cCgLKy2MpyNzoAAADwf0gkAQAAYIRGEgAAAEZoJAEAAGCERhIAAABGaCQBAABghEYSAAAARmgkAQAAYOSy/Ms21dqNcXUJAJwkMyX+4pMAVEo1vV2Xbzmzd/jty+ecdm1XI5EEAACAkcsykQQAACgTC9maCRpJAAAAi8XVFVRKtN8AAAAwQiIJAADA0rYRfjUAAAAYIZEEAADgHkkjJJIAAAAwQiIJAADAPZJG+NUAAABghEQSAACAeySN0EgCAACwtG2EXw0AAABGSCQBAABY2jZCIgkAAAAjJJIAAADcI2mEXw0AAABGSCQBAAC4R9IIiSQAAACMkEgCAABwj6QRGkkAAACWto3QfgMAAMAIiSQAAABL20b41QAAAGCERBIAAIBE0gi/GgAAAIyQSAIAAHjw1LYJEkkAAAAYIZEEAADgHkkjNJIAAABsSG6E9hsAAABGSCQBAABY2jbCrwYAAFCBJCcnq3///goJCZHFYtHq1atLzNm3b59uu+02+fn5ycfHRx07dtTRo0ft58+cOaPo6GjVrl1bNWrU0JAhQ5SRkeFwjaNHj6pv376qXr26AgMDNWnSJJ09e7ZMtdJIAgAAWCzOO8ro1KlTatOmjZYsWXLe899//71uvvlmNW/eXBs2bNDu3bs1depUeXt72+eMHz9eH374od59911t3LhRx44d0+DBg+3nz507p759+6qgoEBbt27VsmXLtHTpUk2bNq1sP5vNZrOV+RtWcNXajXF1CQCcJDMl3tUlAHCSmt6uy7eq3fqs0679W+Jk4/daLBatWrVKAwcOtI9FRESoatWqeuONN877npycHNWtW1crVqzQ7bffLkn69ttv1aJFC6WkpOjGG2/UJ598on79+unYsWMKCgqSJCUkJGjy5Mk6ceKEvLy8SlUfiSQAAIDFw2lHfn6+cnNzHY78/HyjMouKivTRRx/pmmuuUXh4uAIDA9WpUyeH5e/U1FQVFhYqLCzMPta8eXM1aNBAKSkpkqSUlBS1bt3a3kRKUnh4uHJzc7V3795S10MjCQAA4ESxsbHy8/NzOGJjY42ulZmZqby8PD3zzDPq1auX1q1bp0GDBmnw4MHauHGjJCk9PV1eXl7y9/d3eG9QUJDS09Ptc/7YRBafLz5XWjy1DQAA4MR9JKdMmaKYmBiHMavVanStoqIiSdKAAQM0fvx4SVLbtm21detWJSQkqGvXrn+v2DKikQQAAHDi9j9Wq9W4cfyzOnXqyNPTUy1btnQYb9GihTZv3ixJCg4OVkFBgbKzsx1SyYyMDAUHB9vnbN++3eEaxU91F88pDZa2AQAAKgkvLy917NhR+/fvdxj/7rvv1LBhQ0lS+/btVbVqVa1fv95+fv/+/Tp69KhCQ0MlSaGhodqzZ48yMzPtcxITE+Xr61uiSf0rJJIAAAAV6E8k5uXl6eDBg/bXhw8fVlpamgICAtSgQQNNmjRJd9xxh7p06aLu3btr7dq1+vDDD7VhwwZJkp+fn0aOHKmYmBgFBATI19dXY8eOVWhoqG688UZJUs+ePdWyZUsNHz5ccXFxSk9P1xNPPKHo6Ogypac0kgAAABXIzp071b17d/vr4vsro6KitHTpUg0aNEgJCQmKjY3VQw89pGbNmum9997TzTffbH/PggUL5OHhoSFDhig/P1/h4eF6/vnn7eerVKmiNWvWaPTo0QoNDZWPj4+ioqI0c+bMMtXKPpIAKhX2kQQuXy7dR7LPIqdd+7ePH3batV2NeyQBAABghKVtAACACnSPZGVCIgkAAAAjJJIAAABO3EfyckYjCQAAQCNphF8NAAAARkgkAQAAeNjGCIkkAAAAjJBIAgAAcI+kEX41AAAAGCGRBAAA4B5JIySSAAAAMEIiCQAAwD2SRmgkAQAAWNo2QvsNAAAAIySSAADA7VlIJI2QSAIAAMAIiSQAAHB7JJJmSCQBAABghEQSAACAQNIIiSQAAACMkEgCAAC3xz2SZmgkAQCA26ORNMPSNgAAAIyQSAIAALdHImmGRBIAAABGSCQBAIDbI5E0QyIJAAAAIySSAAAABJJGSCQBAABghEQSAAC4Pe6RNEMiCQAAACMkkgAAwO2RSJqhkQQAAG6PRtIMS9sAAAAwQiIJAADcHomkGRJJAAAAGCGRBAAAIJA0QiIJAAAAIySSAADA7XGPpBkSSQAAABghkQQAAG6PRNIMjSQAAHB7NJJmWNoGAACoQJKTk9W/f3+FhITIYrFo9erVF5z7wAMPyGKxaOHChQ7jWVlZioyMlK+vr/z9/TVy5Ejl5eU5zNm9e7duueUWeXt7q379+oqLiytzrTSSAAAAFiceZXTq1Cm1adNGS5Ys+ct5q1at0hdffKGQkJAS5yIjI7V3714lJiZqzZo1Sk5O1qhRo+znc3Nz1bNnTzVs2FCpqamaM2eOpk+frpdeeqlMtbK0DQAAUIH07t1bvXv3/ss5P/30k8aOHatPP/1Uffv2dTi3b98+rV27Vjt27FCHDh0kSYsXL1afPn00d+5chYSEaPny5SooKNBrr70mLy8vtWrVSmlpaZo/f75Dw3kxJJIAAMDtWSwWpx35+fnKzc11OPLz841rLSoq0vDhwzVp0iS1atWqxPmUlBT5+/vbm0hJCgsLk4eHh7Zt22af06VLF3l5ednnhIeHa//+/frll19KXQuNJAAAgBPFxsbKz8/P4YiNjTW+3rPPPitPT0899NBD5z2fnp6uwMBAhzFPT08FBAQoPT3dPicoKMhhTvHr4jmlwdI2AABwe858anvKlCmKiYlxGLNarUbXSk1N1aJFi7Rr164K8aQ5iSQAAIATWa1W+fr6OhymjeSmTZuUmZmpBg0ayNPTU56enjpy5IgmTJigRo0aSZKCg4OVmZnp8L6zZ88qKytLwcHB9jkZGRkOc4pfF88pDRpJAADg9px5j2R5Gj58uHbv3q20tDT7ERISokmTJunTTz+VJIWGhio7O1upqan29yUlJamoqEidOnWyz0lOTlZhYaF9TmJiopo1a6ZatWqVuh6WtgEAgNurCMvExfLy8nTw4EH768OHDystLU0BAQFq0KCBateu7TC/atWqCg4OVrNmzSRJLVq0UK9evXT//fcrISFBhYWFGjNmjCIiIuxbBQ0bNkwzZszQyJEjNXnyZH399ddatGiRFixYUKZaaSQBAAAqkJ07d6p79+7218X3V0ZFRWnp0qWlusby5cs1ZswY9ejRQx4eHhoyZIji4+Pt5/38/LRu3TpFR0erffv2qlOnjqZNm1amrX8kyWKz2WxlekclUK3dGFeXAMBJMlPiLz4JQKVU09t1d9yFPPC+0659LGGw067tatwjCQAAACMsbQMAALdXke6RrExIJAEAAGCERBIAALg9EkkzJJIAAAAwQiIJAADcHomkGRpJAAAA+kgjLG0DAADACIkkAABweyxtmyGRBAAAgBESSQAA4PZIJM2QSAIAAMAIiSRcrvP1TTX+7jBd37KB6tX109DxL+nDDbvt53/78rnzvu+xBau04F/rHca8qnoq+Y2JatPsSnW6I1a7v/vJfu7aq0O08NGhat+qoU7+kqcX3t6o+cs+c86XAlBqmRkZWrxwnrZuSdaZM2d0Zf0GenLmbLVsda0k6fTpU1q8cL42fr5eOTnZCrniSt1x5126fWiEiyvH5YRE0gyNJFzOp5pVe777Sf/6T4pWzh9V4nyjsCkOr3t2bqWEJ4dp1fq0EnNnjxug4ydy1KbZlQ7jNX289eHzY/T5tm819um3de3VVyjhyUhl//qbXnt/S7l+HwCll5ubo5EjhqlDh05atOQl1aoVoB+PHpGvr699zoK5z2rH9m2aOTtOISFX6IuULXp29kzVDQxU127/48LqAdBIwuXWbflG67Z8c8HzGT//6vC6f7fW2rjjgH746WeH8Z6dW6rHjS1056RX1OvmVg7nIvp0kFfVKvrn9OUqPHtO+w6l67pmV+ihu7rTSAIutOy1VxQUVE9PzpptH7viSsf/IfhV2pfq13+AOnS8QZI0+Pahev/fK7X36900kig3JJJmXHqP5MmTJxUXF6dBgwYpNDRUoaGhGjRokObMmaMTJ064sjRUUIEBNdXr5mu1bHVKifHnp96pkVP/pdO/FZR4X6frGmvLroMqPHvOPpa4dZ+aNQ6Wf81qTq8bwPklb/xcLVq10uSJ43Rrt84aNnSwVr33jsOcNm3bKXnj58rMyJDNZtPO7dt09MgPujG0s4uqxmXJ4sTjMuayRnLHjh265pprFB8fLz8/P3Xp0kVdunSRn5+f4uPj1bx5c+3cufOi18nPz1dubq7DYSs6d9H3oXK6q38n/Xr6jFYnpTmMvzTzLr38783a9c3R874vqLZviWQzM+v310F1fM/3FgCXwE///VHvvfO2GjRoqMUvvKzbh0Zo7rOzteaD1fY5kx59Qo2bNFWfnt10Y4frNPbB+/XIY1N1ffuOriscgCQXLm2PHTtW//jHP5SQkFAiTrbZbHrggQc0duxYpaSkXOAKv4uNjdWMGTMcxqoEdVTVejeUe81wvbsH3KiVn+xUfsFZ+9iDd3ZVzeremvPaOhdWBsBEUZFNLVu1UvRD4yVJzVu01PcHD+i9d99Wv9sGSpJWvvWm9uz+SvMXPa96ISHalbpTcbNnqW7dQHW68SYXVo/LCUvbZlyWSH711VcaP378ef+Ls1gsGj9+vNLS0i56nSlTpignJ8fh8Axq74SK4Wqd2zVVs8bBen3VVofxbh2vUafrGitn20L9umOR9n7wpCRpy/JH9PLM4ZKkjJ9zFVS7psP7AgN+f51xMvcSVA/gfOrUraPGTZo6jDVu0kTpx49Lks6cOaMl8QsVM3GyunTrrquvaaY77ozUreG99eay111RMoA/cFkiGRwcrO3bt6t58+bnPb99+3YFBQVd9DpWq1VWq9VhzOJRpVxqRMUSNTBUqd8c1Z4/bOkjSRPi/q3pS9bYX9er66c1L4zR8Edf1449P0iStu0+rOnR/eXp6aGzZ4skST1ubK79h9OV/etvl+w7AHDUpu31OvLDDw5jR478oHohIZKks2fP6uzZQlk8HHMPD48qKioqulRlwg2QSJpxWSM5ceJEjRo1SqmpqerRo4e9aczIyND69ev18ssva+7cua4qD5eQTzUvNa1f1/660RW1dd01V+iX3NP6Mf0XSb9v3zP41nZ6dP6qEu8vnlMs73S+JOnQjyf0U2a2JGnlJzv12Kg+SngyUvNeT1Srq0IUPaybHpn7vpO+FYDSGHZXlO6NGqbXXnlRt/bspb1f79Gqf7+rx6f9fstSjRo1dH2Hjlo0f46sVm/VqxeiXak79PGa/2j8xMkurh6AxWaz2Vz14StXrtSCBQuUmpqqc+d+f0CmSpUqat++vWJiYjR06FCj61ZrN6Y8y4ST3dL+aq175eES42988IVGPfmmJOnewZ01Z+IQNe75mHLzzvzl9RrUC9D+j2f+5YbkP2f/viH5vKVsSF7ZZKbEu7oElLNNGz/Xc/EL9OPRIwq54kpFDo/SoCH//+//kydPaMmiBfoiZYtyc3MUXC9Eg4YMVeTwKFKky0xNb9dtJnPVxE+cdu2Dc3s77dqu5tJGslhhYaFOnjwpSapTp46qVq36t65HIwlcvmgkgcsXjWTlUyE2JK9atarq1avn6jIAAICbIt02UyEaSQAAAFeijzTj0r9sAwAAgMqLRBIAALg9lrbNkEgCAADACIkkAABwewSSZkgkAQAAYIREEgAAuD0PDyJJEySSAAAAMEIiCQAA3B73SJqhkQQAAG6P7X/MsLQNAAAAIySSAADA7RFImiGRBAAAgBESSQAA4Pa4R9IMiSQAAACMkEgCAAC3RyJphkQSAAAARkgkAQCA2yOQNEMjCQAA3B5L22ZY2gYAAKhAkpOT1b9/f4WEhMhisWj16tX2c4WFhZo8ebJat24tHx8fhYSE6O6779axY8ccrpGVlaXIyEj5+vrK399fI0eOVF5ensOc3bt365ZbbpG3t7fq16+vuLi4MtdKIwkAANyexeK8o6xOnTqlNm3aaMmSJSXOnT59Wrt27dLUqVO1a9cuvf/++9q/f79uu+02h3mRkZHau3evEhMTtWbNGiUnJ2vUqFH287m5uerZs6caNmyo1NRUzZkzR9OnT9dLL71UplpZ2gYAAKhAevfurd69e5/3nJ+fnxITEx3GnnvuOd1www06evSoGjRooH379mnt2rXasWOHOnToIElavHix+vTpo7lz5yokJETLly9XQUGBXnvtNXl5ealVq1ZKS0vT/PnzHRrOiyGRBAAAbs9isTjtyM/PV25ursORn59fbrXn5OTIYrHI399fkpSSkiJ/f397EylJYWFh8vDw0LZt2+xzunTpIi8vL/uc8PBw7d+/X7/88kupP5tGEgAAwIliY2Pl5+fncMTGxpbLtc+cOaPJkyfrzjvvlK+vryQpPT1dgYGBDvM8PT0VEBCg9PR0+5ygoCCHOcWvi+eUBkvbAADA7Tnzoe0pU6YoJibGYcxqtf7t6xYWFmro0KGy2Wx64YUX/vb1TNBIAgAAOJHVai2XxvGPipvII0eOKCkpyZ5GSlJwcLAyMzMd5p89e1ZZWVkKDg62z8nIyHCYU/y6eE5psLQNAADcnjPvkSxvxU3kgQMH9Nlnn6l27doO50NDQ5Wdna3U1FT7WFJSkoqKitSpUyf7nOTkZBUWFtrnJCYmqlmzZqpVq1apa6GRBAAAqEDy8vKUlpamtLQ0SdLhw4eVlpamo0ePqrCwULfffrt27typ5cuX69y5c0pPT1d6eroKCgokSS1atFCvXr10//33a/v27dqyZYvGjBmjiIgIhYSESJKGDRsmLy8vjRw5Unv37tXKlSu1aNGiEkvwF8PSNgAAcHsV6Q/b7Ny5U927d7e/Lm7uoqKiNH36dH3wwQeSpLZt2zq87/PPP1e3bt0kScuXL9eYMWPUo0cPeXh4aMiQIYqPj7fP9fPz07p16xQdHa327durTp06mjZtWpm2/pFoJAEAACrUn0js1q2bbDbbBc//1bliAQEBWrFixV/Oue6667Rp06Yy1/dHLG0DAADACIkkAABwexUokKxUSCQBAABghEQSAAC4vYp0j2RlQiIJAAAAIySSAADA7RFImiGRBAAAgBESSQAA4Pa4R9IMjSQAAHB79JFmWNoGAACAERJJAADg9ljaNkMiCQAAACMkkgAAwO2RSJohkQQAAIAREkkAAOD2CCTNkEgCAADACIkkAABwe9wjaYZGEgAAuD36SDMsbQMAAMAIiSQAAHB7LG2bIZEEAACAERJJAADg9ggkzZBIAgAAwAiJJAAAcHseRJJGSCQBAABghEQSAAC4PQJJMzSSAADA7bH9jxmWtgEAAGCERBIAALg9DwJJIySSAAAAMEIiCQAA3B73SJohkQQAAIAREkkAAOD2CCTNkEgCAADACIkkAABwexYRSZqgkQQAAG6P7X/MsLQNAAAAIySSAADA7bH9jxkSSQAAABghkQQAAG6PQNIMiSQAAACMkEgCAAC350EkaYREEgAAAEZoJAEAgNuzWJx3lFVycrL69++vkJAQWSwWrV692uG8zWbTtGnTVK9ePVWrVk1hYWE6cOCAw5ysrCxFRkbK19dX/v7+GjlypPLy8hzm7N69W7fccou8vb1Vv359xcXFlblWGkkAAOD2LBaL046yOnXqlNq0aaMlS5ac93xcXJzi4+OVkJCgbdu2ycfHR+Hh4Tpz5ox9TmRkpPbu3avExEStWbNGycnJGjVqlP18bm6uevbsqYYNGyo1NVVz5szR9OnT9dJLL5WpVu6RBAAAcKL8/Hzl5+c7jFmtVlmt1vPO7927t3r37n3eczabTQsXLtQTTzyhAQMGSJL+9a9/KSgoSKtXr1ZERIT27duntWvXaseOHerQoYMkafHixerTp4/mzp2rkJAQLV++XAUFBXrttdfk5eWlVq1aKS0tTfPnz3doOC+GRBIAALg9Zy5tx8bGys/Pz+GIjY01qvPw4cNKT09XWFiYfczPz0+dOnVSSkqKJCklJUX+/v72JlKSwsLC5OHhoW3bttnndOnSRV5eXvY54eHh2r9/v3755ZdS10MiCQAA4ERTpkxRTEyMw9iF0siLSU9PlyQFBQU5jAcFBdnPpaenKzAw0OG8p6enAgICHOY0bty4xDWKz9WqVatU9dBIAgAAt+fM7X/+ahm7smNpGwAAoJIIDg6WJGVkZDiMZ2Rk2M8FBwcrMzPT4fzZs2eVlZXlMOd81/jjZ5QGjSQAAHB7Fice5alx48YKDg7W+vXr7WO5ubnatm2bQkNDJUmhoaHKzs5WamqqfU5SUpKKiorUqVMn+5zk5GQVFhba5yQmJqpZs2alXtaWaCQBAAAqlLy8PKWlpSktLU3S7w/YpKWl6ejRo7JYLBo3bpyeeuopffDBB9qzZ4/uvvtuhYSEaODAgZKkFi1aqFevXrr//vu1fft2bdmyRWPGjFFERIRCQkIkScOGDZOXl5dGjhypvXv3auXKlVq0aFGJezkvhnskAQCA2zPZ79FZdu7cqe7du9tfFzd3UVFRWrp0qR555BGdOnVKo0aNUnZ2tm6++WatXbtW3t7e9vcsX75cY8aMUY8ePeTh4aEhQ4YoPj7eft7Pz0/r1q1TdHS02rdvrzp16mjatGll2vpHkiw2m832N79vhVOt3RhXlwDASTJT4i8+CUClVNPbdQulkW+kOe3ay4e3ddq1XY2lbQAAABhhaRsAALi9irS0XZmQSAIAAMAIiSQAAHB7BJJmSCQBAABghEQSAAC4Pe6RNEMiCQAAACMkkgAAwO15EEgaoZEEAABuj6VtMyxtAwAAwAiJJAAAcHvkkWZIJAEAAGDEqJHctGmT7rrrLoWGhuqnn36SJL3xxhvavHlzuRYHAABwKXhYLE47LmdlbiTfe+89hYeHq1q1avryyy+Vn58vScrJydHs2bPLvUAAAABUTGVuJJ966iklJCTo5ZdfVtWqVe3jnTt31q5du8q1OAAAgEvBYnHecTkrcyO5f/9+denSpcS4n5+fsrOzy6MmAAAAVAJlbiSDg4N18ODBEuObN29WkyZNyqUoAACAS8lisTjtuJyVuZG8//779fDDD2vbtm2yWCw6duyYli9frokTJ2r06NHOqBEAAAAVUJn3kXz00UdVVFSkHj166PTp0+rSpYusVqsmTpyosWPHOqNGAAAAp7rMg0OnKXMjabFY9Pjjj2vSpEk6ePCg8vLy1LJlS9WoUcMZ9QEAADjd5b5Nj7MY/2UbLy8vtWzZsjxrAQAAQCVS5kaye/fuf3njaFJS0t8qCAAA4FIjkDRT5kaybdu2Dq8LCwuVlpamr7/+WlFRUeVVFwAAACq4MjeSCxYsOO/49OnTlZeX97cLAgAAuNQu9216nMXob22fz1133aXXXnutvC4HAACACs74YZs/S0lJkbe3d3ld7m85tGG+q0sA4CRVPcvtf/8CgB3/ZjFT5kZy8ODBDq9tNpuOHz+unTt3aurUqeVWGAAAACq2MjeSfn5+Dq89PDzUrFkzzZw5Uz179iy3wgAAAC4V7pE0U6ZG8ty5c7rnnnvUunVr1apVy1k1AQAAXFIe9JFGynRLQJUqVdSzZ09lZ2c7qRwAAABUFmW+t/Taa6/VoUOHnFELAACAS3hYnHdczsrcSD711FOaOHGi1qxZo+PHjys3N9fhAAAAgHso9T2SM2fO1IQJE9SnTx9J0m233eZwY6rNZpPFYtG5c+fKv0oAAAAn4mEbM6VuJGfMmKEHHnhAn3/+uTPrAQAAQCVR6kbSZrNJkrp27eq0YgAAAFzhcr+X0VnKdI8ksS8AAACKlWkfyWuuueaizWRWVtbfKggAAOBSIyszU6ZGcsaMGSX+sg0AAEBl50EnaaRMjWRERIQCAwOdVQsAAAAqkVI3ktwfCQAALldl3lgbksrwuxU/tQ0AAABIZUgki4qKnFkHAACAy7DwaoYkFwAAoII4d+6cpk6dqsaNG6tatWpq2rSpZs2a5bAybLPZNG3aNNWrV0/VqlVTWFiYDhw44HCdrKwsRUZGytfXV/7+/ho5cqTy8vLKvV4aSQAA4PY8LBanHWXx7LPP6oUXXtBzzz2nffv26dlnn1VcXJwWL15snxMXF6f4+HglJCRo27Zt8vHxUXh4uM6cOWOfExkZqb179yoxMVFr1qxRcnKyRo0aVW6/VzGL7TK8+fF4ToGrSwDgJLV8vFxdAgAn8S7TXjLla+raAxefZGhWr6tLPbdfv34KCgrSq6++ah8bMmSIqlWrpjfffFM2m00hISGaMGGCJk6cKEnKyclRUFCQli5dqoiICO3bt08tW7bUjh071KFDB0nS2rVr1adPH/33v/9VSEhIuX03EkkAAOD2LBbnHfn5+crNzXU48vPzz1vHTTfdpPXr1+u7776TJH311VfavHmzevfuLUk6fPiw0tPTFRYWZn+Pn5+fOnXqpJSUFElSSkqK/P397U2kJIWFhcnDw0Pbtm0r19+NRhIAALg9D4vzjtjYWPn5+TkcsbGx563j0UcfVUREhJo3b66qVauqXbt2GjdunCIjIyVJ6enpkqSgoCCH9wUFBdnPpaenl9j329PTUwEBAfY55cWFITIAAMDlb8qUKYqJiXEYs1qt5537zjvvaPny5VqxYoVatWqltLQ0jRs3TiEhIYqKiroU5ZYJjSQAAHB7zvwTiVar9YKN459NmjTJnkpKUuvWrXXkyBHFxsYqKipKwcHBkqSMjAzVq1fP/r6MjAy1bdtWkhQcHKzMzEyH6549e1ZZWVn295cXlrYBAAAqiNOnT8vDw7E9q1Klin0/78aNGys4OFjr16+3n8/NzdW2bdsUGhoqSQoNDVV2drZSU1Ptc5KSklRUVKROnTqVa70kkgAAwO1VlA3J+/fvr6effloNGjRQq1at9OWXX2r+/Pm69957Jf3+J6vHjRunp556SldffbUaN26sqVOnKiQkRAMHDpQktWjRQr169dL999+vhIQEFRYWasyYMYqIiCjXJ7YlGkkAAIAKY/HixZo6daoefPBBZWZmKiQkRP/85z81bdo0+5xHHnlEp06d0qhRo5Sdna2bb75Za9eulbe3t33O8uXLNWbMGPXo0UMeHh4aMmSI4uPjy71e9pEEUKmwjyRw+XLlPpJPrz/otGs/3uMqp13b1bhHEgAAAEZY2gYAAG7Pogpyk2QlQyMJAADcngd9pBGWtgEAAGCERBIAALg9EkkzJJIAAAAwQiIJAADcnqWi7EheyZBIAgAAwAiJJAAAcHvcI2mGRBIAAABGSCQBAIDb4xZJMzSSAADA7XnQSRphaRsAAABGSCQBAIDb42EbMySSAAAAMEIiCQAA3B63SJohkQQAAIAREkkAAOD2PEQkaYJEEgAAAEZIJAEAgNvjHkkzNJIAAMDtsf2PGZa2AQAAYIREEgAAuD3+RKIZEkkAAAAYIZEEAABuj0DSDIkkAAAAjJBIAgAAt8c9kmZIJAEAAGCERBIAALg9AkkzNJIAAMDtsURrht8NAAAARkgkAQCA27Owtm2ERBIAAABGSCQBAIDbI480QyIJAAAAIySSAADA7bEhuRkSSQAAABghkQQAAG6PPNIMjSQAAHB7rGybYWkbAAAARkgkAQCA22NDcjMkkgAAADBCIgkAANweyZoZfjcAAIAK5KefftJdd92l2rVrq1q1amrdurV27txpP2+z2TRt2jTVq1dP1apVU1hYmA4cOOBwjaysLEVGRsrX11f+/v4aOXKk8vLyyr1WGkkAAOD2LBaL046y+OWXX9S5c2dVrVpVn3zyib755hvNmzdPtWrVss+Ji4tTfHy8EhIStG3bNvn4+Cg8PFxnzpyxz4mMjNTevXuVmJioNWvWKDk5WaNGjSq336uYxWaz2cr9qi52PKfA1SUAcJJaPl6uLgGAk3i78Ia7d9KOOe3aQ9uGlHruo48+qi1btmjTpk3nPW+z2RQSEqIJEyZo4sSJkqScnBwFBQVp6dKlioiI0L59+9SyZUvt2LFDHTp0kCStXbtWffr00X//+1+FhJS+noshkQQAAG7P4sQjPz9fubm5Dkd+fv556/jggw/UoUMH/eMf/1BgYKDatWunl19+2X7+8OHDSk9PV1hYmH3Mz89PnTp1UkpKiiQpJSVF/v7+9iZSksLCwuTh4aFt27b93Z/KAY0kAACAE8XGxsrPz8/hiI2NPe/cQ4cO6YUXXtDVV1+tTz/9VKNHj9ZDDz2kZcuWSZLS09MlSUFBQQ7vCwoKsp9LT09XYGCgw3lPT08FBATY55QXntoGAABuz5n7SE6ZMkUxMTEOY1ar9bxzi4qK1KFDB82ePVuS1K5dO3399ddKSEhQVFSU02o0RSIJAADcnocTD6vVKl9fX4fjQo1kvXr11LJlS4exFi1a6OjRo5Kk4OBgSVJGRobDnIyMDPu54OBgZWZmOpw/e/assrKy7HPKC40kAABABdG5c2ft37/fYey7775Tw4YNJUmNGzdWcHCw1q9fbz+fm5urbdu2KTQ0VJIUGhqq7Oxspaam2uckJSWpqKhInTp1Ktd6WdoGAABur6L8icTx48frpptu0uzZszV06FBt375dL730kl566SVJv9c5btw4PfXUU7r66qvVuHFjTZ06VSEhIRo4cKCk3xPMXr166f7771dCQoIKCws1ZswYRURElOsT2xKNJAAAQIXRsWNHrVq1SlOmTNHMmTPVuHFjLVy4UJGRkfY5jzzyiE6dOqVRo0YpOztbN998s9auXStvb2/7nOXLl2vMmDHq0aOHPDw8NGTIEMXHx5d7vewjCaBSYR9J4PLlyn0kV+8u36eZ/2jgdeV7X2JFwj2SAAAAMMLSNgAAcHsV5BbJSodEEgAAAEZIJAEAgNvzEJGkCRpJAADg9ljaNsPSNgAAAIyQSAIAALdnYWnbCIkkAAAAjJBIAgAAt8c9kmZIJAEAAGCERBIAALg9tv8xQyIJAAAAIySSAADA7XGPpBkaSQAA4PZoJM2wtA0AAAAjJJIAAMDtsSG5GRJJAAAAGCGRBAAAbs+DQNIIiSQAAACMkEgCAAC3xz2SZkgkAQAAYIREEgAAuD32kTRDIwkAANweS9tmWNoGAACAERJJAADg9tj+xwyJJAAAAIyQSAIAALfHPZJmSCQBAABghEQSFdJXu3bq7TeX6rtvv9HPJ09oVtxC3dKth/18txtan/d9D4yNUcTwe3T82E9649UXtWvndmVlnVSdOnV1a+9+uuueUapateql+hoASuHVl1/U+sR1Onz4kKze3mrbtp3GxUxUo8ZN7HPy8/M1L+4Zrf3kYxUUFOimzjfr8alPqnadOi6sHJcTtv8xQyOJCunMmd/U9Opr1Kf/IE2dPK7E+fc+/tzh9faUTYp76kl1+Z8wSdLRI4dVZCvShCnTdEX9+jr8/UHNnT1dv/32mx58eOKl+AoASmnnju26485ItWrdWufOntPiRfP1wP0j9f4HH6l69eqSpDnPztamjRs1Z/5C1axZU7FPz1LMw2O0bPnbLq4ecG8Wm81mc3UR5e14ToGrS0A56nZD6xKJ5J89PvEh/Xb6tOY//8oF57z9xuv6z3sr9dbqtc4oE5dILR8vV5cAJ8vKylL3W0L12rI31b5DR/3666/qdnOonombq1vDe0mSDh/6XgP799EbK1bqujZtXVswyo23C+OtLQd+cdq1O19dy2nXdjXukUSll/XzSX2xZZP63DboL+fl5f2qmr5+l6gqAKbyfv1VkuTr9/s/r9/s/VpnzxaqU+hN9jmNmzRVvXoh+iotzRUl4jLkYbE47bicVehG8scff9S99977l3Py8/OVm5vrcOTn51+iClERfPrRB6ruU123dA+74Jz//nhUq955S7cN/sclrAxAWRUVFSnu2dlq2+56XX31NZKkn0+eVNWqVeXr6+swN6B2bZ08ecIVZQL4PxW6kczKytKyZcv+ck5sbKz8/PwcjsXz4y5RhagIPv5wlcLC+8pqtZ73/InMDD3y8APq2qOn+g28/RJXB6AsZj81Q98fOKC4uQtcXQrcjMWJx+XMpQ/bfPDBB395/tChQxe9xpQpUxQTE+MwlnXmcv+vDcV2f5mqH4/8oCefnnve8ydPZGr86JG6tnVbTXzsyUtcHYCymP3UTCVv3KDXlr2poOBg+3jtOnVUWFio3Nxch1Qy6+efVadOXVeUCuD/uLSRHDhwoCwWi/7qeR/LRe4tsFqtJZKoUzYetnEXH33wvq5p3lJXXdOsxLkTmRkaP3qkrmnRUpOnzZKHR4UO4AG3ZbPZFPv0LCWtT9SrS9/QlVfWdzjfstW18vSsqu1fpCisZ7gk6YfDh3T8+DG1advWBRXjskQGZcSl/5+1Xr16ev/991VUVHTeY9euXa4sDy50+vRpHfjuWx347ltJUvqxn3Tgu2+VkX7cPudUXp42rk9U3wFDSrz/RGaGxo2+V4HBwRr90ARl//KLfj55Uj+fPHnJvgOA0pk9a4Y+XvOBnombJ5/qPjp54oROnjihM2fOSJJq1qypQUOGaG7cM9q+7Qt9s/drTXviMbVp244ntgEXc2ki2b59e6WmpmrAgAHnPX+xtBKXr/379mr86P9/0GrJwjmSpPC+t2nKk09LkpISP5HNZlOP8N4l3r9ze4p++vGofvrxqP7Rz/EhnA3b9zixcgBl9c7KtyRJI0cMdxif+VSsBgwaLEmaNPkxeVg8NGHcQyoo/L8NyZ/gdhWUH/5EohmX7iO5adMmnTp1Sr169Trv+VOnTmnnzp3q2rVrma7LPpLA5Yt9JIHLlyv3kdz2fY7Trt2p6eW79RwbkgOoVGgkgcuXKxvJ7Yec10je0OTybST5E4kAAMDtsbBthsdYAQAAYIREEgAAgEjSCIkkAABABfXMM8/IYrFo3Lhx9rEzZ84oOjpatWvXVo0aNTRkyBBlZGQ4vO/o0aPq27evqlevrsDAQE2aNElnz54t9/poJAEAgNuzOPH/TO3YsUMvvviirrvuOofx8ePH68MPP9S7776rjRs36tixYxo8eLD9/Llz59S3b18VFBRo69atWrZsmZYuXapp06YZ13IhNJIAAAAVTF5eniIjI/Xyyy+rVq1a9vGcnBy9+uqrmj9/vv7nf/5H7du31+uvv66tW7fqiy++kCStW7dO33zzjd588021bdtWvXv31qxZs7RkyRIVFJTvzjY0kgAAwO1ZLM478vPzlZub63Dk5+f/ZT3R0dHq27evwsIc/6hGamqqCgsLHcabN2+uBg0aKCUlRZKUkpKi1q1bKygoyD4nPDxcubm52rt3bzn+ajSSAAAAThUbGys/Pz+HIzY29oLz3377be3ateu8c9LT0+Xl5SV/f3+H8aCgIKWnp9vn/LGJLD5ffK488dQ2AABwe858aHvKlCmKiYlxGLNareed++OPP+rhhx9WYmKivL29nVhV+SCRBAAAsDjvsFqt8vX1dTgu1EimpqYqMzNT119/vTw9PeXp6amNGzcqPj5enp6eCgoKUkFBgbKzsx3el5GRoeDgYElScHBwiae4i18XzykvNJIAAAAVRI8ePbRnzx6lpaXZjw4dOigyMtL+n6tWrar169fb37N//34dPXpUoaGhkqTQ0FDt2bNHmZmZ9jmJiYny9fVVy5Yty7VelrYBAIDb+zvb9JSnmjVr6tprr3UY8/HxUe3ate3jI0eOVExMjAICAuTr66uxY8cqNDRUN954oySpZ8+eatmypYYPH664uDilp6friSeeUHR09AWTUFM0kgAAAJXIggUL5OHhoSFDhig/P1/h4eF6/vnn7eerVKmiNWvWaPTo0QoNDZWPj4+ioqI0c+bMcq/FYrPZbOV+VRc7nlO+eyQBqDhq+Xi5ugQATuLtwngr7eivTrt22wY1nXZtV+MeSQAAABhhaRsAALi9inGHZOVDIgkAAAAjJJIAAABEkkZoJAEAgNurKNv/VDYsbQMAAMAIiSQAAHB7FgJJIySSAAAAMEIiCQAA3B6BpBkSSQAAABghkQQAACCSNEIiCQAAACMkkgAAwO2xj6QZEkkAAAAYIZEEAABuj30kzdBIAgAAt0cfaYalbQAAABghkQQAACCSNEIiCQAAACMkkgAAwO2x/Y8ZEkkAAAAYIZEEAABuj+1/zJBIAgAAwAiJJAAAcHsEkmZoJAEAAOgkjbC0DQAAACMkkgAAwO2x/Y8ZEkkAAAAYIZEEAABuj+1/zJBIAgAAwAiJJAAAcHsEkmZIJAEAAGCERBIAAIBI0giNJAAAcHts/2OGpW0AAAAYIZEEAABuj+1/zJBIAgAAwAiJJAAAcHsEkmZIJAEAAGCERBIAAIBI0giJJAAAAIyQSAIAALfHPpJmSCQBAIDbs1icd5RFbGysOnbsqJo1ayowMFADBw7U/v37HeacOXNG0dHRql27tmrUqKEhQ4YoIyPDYc7Ro0fVt29fVa9eXYGBgZo0aZLOnj37d3+mEmgkAQAAKoiNGzcqOjpaX3zxhRITE1VYWKiePXvq1KlT9jnjx4/Xhx9+qHfffVcbN27UsWPHNHjwYPv5c+fOqW/fviooKNDWrVu1bNkyLV26VNOmTSv3ei02m81W7ld1seM5Ba4uAYCT1PLxcnUJAJzE24U33P2Yle+0a9cPsBq/98SJEwoMDNTGjRvVpUsX5eTkqG7dulqxYoVuv/12SdK3336rFi1aKCUlRTfeeKM++eQT9evXT8eOHVNQUJAkKSEhQZMnT9aJEyfk5VV+/x4lkQQAAHCi/Px85ebmOhz5+aVrXHNyciRJAQEBkqTU1FQVFhYqLCzMPqd58+Zq0KCBUlJSJEkpKSlq3bq1vYmUpPDwcOXm5mrv3r3l9bUk0UgCAAA49R7J2NhY+fn5ORyxsbEXramoqEjjxo1T586dde2110qS0tPT5eXlJX9/f4e5QUFBSk9Pt8/5YxNZfL74XHniqW0AAAAnmjJlimJiYhzGrNaLL3dHR0fr66+/1ubNm51V2t9GIwkAAODE7X+sVq9SNY5/NGbMGK1Zs0bJycm68sor7ePBwcEqKChQdna2QyqZkZGh4OBg+5zt27c7XK/4qe7iOeWFpW0AAIAKwmazacyYMVq1apWSkpLUuHFjh/Pt27dX1apVtX79evvY/v37dfToUYWGhkqSQkNDtWfPHmVmZtrnJCYmytfXVy1btizXenlqG0ClwlPbwOXLlU9t/5TtvN7hCv/S/3vrwQcf1IoVK/Sf//xHzZo1s4/7+fmpWrVqkqTRo0fr448/1tKlS+Xr66uxY8dKkrZu3Srp9+1/2rZtq5CQEMXFxSk9PV3Dhw/Xfffdp9mzZ5fjN6ORBFDJ0EgCly9XNpLHnNhIhpShkbRcYAfz119/XSNGjJD0+4bkEyZM0FtvvaX8/HyFh4fr+eefd1i2PnLkiEaPHq0NGzbIx8dHUVFReuaZZ+TpWb4/Mo0kgEqFRhK4fNFIVj48bAMAANxeWf+UIX7HwzYAAAAwQiIJAADcnsWJ2/9czkgkAQAAYIREEgAAgEDSCIkkAAAAjJBIAgAAt0cgaYZGEgAAuD22/zHD0jYAAACMkEgCAAC3x/Y/ZkgkAQAAYIREEgAAgEDSCIkkAAAAjJBIAgAAt0cgaYZEEgAAAEZIJAEAgNtjH0kzNJIAAMDtsf2PGZa2AQAAYIREEgAAuD2Wts2QSAIAAMAIjSQAAACM0EgCAADACPdIAgAAt8c9kmZIJAEAAGCERBIAALg99pE0QyMJAADcHkvbZljaBgAAgBESSQAA4PYIJM2QSAIAAMAIiSQAAACRpBESSQAAABghkQQAAG6P7X/MkEgCAADACIkkAABwe+wjaYZEEgAAAEZIJAEAgNsjkDRDIwkAAEAnaYSlbQAAABghkQQAAG6P7X/MkEgCAADACIkkAABwe2z/Y4ZEEgAAAEYsNpvN5uoiAFP5+fmKjY3VlClTZLVaXV0OgHLEP99AxUcjiUotNzdXfn5+ysnJka+vr6vLAVCO+OcbqPhY2gYAAIARGkkAAAAYoZEEAACAERpJVGpWq1VPPvkkN+IDlyH++QYqPh62AQAAgBESSQAAABihkQQAAIARGkkAAAAYoZEEAACAERpJVGpLlixRo0aN5O3trU6dOmn79u2uLgnA35ScnKz+/fsrJCREFotFq1evdnVJAC6ARhKV1sqVKxUTE6Mnn3xSu3btUps2bRQeHq7MzExXlwbgbzh16pTatGmjJUuWuLoUABfB9j+otDp16qSOHTvqueeekyQVFRWpfv36Gjt2rB599FEXVwegPFgsFq1atUoDBw50dSkAzoNEEpVSQUGBUlNTFRYWZh/z8PBQWFiYUlJSXFgZAADug0YSldLJkyd17tw5BQUFOYwHBQUpPT3dRVUBAOBeaCQBAABghEYSlVKdOnVUpUoVZWRkOIxnZGQoODjYRVUBAOBeaCRRKXl5eal9+/Zav369fayoqEjr169XaGioCysDAMB9eLq6AMBUTEyMoqKi1KFDB91www1auHChTp06pXvuucfVpQH4G/Ly8nTw4EH768OHDystLU0BAQFq0KCBCysD8Gds/4NK7bnnntOcOXOUnp6utm3bKj4+Xp06dXJ1WQD+hg0bNqh79+4lxqOiorR06dJLXxCAC6KRBAAAgBHukQQAAIARGkkAAAAYoZEEAACAERpJAAAAGKGRBAAAgBEaSQAAABihkQQAAIARGkkAAAAYoZEEUGGNGDFCAwcOtL/u1q2bxo0bd8nr2LBhgywWi7Kzsy/5ZwNARUYjCaDMRowYIYvFIovFIi8vL1111VWaOXOmzp4969TPff/99zVr1qxSzaX5AwDn83R1AQAqp169eun1119Xfn6+Pv74Y0VHR6tq1aqaMmWKw7yCggJ5eXmVy2cGBASUy3UAAOWDRBKAEavVquDgYDVs2FCjR49WWFiYPvjgA/ty9NNPP62QkBA1a9ZMkvTjjz9q6NCh8vf3V0BAgAYMGKAffvjBfr1z584pJiZG/v7+ql27th555BHZbDaHz/zz0nZ+fr4mT56s+vXry2q16qqrrtKrr76qH374Qd27d5ck1apVSxaLRSNGjJAkFRUVKTY2Vo0bN1a1atXUpk0b/fvf/3b4nI8//ljXXHONqlWrpu7duzvUCQD4fzSSAMpFtWrVVFBQIElav3699u/fr8TERK1Zs0aFhYUKDw9XzZo1tWnTJm3ZskU1atRQr1697O+ZN2+eli5dqtdee02bN29WVlaWVq1a9Zefeffdd+utt95SfHy89u3bpxdffFE1atRQ/fr19d5770mS9u/fr+PHj2vRokWSpNjYWP3rX/9SQkKC9u7dq/Hjx+uuu+7Sxo0bJf3e8A4ePFj9+/dXWlqa7rvvPj366KPO+tkAoFJjaRvA32Kz2bR+/Xp9+umnGjt2rE6cOCEfHx+98sor9iXtN998U0VFRXrllVdksVgkSa+//rr8/f21YcMG9ezZUwsXLtSUKVM0ePBgSVJCQoI+/fTTC37ud999p3feeUeJiYkKCwuTJDVp0sR+vngZPDAwUP7+/pJ+TzBnz56tzz77TKGhofb3bN68WS+++KK6du2qF154QU2bNtW8efMkSc2aNdOePXv07LPPluOvBgCXBxpJAEbWrFmjGjVqqLCwUEVFRRo2bJimT5+u6OhotW7d2uG+yK+++koHDx5UzZo1Ha5x5swZff/998rJydHx48fVqVMn+zlPT0916NChxPJ2sbS0NFWpUkVdu3Ytdc0HDx7U6dOndeuttzqMFxQUqF27dpKkffv2OdQhyd50AgAc0UgCMNK9e3e98MIL8vLyUkhIiDw9//9fJz4+Pg5z8/Ly1L59ey1fvrzEderWrWv0+dWqVSvze/Ly8iRJH330ka644gqHc1ar1agOAHBnNJIAjPj4+Oiqq64q1dzrr79eK1euVGBgoHx9fc87p169etq2bZu6dOkiSTp79qxSU1N1/fXXn3d+69atVVRUpI0bN9qXtv+oOBE9d+6cfaxly5ayWq06evToBZPMFi1a6IMPPnAY++KLLy7+JQHADfGwDQCni4yMVJ06dTRgwABt2rRJhw8f1oYNG/TQQw/pv//9ryTp4Ycf1jPPPKPVq1fr22+/1YMPPviXe0A2atRIUVFRuvfee7V69Wr7Nd955x1JUsOGDWWxWLRmzRqdOHFCeXl5qlmzpiZOnKjx48dr2bJl+v7777Vr1y4tXrxYy5YtkyQ98MADOnDggCZNmqT9+/drxYoVWrp0qbN/IgColGgkAThd9erVlZycrAYNGmjw4MFq0aKFRo4cqTNnztgTygkTJmj48OGKiopSaGioatasqUGDBv3ldV944QXdfvvtevDBB9W8eXPdf//9OnXqlCTpiiuu0IwZM/Too48qKChIY8aMkSTNmjVLU6dOVWxsrFq0aKFevXrpo48+UuPGjSVJDRo00HvvvafVq1erTZs2SkhI0OzZs5346wBA5WWxXehOdgAAAOAvkEgCAADACI0kAAAAjNBIAgAAwAiNJAAAAIzQSAIAAMAIjSQAAACM0EgCAADACI0kAAAAjNBIAgAAwAiNJAAAAIzQSAIAAMDI/wLQ2kXpqD2nBAAAAABJRU5ErkJggg=="},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.91      0.96      0.94      1808\n           1       0.23      0.10      0.14       192\n\n    accuracy                           0.88      2000\n   macro avg       0.57      0.53      0.54      2000\nweighted avg       0.84      0.88      0.86      2000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"test_accuracy = accuracy_score(y_test, y_pred)\ntest_precision = precision_score(y_test, y_pred, average='macro')\ntest_recall = recall_score(y_test, y_pred, average='macro')\ntest_f1 = f1_score(y_test, y_pred, average='macro')\ntest_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"Accuracy:\", test_accuracy)\nprint('Precison:', test_precision)\nprint('Recall:', test_recall)\nprint('F1 Score:', test_f1)\nprint('AUC:', test_auc)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T09:08:19.780536Z","iopub.execute_input":"2024-04-11T09:08:19.781871Z","iopub.status.idle":"2024-04-11T09:08:19.799867Z","shell.execute_reply.started":"2024-04-11T09:08:19.781812Z","shell.execute_reply":"2024-04-11T09:08:19.798918Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Accuracy: 0.88\nPrecison: 0.5686572841384556\nRecall: 0.53327802359882\nF1 Score: 0.5391705069124424\nAUC: 0.53327802359882\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}